{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redis\n",
    "\n",
    "this notebook has code to insert the raw_html dataset as small chunks that refer to their parents, and a python object to store the parents (original docs). the code also will save the .rdb (if running redis-stack locally through docker) and the pickled parent store object to the current directory.\n",
    "\n",
    "at the end of the notebook are two code cells that demo how to use all this for retrieval\n",
    "\n",
    "get_html() or at least part of it I think might be useful because it makes the formatting of wiki pages nice for the recursive chunking\n",
    "\n",
    "a further idea is having the parent documents be chunked based on section, but I don't think any page is long enough to have to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import ParentDocumentRetriever\n",
    "from OurParentDocumentRetriever import OurParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore  # ...\n",
    "from langchain_redis import RedisConfig, RedisVectorStore\n",
    "\n",
    "# from langchain_community.storage import RedisStore\n",
    "# from langchain_community.vectorstores.redis import Redis\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import re\n",
    "import redis\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(file, sep=\" \"):\n",
    "\n",
    "    # stolen from https://stackoverflow.com/a/75501596 this sucks\n",
    "    NON_BREAKING_ELEMENTS = [\n",
    "        \"a\",\n",
    "        \"abbr\",\n",
    "        \"acronym\",\n",
    "        \"audio\",\n",
    "        \"b\",\n",
    "        \"bdi\",\n",
    "        \"bdo\",\n",
    "        \"big\",\n",
    "        \"button\",\n",
    "        \"canvas\",\n",
    "        \"cite\",\n",
    "        \"code\",\n",
    "        \"data\",\n",
    "        \"datalist\",\n",
    "        \"del\",\n",
    "        \"dfn\",\n",
    "        \"em\",\n",
    "        \"embed\",\n",
    "        \"i\",\n",
    "        \"iframe\",\n",
    "        \"img\",\n",
    "        \"input\",\n",
    "        \"ins\",\n",
    "        \"kbd\",\n",
    "        \"label\",\n",
    "        \"map\",\n",
    "        \"mark\",\n",
    "        \"meter\",\n",
    "        \"noscript\",\n",
    "        \"object\",\n",
    "        \"output\",\n",
    "        \"picture\",\n",
    "        \"progress\",\n",
    "        \"q\",\n",
    "        \"ruby\",\n",
    "        \"s\",\n",
    "        \"samp\",\n",
    "        \"script\",\n",
    "        \"select\",\n",
    "        \"slot\",\n",
    "        \"small\",\n",
    "        \"span\",\n",
    "        \"strong\",\n",
    "        \"sub\",\n",
    "        \"sup\",\n",
    "        \"svg\",\n",
    "        \"template\",\n",
    "        \"textarea\",\n",
    "        \"time\",\n",
    "        \"u\",\n",
    "        \"tt\",\n",
    "        \"var\",\n",
    "        \"video\",\n",
    "        \"wbr\",\n",
    "    ]\n",
    "\n",
    "    def html_to_text(text, preserve_new_lines=True, strip_tags=[\"style\", \"script\"]):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        for element in soup(strip_tags):\n",
    "            element.extract()\n",
    "        prev = False\n",
    "        if preserve_new_lines:\n",
    "            for element in soup.find_all():\n",
    "                strings = element.find_all(string=True, recursive=False)\n",
    "                if strings is not None:\n",
    "                    strings = [\n",
    "                        s\n",
    "                        for s in (None if s.strip() == \"\" else s for s in strings)\n",
    "                        if s is not None\n",
    "                    ]\n",
    "                strings = strings is not None and strings != []\n",
    "                strings = True\n",
    "                if element.name not in NON_BREAKING_ELEMENTS and strings:\n",
    "                    (\n",
    "                        element.append(\"\\n\")\n",
    "                        if element.name == \"br\"\n",
    "                        else element.append(\"\\n\\n\")\n",
    "                    )\n",
    "        return soup.get_text(separator=\"\")  # close enough\n",
    "\n",
    "    def replace_newlines(text):\n",
    "        text = re.sub(r\"\\n{3}\", \"\\n\", text)\n",
    "        text = re.sub(r\"\\n{4,}\", \"\\n\\n\", text)\n",
    "        return text\n",
    "\n",
    "    with open(file, \"r\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        iwant = soup.find_all(\"div\", {\"id\": \"content\"})\n",
    "        assert len(iwant) == 1\n",
    "        text = html_to_text(str(iwant[0]))\n",
    "        # text = soup.get_text(separator=sep)\n",
    "        # text = str(iwant[0]) # recursivecharactertextsplitter has html splitting built in so i will try that\n",
    "        # it sucks nevermind\n",
    "        text = replace_newlines(text).strip()\n",
    "        return text  # idk how to do better idk why there's just double spaces sometimes\n",
    "    return \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../dataset/raw_html\"\n",
    "files = os.listdir(dataset_path)\n",
    "pages = []\n",
    "\n",
    "for f in files:\n",
    "    pages.append(get_html(dataset_path + \"/\" + f))\n",
    "\n",
    "docs = []\n",
    "for i, page in enumerate(pages):\n",
    "    docs.append(Document(page, metadata={\"name\": files[i]}))\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)  # characters not toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accounts\n",
      "\n",
      "From UMIACS\n",
      "\n",
      "Jump to navigation\n",
      "Jump to search\n",
      "This page outlines the various account types at UMIACS and their management methods.\n",
      "Do not share any UMIACS-affiliated account for any purpose. Action may be taken against shared accounts.\n",
      "\n",
      "Contents\n",
      "\n",
      "1 UMIACS Account\n",
      "2 Guest / Collaborator Account\n",
      "3 Request a UMIACS account\n",
      "4 Password Requirements\n",
      "5 Changing account password\n",
      "\n",
      "5.1 If you know your current password\n",
      "5.2 If you do not know your current password\n",
      "\n",
      "\n",
      "6 Sponsored Accounts\n",
      "7 All Accounts Related Pages\n",
      "\n",
      "\n",
      "UMIACS Account\n",
      "A full UMIACS account consists of the following components:\n",
      "\n",
      " Active Directory Account\n",
      "Provides access to UMIACS-supported computers,  secgroup controlled resources, web services such as GitLab, and the VPN.\n",
      " Jira Account\n",
      "Provides access to the JIRA Request Tracker. Please note that it is possible to set a separate password for this account, at which point it will no longer be synced from your Active Directory Account. Changing your Jira password DOES NOT change your Active Directory Account password.\n",
      "\n",
      "\n",
      "Guest / Collaborator Account\n",
      " Collaborator Accounts\n",
      "Sponsored account to provide access to UMIACS Web Services for non-UMIACS collaborators.\n",
      "\n",
      "\n",
      "Request a UMIACS account\n",
      "Fill out the account request form located at the UMIACS Account Request webapp.  Please note that for the PI field you will be entering your PI/Professor/Sponsor's account name here at UMIACS.  Also indicate any labs you are a member of in the notes field to speed up access to your lab's resources. \n",
      "Once the form is submitted, it will send an email with a link to verify your email address.  You must click this link to verify your email address before the system will send email to your PI to review the account request.  Accounts can take anywhere from a few hours to a day for installation, depending on how quickly your PI approves your account.\n",
      "\n",
      "Password Requirements\n",
      "We have the following policies in regards to our ActiveDirectory passwords:\n",
      "\n",
      "Minimum Password Length : 8 Characters (more is allowed)\n",
      "Minimum Character Classes : 3 Character Classes\n",
      "English uppercase characters (A - Z)\n",
      "English lowercase characters (a - z)\n",
      "Base 10 digits (0 - 9)\n",
      "Non-alphanumeric (For example: !, $, #, or %)\n",
      "Unicode characters\n",
      "\n",
      "\n",
      "Password cannot contain (or match a substring of 3+ sequential characters) of your username or any part of your first or last name.\n",
      "Password was not previously used in your last 10 passwords.\n",
      "Password was not previously changed in the last 2 days.\n",
      "Password has not been  compromised in a known data breach.\n",
      "\n",
      "\n",
      "It is your responsibility to secure your password. Please choose a strong password, do not use your UMIACS password for any other service, and only connect using secure protocols.\n",
      "UMIACS staff members do not store your password in a retrievable format, so if you forget your password, you must reset it (next section).\n",
      "\n",
      "Changing account password\n",
      "If you know your current password\n",
      "You can use the UMIACS Password Web Application to change your password.\n",
      "\n",
      "\n",
      "If you do not know your current password\n",
      "You can reset your password yourself if you know your username and have already registered a mobile phone number and alternate E-mail address in our Directory Application.\n",
      "If your account was a UMIACS Collaborator Account, your password can be reset by your sponsor via our Requests Application.\n",
      "You can stop by the UMIACS Help Desk in room 3109 Iribe Center with a photo ID.\n",
      "If you are unable to physically stop by the UMIACS Help Desk, you can have your account sponsor (PI) work with the UMIACS Help Desk to receive a temporary password on your behalf.  The account sponsor will then be responsible for conveying the password to you in a secure manner. (e.g., Telephone, where they could identify you by voice)\n",
      "\n",
      "\n",
      "Sponsored Accounts\n",
      "You can view a list of the accounts you sponsor in the UMIACS Account Management Web Application. Clicking on an account's display name will bring you to a management page where you can see details such as the installation date or email forward. From here you can edit your relationship to the account.\n",
      "You can also request to remove your sponsorship or modify the expiration date, but these require staff action and are not immediate. \n",
      "\n",
      "All Accounts Related Pages\n",
      "Accounts/Collaborator\n",
      "\n",
      "\n",
      "Retrieved from \"https://wiki.umiacs.umd.edu/umiacs/index.php?title=Accounts&oldid=12071\"\n",
      "SLURM/ArrayJobs\n",
      "\n",
      "From UMIACS\n",
      "\n",
      "Jump to navigation\n",
      "Jump to search\n",
      "Here is an example to get you started using array jobs in SLURM.\n",
      "\n",
      "Array computation example job\n",
      "Save this code to a file called test.py.\n",
      "\n",
      "import time\n",
      "\n",
      "print('start at ' + time.strftime('%H:%M:%S'))\n",
      "\n",
      "print('sleep for 10 seconds ...')\n",
      "time.sleep(10)\n",
      "\n",
      "print('stop at ' + time.strftime('%H:%M:%S'))\n",
      "\n",
      "Submission script\n",
      "Save this to a file called array.sh and you should be able to submit the job as sbatch array.sh.\n",
      "\n",
      "#!/bin/bash\n",
      "\n",
      "#####################\n",
      "# job-array example #\n",
      "#####################\n",
      "\n",
      "#SBATCH --job-name=example\n",
      "#SBATCH --array=1-16        # run 16 jobs at the same time \n",
      "#SBATCH --time=0-00:05:00   # run for 5 minutes (d-hh:mm:ss)\n",
      "#SBATCH --mem-per-cpu=500MB # use 500MB per core\n",
      "\n",
      "# all bash commands must be after all SBATCH directives\n",
      "\n",
      "# define and create a unique scratch directory\n",
      "SCRATCH_DIRECTORY=/scratch0/${USER}/job-array-example/${SLURM_JOBID}\n",
      "mkdir -p ${SCRATCH_DIRECTORY}\n",
      "cd ${SCRATCH_DIRECTORY}\n",
      "SPLIT-------------------------\n",
      "# define and create a unique scratch directory\n",
      "SCRATCH_DIRECTORY=/scratch0/${USER}/job-array-example/${SLURM_JOBID}\n",
      "mkdir -p ${SCRATCH_DIRECTORY}\n",
      "cd ${SCRATCH_DIRECTORY}\n",
      "\n",
      "cp ${SLURM_SUBMIT_DIR}/test.py ${SCRATCH_DIRECTORY}\n",
      "\n",
      "# each job will see a different ${SLURM_ARRAY_TASK_ID}\n",
      "echo \"now processing task id:: \" ${SLURM_ARRAY_TASK_ID}\n",
      "python test.py > output_${SLURM_ARRAY_TASK_ID}.txt\n",
      "\n",
      "# after the job is done we copy our output back to $SLURM_SUBMIT_DIR\n",
      "cp output_${SLURM_ARRAY_TASK_ID}.txt ${SLURM_SUBMIT_DIR}\n",
      "\n",
      "# we step out of the scratch directory and remove it\n",
      "cd ${SLURM_SUBMIT_DIR}\n",
      "rm -rf ${SCRATCH_DIRECTORY}\n",
      "\n",
      "# happy end\n",
      "exit 0\n",
      "\n",
      "\n",
      "Retrieved from \"https://wiki.umiacs.umd.edu/umiacs/index.php?title=SLURM/ArrayJobs&oldid=11731\"\n",
      "SPLIT-------------------------\n",
      "\u001b[32m01:04:07\u001b[0m \u001b[34mredisvl.index.index\u001b[0m \u001b[1;30mINFO\u001b[0m   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew\\.cache\\huggingface\\modules\\transformers_modules\\dunzhang\\stella_en_1.5B_v5\\d03be74b361d4eb24f42a2fe5bd2e29917df4604\\modeling_qwen.py:704: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "out = child_splitter.split_documents([docs[212]])\n",
    "print(docs[0].page_content)\n",
    "for o in out:\n",
    "    print(o.page_content)\n",
    "    print(\"SPLIT-------------------------\")\n",
    "\n",
    "store = (\n",
    "    InMemoryStore()\n",
    ")  # theres a way to do this using a different database still in redis but idk how\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"dunzhang/stella_en_1.5B_v5\",\n",
    "    model_kwargs={\n",
    "        \"trust_remote_code\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "config = RedisConfig(\n",
    "    index_name=\"umiacs\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    metadata_schema=[\n",
    "        {\"name\": \"doc_id\", \"type\": \"tag\"},\n",
    "        {\"name\": \"name\", \"type\": \"tag\"},\n",
    "    ],\n",
    ")\n",
    "redis_conn = redis.from_url(\"redis://localhost:6379\", db=0)\n",
    "vectorstore = RedisVectorStore(embedding_model, config)\n",
    "\n",
    "retriever = OurParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_vectors(redis_conn, index_name):\n",
    "    \"\"\"Deletes all keys in the given Redis namespace.\"\"\"\n",
    "    try:\n",
    "        # Use SCAN to retrieve all keys that belong to the index namespace\n",
    "        keys = redis_conn.scan_iter(f\"{index_name}:*\")\n",
    "        for key in keys:\n",
    "            redis_conn.delete(key)  # Delete each key\n",
    "        print(f\"All vectors from index '{index_name}' have been deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting vectors: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not empty. deleting everything.\n",
      "All vectors from index 'umiacs' have been deleted.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "if len(redis_conn.keys()) != 0:\n",
    "    print(\"not empty. deleting everything.\")\n",
    "    delete_all_vectors(redis_conn, \"umiacs\")\n",
    "    print(len(redis_conn.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.262935638427734\n",
      "9.369140625\n",
      "9.369140625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_allocated() / 1024 / 1024 / 1024)\n",
    "print(torch.cuda.memory_reserved() / 1024 / 1024 / 1024)\n",
    "print(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024)\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Apptainer.html\n",
      "10.59765625\n",
      "empty cache\n",
      "18 BitLocker_PersonalUse.html\n",
      "10.037109375\n",
      "empty cache\n",
      "25 ClassAccounts.html\n",
      "11.404296875\n",
      "empty cache\n",
      "33 Compute_DataLocality.html\n",
      "9.93359375\n",
      "empty cache\n",
      "37 ConferenceRooms_TouchPanel.html\n",
      "9.80078125\n",
      "empty cache\n",
      "46 Data_Storage.html\n",
      "9.783203125\n",
      "empty cache\n",
      "62 Google_Drive_Drive_for_Desktop.html\n",
      "9.79296875\n",
      "empty cache\n",
      "94 Lamsub.sh.o127.html\n",
      "9.94921875\n",
      "empty cache\n",
      "99 LocalDataStorage.html\n",
      "9.947265625\n",
      "empty cache\n",
      "102 MacOSPrinting.html\n",
      "9.869140625\n",
      "empty cache\n",
      "104 MailmanFAQ.html\n",
      "9.8515625\n",
      "empty cache\n",
      "105 MailmanListAdmin.html\n",
      "9.83984375\n",
      "empty cache\n",
      "110 Matlab.html\n",
      "9.83203125\n",
      "empty cache\n",
      "117 Modules.html\n",
      "10.13671875\n",
      "empty cache\n",
      "119 MonthlyMaintenanceWindow.html\n",
      "10.259765625\n",
      "empty cache\n",
      "123 Mpich1sub.sh.o167.html\n",
      "9.92578125\n",
      "empty cache\n",
      "130 Network_Troubleshooting.html\n",
      "10.6484375\n",
      "empty cache\n",
      "139 Network_VPN_Windows.html\n",
      "9.7578125\n",
      "empty cache\n",
      "140 Nexus.html\n",
      "12.291015625\n",
      "empty cache\n",
      "142 Nexus_CBCB.html\n",
      "10.890625\n",
      "empty cache\n",
      "143 Nexus_CLIP.html\n",
      "9.853515625\n",
      "empty cache\n",
      "144 Nexus_CML.html\n",
      "10.732421875\n",
      "empty cache\n",
      "145 Nexus_GAMMA.html\n",
      "10.4765625\n",
      "empty cache\n",
      "147 Nexus_MBRC.html\n",
      "10.025390625\n",
      "empty cache\n",
      "148 Nexus_MC2.html\n",
      "9.86328125\n",
      "empty cache\n",
      "150 Nexus_Vulcan.html\n",
      "10.765625\n",
      "empty cache\n",
      "154 OBJ.html\n",
      "9.859375\n",
      "empty cache\n",
      "156 OpenCVVersions.html\n",
      "9.7109375\n",
      "empty cache\n",
      "165 Podman.html\n",
      "10.080078125\n",
      "empty cache\n",
      "173 PythonVirtualEnv.html\n",
      "9.76953125\n",
      "empty cache\n",
      "177 Rclone.html\n",
      "10.158203125\n",
      "empty cache\n",
      "178 Remote_Desktop.html\n",
      "9.9453125\n",
      "empty cache\n",
      "183 S3Clients.html\n",
      "9.75\n",
      "empty cache\n",
      "187 SecureShell.html\n",
      "9.98046875\n",
      "empty cache\n",
      "189 SecureShell_MFA.html\n",
      "10.205078125\n",
      "empty cache\n",
      "211 SLURM.html\n",
      "10.0703125\n",
      "empty cache\n",
      "213 SLURM_ClusterStatus.html\n",
      "9.849609375\n",
      "empty cache\n",
      "214 SLURM_JobStatus.html\n",
      "10.173828125\n",
      "empty cache\n",
      "215 SLURM_JobSubmission.html\n",
      "12.251953125\n",
      "empty cache\n",
      "217 SLURM_Priority.html\n",
      "11.337890625\n",
      "empty cache\n",
      "220 SSHFileTransferProtocol.html\n",
      "9.701171875\n",
      "empty cache\n",
      "222 SSH_Keys.html\n",
      "10.224609375\n",
      "empty cache\n",
      "226 Tensorflow.html\n",
      "9.783203125\n",
      "empty cache\n",
      "232 Umask.html\n",
      "9.701171875\n",
      "empty cache\n",
      "239 UNIXPrinting.html\n",
      "9.96875\n",
      "empty cache\n",
      "251 WebSpace.html\n",
      "9.828125\n",
      "empty cache\n",
      "256 WindowsServicing.html\n",
      "9.72265625\n",
      "empty cache\n",
      "inserted 1014 chunks and 263 docs\n"
     ]
    }
   ],
   "source": [
    "# retriever.add_documents(docs)\n",
    "for i, doc in enumerate(docs):\n",
    "    retriever.add_documents([doc], ids=[files[i]])\n",
    "    # print(torch.cuda.memory_reserved() / 1024 / 1024 / 1024)\n",
    "    if torch.cuda.memory_reserved() / 1024 / 1024 / 1024 > 9.7: # lol\n",
    "        print(i, doc.metadata[\"name\"])\n",
    "        print(torch.cuda.memory_reserved() / 1024 / 1024 / 1024)\n",
    "        print(\"empty cache\")\n",
    "        torch.cuda.empty_cache()\n",
    "print(\n",
    "    f\"inserted {len(redis_conn.keys())} chunks and {len(list(store.yield_keys()))} docs\"\n",
    ")\n",
    "assert len(files) == len(list(store.yield_keys())), f\"{len(files)} {len(list(store.yield_keys()))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter notebook --no-browser --port=8889 --ip=0.0.0.0\n",
      "\n",
      "This will start running the notebook on port 8889. Note: You must keep this shell window open to be able to connect. If the submission node for the cluster you are using is not accessible via the public internet, you must also be on a machine connected to the UMIACS network or connected to our  VPN in order to access the Jupyter notebook once you start the SSH tunnel, so ensure this is the case before starting the tunnel. Then, on your local machine, run\n",
      "\n",
      "ssh -N -f -L localhost:8888:<NODENAME>:8889 <USERNAME>@<SUBMISSIONNODE>.umiacs.umd.edu\n"
     ]
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search_with_score(\"python notebook nexus\", return_all=True)\n",
    "print(sub_docs[0][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you ran redis-stack docker image locally using \n",
    "# docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "# otherwise manually copy it yourself idk \n",
    "!docker cp redis-stack:/data/dump.rdb ./parentdoc.rdb\n",
    "with open(\"parentdoc.pkl\", \"wb\") as out:\n",
    "    pickle.dump(store, out, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SLURM.html', [0.39104783535, 0.403365552425, 0.406363248825, 0.437749147415])]\n",
      "s2p embedding:\n",
      "[('SLURM.html', [0.340046286583, 0.347794413567, 0.348588466644, 0.382137238979])]\n"
     ]
    }
   ],
   "source": [
    "query = \"how do i run a python notebook in nexus?\"\n",
    "# query = \"nexus python notebook\"\n",
    "res = retriever.similarity_search_with_score(query, k=4)\n",
    "print([(x[0].metadata[\"name\"], x[1]) for x in res])\n",
    "\n",
    "embedding = embedding_model.client.encode(query, prompt_name=\"s2p_query\")\n",
    "res = retriever.similarity_search_with_score_by_vector(embedding, k=4)\n",
    "print(\"s2p embedding:\", [(x[0].metadata[\"name\"], x[1]) for x in res], sep=\"\\n\")\n",
    "# print(*[x[0].page_content for x in res], sep=\"\\n\\n\\n\\n\")\n",
    "# print(len(thing.similarity_search(\"nexus\", k=4)))\n",
    "# print(len(thing.similarity_search_with_score(\"nexus\", k=4, distance_threshold=.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import gc \n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew\\miniforge3\\lib\\site-packages\\langchain\\retrievers\\document_compressors\\chain_extract.py:15: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain.chains.llm import LLMChain\n",
      "c:\\Users\\Andrew\\miniforge3\\lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n",
      "c:\\Users\\Andrew\\miniforge3\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m01:22:30\u001b[0m \u001b[34mredisvl.index.index\u001b[0m \u001b[1;30mINFO\u001b[0m   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew\\.cache\\huggingface\\modules\\transformers_modules\\dunzhang\\stella_en_1.5B_v5\\d03be74b361d4eb24f42a2fe5bd2e29917df4604\\modeling_qwen.py:704: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from langchain_redis import RedisConfig, RedisVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from OurParentDocumentRetriever import OurParentDocumentRetriever\n",
    "import pickle\n",
    "\n",
    "store = None\n",
    "with open(\"parentdoc.pkl\", \"rb\") as f: # you need the file to be there\n",
    "    store = pickle.load(f)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"dunzhang/stella_en_1.5B_v5\",\n",
    "    model_kwargs={\n",
    "        \"trust_remote_code\": True,\n",
    "    },\n",
    ")\n",
    "config = RedisConfig(\n",
    "    index_name=\"umiacs\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    metadata_schema=[\n",
    "        {\"name\": \"doc_id\", \"type\": \"tag\"},\n",
    "        {\"name\": \"name\", \"type\": \"tag\"},\n",
    "    ],\n",
    ")\n",
    "vectorstore = RedisVectorStore(embedding_model, config) # you need redis to be there\n",
    "\n",
    "\n",
    "retriever = OurParentDocumentRetriever( # you need vectorstore and docstore to match i think probably idk\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    # child_splitter=child_splitter, # dont need for retrieval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SLURM.html', [0.39104783535, 0.403365552425, 0.406363248825, 0.437749147415])]\n",
      "s2p embedding:\n",
      "[('SLURM.html', [0.340046286583, 0.347794413567, 0.348588466644, 0.382137238979])]\n",
      "Context:\n",
      "\n",
      "SLURM\n",
      "\n",
      "From UMIACS\n",
      "\n",
      "Jump to navigation\n",
      "Jump to search\n",
      "Contents\n",
      "\n",
      "1 Slurm Workload Manager\n",
      "\n",
      "1.1 Documentation\n",
      "1.2 Commands\n",
      "\n",
      "1.2.1 srun\n",
      "1.2.2 salloc\n",
      "1.2.3 sbatch\n",
      "1.2.4 squeue\n",
      "1.2.5 scancel\n",
      "1.2.6 sacct\n",
      "1.2.7 sstat\n",
      "\n",
      "\n",
      "1.3 Modules\n",
      "1.4 Running Jupyter Notebook on a Compute Node\n",
      "\n",
      "1.4.1 Setting up your Python Virtual Environment\n",
      "1.4.2 Running Jupyter Notebook\n",
      "\n",
      "2 Quick Guide to translate PBS/TORQUE to Slurm\n",
      "\n",
      "\n",
      "Slurm Workload Manager\n",
      "Slurm is an open-source workload manager designed for Linux clusters of all sizes. It provides three key functions. First, it allocates exclusive or non-exclusive access to resources (computer nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (typically a parallel job) on a set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.\n",
      "\n",
      "Documentation\n",
      " Submitting Jobs\n",
      " Checking Job Status\n",
      " Checking Cluster Status\n",
      " Understanding Job Priority\n",
      " Job Preemption Overview\n",
      "Official Documentation\n",
      "FAQ\n",
      "\n",
      "\n",
      "Related documentation:\n",
      "\n",
      " Optimizing Storage Performance\n",
      " Using VS Code\n",
      "\n",
      "\n",
      "Commands\n",
      "Below are some of the common commands used in Slurm. Further information on how to use these commands is found in the documentation linked above. To see all flags available for a command, please check the command's manual by using man <COMMAND> on the command line.\n",
      "\n",
      "srun\n",
      "srun runs a parallel job on a cluster managed by Slurm.  If necessary, it will first create a resource allocation in which to run the parallel job.\n",
      "\n",
      "salloc\n",
      "salloc allocates a Slurm job allocation, which is a set of resources (nodes), possibly with some set of constraints (e.g. number of processors per node).  When salloc successfully obtains the requested allocation, it then runs the command specified by the user.  Finally, when the user specified command is complete, salloc relinquishes the job allocation.  If no command is specified, salloc runs the user's default shell.\n",
      "\n",
      "sbatch\n",
      "sbatch submits a batch script to Slurm.  The batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input.  The batch script may contain options preceded with #SBATCH before any executable commands in the script.\n",
      "\n",
      "squeue\n",
      "squeue views job and job step information for jobs managed by Slurm.\n",
      "\n",
      "scancel\n",
      "scancel signals or cancels jobs, job arrays, or job steps.  An arbitrary number of jobs or job steps may be signaled using job specification filters or a space separated list of specific job and/or job step IDs.\n",
      "\n",
      "sacct\n",
      "sacct displays job accounting data stored in the job accounting log file or Slurm database in a variety of forms for your analysis.  The sacct command displays information on jobs, job steps, status, and exitcodes by default.  You can tailor the output with the use of the --format= option to specify the fields to be shown.\n",
      "\n",
      "sstat\n",
      "sstat displays job status information for your analysis.  The sstat command displays information pertaining to CPU, Task, Node, Resident Set Size (RSS) and Virtual Memory (VM).  You can tailor the output with the use of the --fields= option to specify the fields to be shown.\n",
      "\n",
      "Modules\n",
      "If you are trying to use  GNU Modules in a Slurm job, please read the section of our Modules documentation on  non-interactive shell sessions.  This also needs to be done if the OS version of the compute node you are scheduled on is different from the OS version of the submission node you are submitting the job from.\n",
      "\n",
      "Running Jupyter Notebook on a Compute Node\n",
      "The steps to run a Jupyter Notebook from a compute node are listed below.\n",
      "\n",
      "Setting up your Python Virtual Environment\n",
      " Create a Python virtual environment on the compute node you are assigned and  activate it. Next, install Jupyter using pip by following the steps here. You may also use other environment management systems such as Conda if desired.\n",
      "\n",
      "Running Jupyter Notebook\n",
      "After you've set up the Python virtual environment, submit a job, activate the environment within the job, and run the following command on the compute node you are assigned:\n",
      "\n",
      "jupyter notebook --no-browser --port=8889 --ip=0.0.0.0\n",
      "\n",
      "This will start running the notebook on port 8889. Note: You must keep this shell window open to be able to connect. If the submission node for the cluster you are using is not accessible via the public internet, you must also be on a machine connected to the UMIACS network or connected to our  VPN in order to access the Jupyter notebook once you start the SSH tunnel, so ensure this is the case before starting the tunnel. Then, on your local machine, run\n",
      "\n",
      "ssh -N -f -L localhost:8888:<NODENAME>:8889 <USERNAME>@<SUBMISSIONNODE>.umiacs.umd.edu\n",
      "\n",
      "This will tunnel port 8889 from the compute node to port 8888 on your local machine, using <SUBMISSIONNODE> as an intermediate node. Make sure to replace <USERNAME> with your username, <SUBMISSIONNODE> with the name of the submission node you want to use, and <NODENAME> with the name of the compute node you are assigned. Note that this command will not display any output if the connection is successful due to the included ssh flags. You must also keep this shell window open to be able to connect.\n",
      "For example, assuming your username is username and that you are using the Nexus cluster, have been  assigned the nexusgroup submission nodes, and are assigned compute node tron00.umiacs.umd.edu:\n",
      "\n",
      "ssh -N -f -L localhost:8888:tron00.umiacs.umd.edu:8889 username@nexusgroup.umiacs.umd.edu\n",
      "\n",
      "You can then open a web browser and type in localhost:8888 to access the notebook.\n",
      "Notes:\n",
      "\n",
      "Later versions of Jupyter have token authentication enabled by default - you will need to prepend the /?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX part of the URL provided by the terminal output after starting the notebook in order to connect if this is the case. e.g. localhost:8888/?token=fcc6bd0f996e7aa89376c33cb34f7b80890502aacc97d98e\n",
      "If the port on the compute node mentioned in the example above (8889) is not working, it may be that someone else has already started a process (Jupyter notebook or otherwise) using that specific port number on that specific compute node. The port number can be replaced with any other ephemeral port number you'd like, just make sure to change it in both the command you run on the compute node and the ssh command from your local machine.\n",
      "\n",
      "\n",
      "Quick Guide to translate PBS/TORQUE to Slurm\n",
      "PBS/TORQUE was the previous workload manager and job submission framework used at UMIACS prior to Slurm's adoption. Below is a quick guide of how to translate some common PBS/TORQUE commands to Slurm ones.\n",
      "\n",
      "\n",
      "User commands\n",
      "\n",
      "\n",
      "PBS/TORQUE\n",
      "\n",
      "Slurm\n",
      "\n",
      "\n",
      "Job submission\n",
      "\n",
      "qsub [filename]\n",
      "\n",
      "sbatch [filename]\n",
      "\n",
      "\n",
      "Job deletion\n",
      "\n",
      "qdel [job_id]\n",
      "\n",
      "scancel [job_id]\n",
      "\n",
      "\n",
      "Job status (by job)\n",
      "\n",
      "qstat [job_id]\n",
      "\n",
      "squeue --job [job_id]\n",
      "\n",
      "\n",
      "Full job status (by job)\n",
      "\n",
      "qstat -f [job_id]\n",
      "\n",
      "scontrol show job [job_id]\n",
      "\n",
      "\n",
      "Job status (by user)\n",
      "\n",
      "qstat -u [username]\n",
      "\n",
      "squeue --user=[username]\n",
      "\n",
      "Environment variables\n",
      "\n",
      "\n",
      "PBS/TORQUE\n",
      "\n",
      "Slurm\n",
      "\n",
      "\n",
      "Job ID\n",
      "\n",
      "$PBS_JOBID\n",
      "\n",
      "$SLURM_JOBID\n",
      "\n",
      "\n",
      "Submit Directory\n",
      "\n",
      "$PBS_O_WORKDIR\n",
      "\n",
      "$SLURM_SUBMIT_DIR\n",
      "\n",
      "\n",
      "Node List\n",
      "\n",
      "$PBS_NODEFILE\n",
      "\n",
      "$SLURM_JOB_NODELIST\n",
      "\n",
      "Job specification\n",
      "\n",
      "\n",
      "PBS/TORQUE\n",
      "\n",
      "Slurm\n",
      "\n",
      "\n",
      "Script directive\n",
      "\n",
      "#PBS\n",
      "\n",
      "#SBATCH\n",
      "\n",
      "\n",
      "Job Name\n",
      "\n",
      "-N [name]\n",
      "\n",
      "--job-name=[name] OR -J [name]\n",
      "\n",
      "\n",
      "Node Count\n",
      "\n",
      "-l nodes=[count]\n",
      "\n",
      "--nodes=[min[-max]] OR -N [min[-max]]\n",
      "\n",
      "\n",
      "CPU Count\n",
      "\n",
      "-l ppn=[count]\n",
      "\n",
      "--ntasks-per-node=[count]\n",
      "\n",
      "\n",
      "CPUs Per Task\n",
      "\n",
      "--cpus-per-task=[count]\n",
      "\n",
      "\n",
      "Memory Size\n",
      "\n",
      "-l mem=[MB]\n",
      "\n",
      "--mem=[MB] OR --mem-per-cpu=[MB]\n",
      "\n",
      "\n",
      "Wall Clock Limit\n",
      "\n",
      "-l walltime=[hh:mm:ss]\n",
      "\n",
      "--time=[min] OR --time=[days-hh:mm:ss]\n",
      "\n",
      "\n",
      "Node Properties\n",
      "\n",
      "-l nodes=4:ppn=8:[property]\n",
      "\n",
      "--constraint=[list]\n",
      "\n",
      "\n",
      "Standard Output File\n",
      "\n",
      "-o [file_name]\n",
      "\n",
      "--output=[file_name] OR -o [file_name]\n",
      "\n",
      "\n",
      "Standard Error File\n",
      "\n",
      "-e [file_name]\n",
      "\n",
      "--error=[file_name] OR -e [file_name]\n",
      "\n",
      "\n",
      "Combine stdout/stderr\n",
      "\n",
      "-j oe (both to stdout)\n",
      "\n",
      "(Default if you don't specify --error)\n",
      "\n",
      "\n",
      "Job Arrays\n",
      "\n",
      "-t [array_spec]\n",
      "\n",
      "--array=[array_spec] OR -a [array_spec]\n",
      "\n",
      "\n",
      "Delay Job Start\n",
      "\n",
      "-a [time]\n",
      "\n",
      "--begin=[time]\n",
      "\n",
      "Retrieved from \"https://wiki.umiacs.umd.edu/umiacs/index.php?title=SLURM&oldid=12148\"\n"
     ]
    }
   ],
   "source": [
    "#duplicate cell from above\n",
    "\n",
    "query = \"how do i run a python notebook in nexus?\"\n",
    "# query = \"nexus python notebook\"\n",
    "res = retriever.similarity_search_with_score(query, k=4)\n",
    "print([(x[0].metadata[\"name\"], x[1]) for x in res])\n",
    "\n",
    "embedding = embedding_model.client.encode(query, prompt_name=\"s2p_query\")\n",
    "res = retriever.similarity_search_with_score_by_vector(embedding, k=4)\n",
    "print(\"s2p embedding:\", [(x[0].metadata[\"name\"], x[1]) for x in res], sep=\"\\n\")\n",
    "# print(*[x[0].page_content for x in res], sep=\"\\n\\n\\n\\n\")\n",
    "# print(len(thing.similarity_search(\"nexus\", k=4)))\n",
    "# print(len(thing.similarity_search_with_score(\"nexus\", k=4, distance_threshold=.5)))\n",
    "context = [x[0].page_content for x in res]\n",
    "print(f\"Context:\\n\", *context, sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
