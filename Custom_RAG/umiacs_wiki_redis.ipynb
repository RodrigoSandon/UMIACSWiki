{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM7SgS8bmJPv",
        "outputId": "4e5ad29a-d26a-4eab-9e5e-be0fd06cee50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: redis in /usr/local/lib/python3.10/dist-packages (5.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.9)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.10)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.134)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install redis requests beautifulsoup4 sentence-transformers langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-community redis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8rSv8ISqObM",
        "outputId": "30b9368d-8aa9-4654-cf5a-39f0ab217a22"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.10/dist-packages (5.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.9)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.10)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.134)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis) (4.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.13.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.3->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain-community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.2-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.2 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import redis\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.vectorstores.redis import Redis\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Step 1: Connect to Redis Cloud (Replace with your Redis Cloud credentials)\n",
        "redis_host = \"redis-16332.c263.us-east-1-2.ec2.redns.redis-cloud.com\"\n",
        "redis_port = 16332  # Replace with your port\n",
        "redis_password = \"\"  # Replace with your password\n",
        "\n",
        "redis_conn = redis.StrictRedis(\n",
        "    host=redis_host,\n",
        "    port=redis_port,\n",
        "    password=redis_password,\n",
        "    decode_responses=True\n",
        ")\n",
        "redis_url = f\"redis://:{redis_password}@{redis_host}:{redis_port}\"\n",
        "\n",
        "# Step 2: Initialize the embedding model and vector store\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = Redis(\n",
        "    redis_url=redis_url,\n",
        "    embedding=embedding_model,\n",
        "    index_name=\"umiacs\"  # Optional: Specify an index name\n",
        ")\n",
        "\n",
        "\n",
        "# URL of the Special:AllPages page on the wiki\n",
        "all_pages_url = 'https://wiki.umiacs.umd.edu/umiacs/index.php/Special:AllPages'\n",
        "response = requests.get(all_pages_url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Create a set of unique pages on the wiki\n",
        "pageset = set()\n",
        "for link in soup.find_all('a'):\n",
        "    href = link.get('href')\n",
        "    # Ensure the link is valid and points to an internal page\n",
        "    if href and href.startswith('/umiacs/index.php'):\n",
        "        # Build the full URL for the wiki page\n",
        "        full_url = 'https://wiki.umiacs.umd.edu' + href\n",
        "        # Add the unique URL to the set, following redirects\n",
        "        response = requests.get(full_url, allow_redirects=True)\n",
        "        response_url = response.url\n",
        "        pageset.add(response_url)\n",
        "\n",
        "# List of URLs for HTML pages\n",
        "html_links = list(pageset)\n"
      ],
      "metadata": {
        "id": "9k_Y5F7FmYEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "MAX_TOKENS = 2048  # Max token limit for the model\n",
        "OVERLAP = 100  # Token overlap for better context\n",
        "\n",
        "# Step 2: Token-based chunking with overlap (ensures token limits are respected)\n",
        "def chunk_text_with_overlap(text, max_tokens=MAX_TOKENS, overlap=OVERLAP):\n",
        "    \"\"\"Chunks text within token limits with overlap for context retention.\"\"\"\n",
        "    tokens = tokenizer.tokenize(text)  # Tokenize the input text\n",
        "    chunks = []\n",
        "\n",
        "    # Create sliding window chunks with overlap\n",
        "    for i in range(0, len(tokens), max_tokens - overlap):\n",
        "        chunk_tokens = tokens[i:i + max_tokens]\n",
        "        chunk = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Step 3: Function to fetch HTML content\n",
        "def fetch_html_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to fetch {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 4: Chunk HTML content into smaller pieces\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        yield \" \".join(words[i:i + chunk_size])\n",
        "\n",
        "# Step 5: Process each page, generate embeddings, and store them in Redis\n",
        "def process_and_store_page(url):\n",
        "    html_content = fetch_html_from_url(url)\n",
        "    if html_content:\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "        text_content = soup.get_text(separator=\" \", strip=True)\n",
        "        chunks = chunk_text_with_overlap(text_content)\n",
        "        for chunk in chunks:\n",
        "            embedding = embedding_model.embed_query(chunk)\n",
        "            vector_store.add_texts([chunk], embeddings=[embedding])\n",
        "\n",
        "# Step 6: Process all pages and store embeddings in Redis\n",
        "for url in html_links:\n",
        "    process_and_store_page(url)\n",
        "\n",
        "print(\"All pages processed and embeddings stored in Redis!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVU6URZJs6xj",
        "outputId": "a06d911f-9475-4b38-ce41-ceaa472a4df0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2931 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch https://wiki.umiacs.umd.edu/umiacs/index.php/UMIACS:General_disclaimer: 404 Client Error: Not Found for url: https://wiki.umiacs.umd.edu/umiacs/index.php/UMIACS:General_disclaimer\n",
            "Failed to fetch https://wiki.umiacs.umd.edu/umiacs/index.php/UMIACS:Privacy_policy: 404 Client Error: Not Found for url: https://wiki.umiacs.umd.edu/umiacs/index.php/UMIACS:Privacy_policy\n",
            "All pages processed and embeddings stored in Redis!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate query embedding\n",
        "query = \"sbatch jobs\"\n",
        "query_embedding = embedding_model.embed_query(query)  # Ensure this is a valid vector\n",
        "\n",
        "# Step: Perform similarity search by vector\n",
        "try:\n",
        "    # Use the correct syntax for similarity search by vector\n",
        "    results = vector_store.similarity_search_by_vector(embedding=query_embedding, k=5)\n",
        "\n",
        "    # Correctly extract text from Document objects\n",
        "    for doc in results:\n",
        "        print(doc.page_content)  # Access the content of the Document object\n",
        "except Exception as e:\n",
        "    print(f\"Error during similarity search: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwJSjV5ywXW2",
        "outputId": "42b65f08-759c-4a5b-cacc-294a56860119"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": ss # sbatch - - qos = default # set qos, this will determine what resources can be requested # sbatch - - nodes = 2 # number of nodes to allocate for your job # sbatch - - ntasks = 4 # request 4 cpu cores be reserved for your node total # sbatch - - ntasks - per - node = 2 # request 2 cpu cores be reserved per node # sbatch - - mem = 1g # memory required by job ; if unit is not specified mb will be assumed. for multi - node jobs, this argument allocates this much memory * per node * srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # use srun to invoke commands within your job ; using an ' & ' srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # will background the process allowing them to run concurrently wait # wait for any background processes to complete # once the end of the batch script is reached your job allocation will be revoked another useful thing to know is that you can pass additional arguments into your sbatch scripts on the command line and reference them as $ { 1 } for the first argument and so on. more examples slurm / arrayjobs scancel the scancel command can be used to cancel job allocations or job steps that are no longer needed. it can be passed individual job ids or an option to delete all of your jobs or jobs that meet certain criteria. scancel 255 cancel job 255 scancel 255. 3 cancel job step 3 of job 255 scancel - - user username - - partition = tron cancel all jobs for username in the tron partition identifying resources and features the sinfo command can show you additional features of nodes in the cluster but you need to ask it to show some non - default options using a command like sinfo - o \" % 40n % 8c % 8m % 35f % 35g \". $ sinfo - o \" % 40n % 8c % 8m % 35f % 35g \" nodelist cpus memory avail _ features gres legacy00 48 125940 rhel8, zen, epyc - 7402 ( null ) legacy [ 01 - 11, 13 - 19, 22 - 28, 30 ] 12 + 61804 + rhel8, xeon, e5 - 2620 ( null ) cbcb [ 23 - 24 ], twist [ 02 - 05 ] 24 255150 rhel8, xeon, e5 - 2650 ( null ) cbcb26 128 513243 rhel8, zen, epyc - 7763, ampere gpu : rtxa5000 : 8 cbcb27 64 255167 rhel8, zen, epyc - 7513, ampere gpu : rtxa6000 : 8 cbcb [ 00 - 21 ] 32 2061175 rhel8, zen, epyc - 7313 ( null ) cbcb22, cmlcpu [ 00, 06 - 07 ], legacy20 24 + 384270 + rhel8, xeon, e5 - 2680 ( null ) cbcb25 24 255278 rhel8, xeon, e5 - 2650, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 1 legacy21 8 61746 rhel8, xeon, e5 - 2623 ( null ) tron [ 06 - 09, 12 - 15, 21 ] 16 126214 + rhel8, zen, epyc - 7302p, ampere gpu : rtxa4000 : 4 tron [ 10 - 11, 16 - 20, 34 ] 16 126217 rhel8, zen, epyc - 7313p, ampere gpu : rtxa4000 : 4 tron [ 22 - 33, 35 - 45 ] 16 126214 + rhel8, zen, epyc - 7302, ampere gpu : rtxa4000 : 4 clip11 16 126217 rhel8, zen, epyc - 7313, ampere gpu : rtxa4000 : 4 clip00 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 3 clip02 20 126255 rhel8, xeon, e5 - 2630, pascal gpu : gtx1080ti : 3 clip03 20 126243 rhel8, xeon, e5 - 2630, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 2 clip04 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtx3090 : 4 clip [ 05 - 06 ] 24 126216 rhel8, zen, epyc - 7352, ampere gpu : rtxa6000 : 2 clip07 8 255263 rhel8, xeon, e5 - 2623, pascal gpu : gtx1080ti : 3 clip09 32 383043 rhel8, xeon, 6130, pascal, turing gpu : rtx2080ti : 5, gpu : gtx1080ti : 3 clip13, cml30, vulcan [ 29 - 32 ] 32 255218 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 8 clip08, vulcan [ 08 - 22, 25 ] 32 255258 + rhel8, xeon, e5 - 2683, pascal gpu : gtx1080ti : 8 clip12, gammagpu [ 10 - 17 ] 16 126203 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 4 clip01 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 1, gpu : titanxp : 2 clip10 44 1029404 rhel8, xeon, e5 - 2699 ( null ) cml [ 00, 02 - 11, 13 - 14 ], tron [ 62 - 63, 65 - 66, 68 - 32 351530 + rhel8, xeon, 4216, turing gpu : rtx2080ti : 8 cml01 32 383030 rhel8, xeon, 4216, turing gpu : rtx2080ti : 6 cml12 32 383038 rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtxa4000 : 1 cml [ 15 - 16 ] 32 383038 rhel8, xeon, 4216, turing gpu : rtx2080ti : 7 cml [ 17 - 28 ], gammagpu05 32 255225 + rhel8, zen, epyc - 7282, ampere gpu : rtxa4000 : 8 cml31 32 384094 rhel8, zen, epyc - 9124, ampere gpu : a100 : 1 cml32 64 512999 rhel8, zen, epyc - 7543, ampere gpu : a100 : 4 cmlcpu [ 01 - 04 ] 20 384271 rhel8, xeon, e5 - 2660 ( null ) gammagpu00 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa5000 : 8 mbrc [ 00 - 01 ] 20 189498 rhel8, xeon, 4114, turing gpu : rtx2080ti : 8 twist [ 00 - 01 ] 8 61727 rhel8, xeon, e5 - 1660 ( null ) legacygpu08 20 513327 rhel8, xeon, e5 - 2640, maxwell gpu : m40 : 2 brigid [ 16 - 17 ] 48 512897 rhel8, zen, epyc - 7443 ( null ) brigid [ 18 - 19 ] 20 61739 rhel8, xeon, e5 - 2640 ( null ) legacygpu06 20 255249 rhel8, xeon, e5 - 2699, maxwell gpu : gtxtitanx : 4 tron [ 00 - 05 ] 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa6000 : 8 tron [ 46 - 61 ] 48 255232 rhel8, zen, epyc - 7352, ampere gpu : rtxa5000 : 8 tron [ 64, 67 ] 32 383028 + rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtx3070 : 1 vulcan00 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7, gpu : p100 : 1 vulcan [ 01 - 04, 06 - 07 ] 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 8 vulcan05 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7 janus [ 02 - 04 ] 40 3830\n",
            "; format = dd - hh : mm : ss # sbatch - - qos = default # set qos, this will determine what resources can be requested # sbatch - - nodes = 2 # number of nodes to allocate for your job # sbatch - - ntasks = 4 # request 4 cpu cores be reserved for your node total # sbatch - - ntasks - per - node = 2 # request 2 cpu cores be reserved per node # sbatch - - mem = 1g # memory required by job ; if unit is not specified mb will be assumed. for multi - node jobs, this argument allocates this much memory * per node * srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # use srun to invoke commands within your job ; using an ' & ' srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # will background the process allowing them to run concurrently wait # wait for any background processes to complete # once the end of the batch script is reached your job allocation will be revoked another useful thing to know is that you can pass additional arguments into your sbatch scripts on the command line and reference them as $ { 1 } for the first argument and so on. more examples slurm / arrayjobs scancel the scancel command can be used to cancel job allocations or job steps that are no longer needed. it can be passed individual job ids or an option to delete all of your jobs or jobs that meet certain criteria. scancel 255 cancel job 255 scancel 255. 3 cancel job step 3 of job 255 scancel - - user username - - partition = tron cancel all jobs for username in the tron partition identifying resources and features the sinfo command can show you additional features of nodes in the cluster but you need to ask it to show some non - default options using a command like sinfo - o \" % 40n % 8c % 8m % 35f % 35g \". $ sinfo - o \" % 40n % 8c % 8m % 35f % 35g \" nodelist cpus memory avail _ features gres legacy00 48 125940 rhel8, zen, epyc - 7402 ( null ) legacy [ 01 - 11, 13 - 19, 22 - 28, 30 ] 12 + 61804 + rhel8, xeon, e5 - 2620 ( null ) cbcb [ 23 - 24 ], twist [ 02 - 05 ] 24 255150 rhel8, xeon, e5 - 2650 ( null ) cbcb26 128 513243 rhel8, zen, epyc - 7763, ampere gpu : rtxa5000 : 8 cbcb27 64 255167 rhel8, zen, epyc - 7513, ampere gpu : rtxa6000 : 8 cbcb [ 00 - 21 ] 32 2061175 rhel8, zen, epyc - 7313 ( null ) cbcb22, cmlcpu [ 00, 06 - 07 ], legacy20 24 + 384270 + rhel8, xeon, e5 - 2680 ( null ) cbcb25 24 255278 rhel8, xeon, e5 - 2650, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 1 legacy21 8 61746 rhel8, xeon, e5 - 2623 ( null ) tron [ 06 - 09, 12 - 15, 21 ] 16 126214 + rhel8, zen, epyc - 7302p, ampere gpu : rtxa4000 : 4 tron [ 10 - 11, 16 - 20, 34 ] 16 126217 rhel8, zen, epyc - 7313p, ampere gpu : rtxa4000 : 4 tron [ 22 - 33, 35 - 45 ] 16 126214 + rhel8, zen, epyc - 7302, ampere gpu : rtxa4000 : 4 clip11 16 126217 rhel8, zen, epyc - 7313, ampere gpu : rtxa4000 : 4 clip00 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 3 clip02 20 126255 rhel8, xeon, e5 - 2630, pascal gpu : gtx1080ti : 3 clip03 20 126243 rhel8, xeon, e5 - 2630, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 2 clip04 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtx3090 : 4 clip [ 05 - 06 ] 24 126216 rhel8, zen, epyc - 7352, ampere gpu : rtxa6000 : 2 clip07 8 255263 rhel8, xeon, e5 - 2623, pascal gpu : gtx1080ti : 3 clip09 32 383043 rhel8, xeon, 6130, pascal, turing gpu : rtx2080ti : 5, gpu : gtx1080ti : 3 clip13, cml30, vulcan [ 29 - 32 ] 32 255218 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 8 clip08, vulcan [ 08 - 22, 25 ] 32 255258 + rhel8, xeon, e5 - 2683, pascal gpu : gtx1080ti : 8 clip12, gammagpu [ 10 - 17 ] 16 126203 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 4 clip01 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 1, gpu : titanxp : 2 clip10 44 1029404 rhel8, xeon, e5 - 2699 ( null ) cml [ 00, 02 - 11, 13 - 14 ], tron [ 62 - 63, 65 - 66, 68 - 32 351530 + rhel8, xeon, 4216, turing gpu : rtx2080ti : 8 cml01 32 383030 rhel8, xeon, 4216, turing gpu : rtx2080ti : 6 cml12 32 383038 rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtxa4000 : 1 cml [ 15 - 16 ] 32 383038 rhel8, xeon, 4216, turing gpu : rtx2080ti : 7 cml [ 17 - 28 ], gammagpu05 32 255225 + rhel8, zen, epyc - 7282, ampere gpu : rtxa4000 : 8 cml31 32 384094 rhel8, zen, epyc - 9124, ampere gpu : a100 : 1 cml32 64 512999 rhel8, zen, epyc - 7543, ampere gpu : a100 : 4 cmlcpu [ 01 - 04 ] 20 384271 rhel8, xeon, e5 - 2660 ( null ) gammagpu00 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa5000 : 8 mbrc [ 00 - 01 ] 20 189498 rhel8, xeon, 4114, turing gpu : rtx2080ti : 8 twist [ 00 - 01 ] 8 61727 rhel8, xeon, e5 - 1660 ( null ) legacygpu08 20 513327 rhel8, xeon, e5 - 2640, maxwell gpu : m40 : 2 brigid [ 16 - 17 ] 48 512897 rhel8, zen, epyc - 7443 ( null ) brigid [ 18 - 19 ] 20 61739 rhel8, xeon, e5 - 2640 ( null ) legacygpu06 20 255249 rhel8, xeon, e5 - 2699, maxwell gpu : gtxtitanx : 4 tron [ 00 - 05 ] 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa6000 : 8 tron [ 46 - 61 ] 48 255232 rhel8, zen, epyc - 7352, ampere gpu : rtxa5000 : 8 tron [ 64, 67 ] 32 383028 + rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtx3070 : 1 vulcan00 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7, gpu : p100 : 1 vulcan [ 01 - 04, 06 - 07 ] 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 8 vulcan05 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7 jan\n",
            "SLURM/ArrayJobs - UMIACS SLURM/ArrayJobs From UMIACS Jump to navigation Jump to search Here is an example to get you started using array jobs in SLURM . Array computation example job Save this code to a file called test.py . import time print('start at ' + time.strftime('%H:%M:%S')) print('sleep for 10 seconds ...') time.sleep(10) print('stop at ' + time.strftime('%H:%M:%S')) Submission script Save this to a file called array.sh and you should be able to submit the job as sbatch array.sh . #!/bin/bash ##################### # job-array example # ##################### #SBATCH --job-name=example #SBATCH --array=1-16 # run 16 jobs at the same time #SBATCH --time=0-00:05:00 # run for 5 minutes (d-hh:mm:ss) #SBATCH --mem-per-cpu=500MB # use 500MB per core # all bash commands must be after all SBATCH directives # define and create a unique scratch directory SCRATCH_DIRECTORY=/scratch0/${USER}/job-array-example/${SLURM_JOBID} mkdir -p ${SCRATCH_DIRECTORY} cd ${SCRATCH_DIRECTORY} cp ${SLURM_SUBMIT_DIR}/test.py ${SCRATCH_DIRECTORY} # each job will see a different ${SLURM_ARRAY_TASK_ID} echo \"now processing task id:: \" ${SLURM_ARRAY_TASK_ID} python test.py > output_${SLURM_ARRAY_TASK_ID}.txt # after the job is done we copy our output back to $SLURM_SUBMIT_DIR cp output_${SLURM_ARRAY_TASK_ID}.txt ${SLURM_SUBMIT_DIR} # we step out of the scratch directory and remove it cd ${SLURM_SUBMIT_DIR} rm -rf ${SCRATCH_DIRECTORY} # happy end exit 0 Retrieved from \" https://wiki.umiacs.umd.edu/umiacs/index.php?title=SLURM/ArrayJobs&oldid=11731 \" Navigation menu Personal tools Log in Namespaces Page Discussion English Views Read View source View history More Search Navigation Main Page Getting Started Core Services Lab Facilities Placing Orders Support Tools What links here Related changes Special pages Printable version Permanent link Page information This page was last edited on 28 March 2024, at 20:30. Privacy policy About UMIACS Disclaimers\n",
            "slurm / arrayjobs - umiacs slurm / arrayjobs from umiacs jump to navigation jump to search here is an example to get you started using array jobs in slurm. array computation example job save this code to a file called test. py. import time print ( ' start at ' + time. strftime ( ' % h : % m : % s ' ) ) print ( ' sleep for 10 seconds... ' ) time. sleep ( 10 ) print ( ' stop at ' + time. strftime ( ' % h : % m : % s ' ) ) submission script save this to a file called array. sh and you should be able to submit the job as sbatch array. sh. #! / bin / bash # # # # # # # # # # # # # # # # # # # # # # job - array example # # # # # # # # # # # # # # # # # # # # # # # sbatch - - job - name = example # sbatch - - array = 1 - 16 # run 16 jobs at the same time # sbatch - - time = 0 - 00 : 05 : 00 # run for 5 minutes ( d - hh : mm : ss ) # sbatch - - mem - per - cpu = 500mb # use 500mb per core # all bash commands must be after all sbatch directives # define and create a unique scratch directory scratch _ directory = / scratch0 / $ { user } / job - array - example / $ { slurm _ jobid } mkdir - p $ { scratch _ directory } cd $ { scratch _ directory } cp $ { slurm _ submit _ dir } / test. py $ { scratch _ directory } # each job will see a different $ { slurm _ array _ task _ id } echo \" now processing task id : : \" $ { slurm _ array _ task _ id } python test. py > output _ $ { slurm _ array _ task _ id }. txt # after the job is done we copy our output back to $ slurm _ submit _ dir cp output _ $ { slurm _ array _ task _ id }. txt $ { slurm _ submit _ dir } # we step out of the scratch directory and remove it cd $ { slurm _ submit _ dir } rm - rf $ { scratch _ directory } # happy end exit 0 retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / arrayjobs & oldid = 11731 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 28 march 2024, at 20 : 30. privacy policy about umiacs disclaimers\n",
            "--nodes=2 Requests 2 nodes be allocated to your job; if passed to srun, runs the given command concurrently on each node --nodelist=<NODENAME> Requests to run your job on the <NODENAME> node --time=dd-hh:mm:ss Requests your job run for dd days, hh hours, mm minutes, and ss seconds --error=<ERRNAME> Redirects stderr for your job to the <ERRNAME> file --partition=<PARTITIONNAME> Requests your job run in the <PARTITIONNAME> partition --qos=<QOSNAME>default Requests your job run with the <QOSNAME> QOS, to see the available QOS options on a cluster, run show_qos --account=<ACCOUNTNAME> Requests your job runs under the <ACCOUNTNAME> Slurm account, different accounts have different available partitions/QOS --output=<OUTNAME> Redirects stdout for your job to the <OUTNAME> file --requeue Requests your job be automatically requeued if it is preempted --exclusive Requests your job be the only one running on the node(s) it is assigned to. This requires that your job be allocated all of the resources on the node(s). The scheduler does not automatically give your job all of the node's/nodes' resources, however, so if you need more than the default, you still need to request these with --ntasks and --mem Interactive Shell Sessions An interactive shell session on a compute node can be useful for debugging or developing code that isn't ready to be run as a batch job. To get an interactive shell on a node, use srun with the --pty argument to invoke a shell: $ srun --pty --qos=default --mem=1g --time=01:00:00 bash $ hostname tron33.umiacs.umd.edu Please do not leave interactive shells running for long periods of time when you are not working. This blocks resources from being used by everyone else. salloc The salloc command can also be used to request resources be allocated without needing a batch script. Running salloc with a list of resources will allocate the resources you requested, create a job, and drop you into a subshell with the environment variables necessary to run commands in the newly created job allocation. When your time is up or you exit the subshell, your job allocation will be relinquished. $ salloc --qos=default -N 1 --mem=2g --time=01:00:00 salloc: Granted job allocation 159 $ srun /usr/bin/hostname tron33.umiacs.umd.edu $ exit exit salloc: Relinquishing job allocation 159 Please note that any commands not invoked with srun will be run locally on the submit node. Please be careful when using salloc. sbatch The sbatch command allows you to write a batch script to be submitted and run non-interactively on the compute nodes. To run a simple Hello World command on the compute nodes you could write a file, helloWorld.sh with the following contents: #!/bin/bash srun bash -c 'echo Hello World from `hostname`' Then you need to submit the script with sbatch and request resources: $ sbatch --qos=default --mem=1g --time=1:00:00 helloWorld.sh Submitted batch job 121 SLURM will return a job number that you can use to check the status of your job with squeue: $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 121 tron helloWor username R 0:01 1 tron32 Advanced Batch Scripts You can also\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In case we want to delete the vectors"
      ],
      "metadata": {
        "id": "0bA51j0pN2UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_all_vectors(redis_conn, index_name):\n",
        "    \"\"\"Deletes all keys in the given Redis namespace.\"\"\"\n",
        "    try:\n",
        "        # Use SCAN to retrieve all keys that belong to the index namespace\n",
        "        keys = redis_conn.scan_iter(f\"{index_name}:*\")\n",
        "        for key in keys:\n",
        "            redis_conn.delete(key)  # Delete each key\n",
        "        print(f\"All vectors from index '{index_name}' have been deleted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting vectors: {e}\")\n",
        "\n",
        "# Example usage\n",
        "delete_all_vectors(redis_conn, \"umiacs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8KQyODR4OHB",
        "outputId": "d27097f3-9438-4c24-ee55-38a2ceb64d27"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All vectors from index 'umiacs' have been deleted.\n"
          ]
        }
      ]
    }
  ]
}