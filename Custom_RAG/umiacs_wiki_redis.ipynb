{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM7SgS8bmJPv",
        "outputId": "4e5ad29a-d26a-4eab-9e5e-be0fd06cee50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: redis in /opt/anaconda3/envs/rag/lib/python3.13/site-packages (3.5.3)\n",
            "Collecting requests\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests)\n",
            "  Downloading charset_normalizer-3.4.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests)\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests)\n",
            "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting tqdm (from sentence-transformers)\n",
            "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading sentence_transformers-3.1.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.6.0-py3-none-any.whl.metadata (11 kB)\n",
            "INFO: pip is still looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Downloading sentence-transformers-2.2.1.tar.gz (84 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting tokenizers>=0.10.3 (from sentence-transformers)\n",
            "  Downloading tokenizers-0.20.1.tar.gz (339 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m Checking for Rust toolchain....\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Cargo, the Rust package manager, is not installed or is not on PATH.\n",
            "  \u001b[31m   \u001b[0m This package requires Rust and Cargo to compile extensions. Install it through\n",
            "  \u001b[31m   \u001b[0m the system's package manager or via https://rustup.rs/\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install redis requests beautifulsoup4 sentence-transformers langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8rSv8ISqObM",
        "outputId": "30b9368d-8aa9-4654-cf5a-39f0ab217a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: redis in /opt/anaconda3/envs/rag/lib/python3.13/site-packages (3.5.3)\n",
            "Collecting redis\n",
            "  Downloading redis-5.1.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain-community)\n",
            "  Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain-community)\n",
            "  Downloading SQLAlchemy-2.0.36-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Downloading aiohttp-3.10.10-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.3 (from langchain-community)\n",
            "  Using cached langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.10 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.12-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-community)\n",
            "  Downloading langsmith-0.1.135-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from langchain-community)\n",
            "  Downloading numpy-1.26.4.tar.gz (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting requests<3,>=2 (from langchain-community)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-community)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading frozenlist-1.4.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading multidict-6.1.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
            "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading yarl-1.15.4-cp313-cp313-macosx_11_0_arm64.whl.metadata (58 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.3->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pydantic<3.0.0,>=2.7.4 (from langchain<0.4.0,>=0.3.3->langchain-community)\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.10->langchain-community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/rag/lib/python3.13/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/rag/lib/python3.13/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading orjson-3.10.7-cp313-cp313-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain-community)\n",
            "  Using cached charset_normalizer-3.4.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain-community)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain-community)\n",
            "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain-community)\n",
            "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting sniffio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain-community)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain-community)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain-community)\n",
            "  Downloading pydantic_core-2.23.4-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading propcache-0.2.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
            "Downloading langchain_community-0.3.2-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading redis-5.1.1-py3-none-any.whl (261 kB)\n",
            "Downloading aiohttp-3.10.10-cp313-cp313-macosx_11_0_arm64.whl (387 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.12-py3-none-any.whl (407 kB)\n",
            "Downloading langsmith-0.1.135-py3-none-any.whl (295 kB)\n",
            "Downloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\n",
            "Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading SQLAlchemy-2.0.36-cp313-cp313-macosx_11_0_arm64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "Downloading charset_normalizer-3.4.0-cp313-cp313-macosx_11_0_arm64.whl (119 kB)\n",
            "Downloading frozenlist-1.4.1-py3-none-any.whl (11 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
            "Downloading multidict-6.1.0-cp313-cp313-macosx_11_0_arm64.whl (29 kB)\n",
            "Downloading orjson-3.10.7-cp313-cp313-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (251 kB)\n",
            "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "Downloading pydantic_core-2.23.4-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Downloading yarl-1.15.4-cp313-cp313-macosx_11_0_arm64.whl (86 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading propcache-0.2.0-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
            "Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Building wheels for collected packages: numpy\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for numpy: filename=numpy-1.26.4-cp313-cp313-macosx_14_0_arm64.whl size=4680545 sha256=11b88248c73ed577e92d2f96d2bdd0f3e8148b4e6d813a2ce5a824a40036afd2\n",
            "  Stored in directory: /Users/rodrigosandon/Library/Caches/pip/wheels/8b/2d/9f/b6b46373f328e2ef50388915d351ccacbedac929459b5459bf\n",
            "Successfully built numpy\n",
            "Installing collected packages: urllib3, tenacity, SQLAlchemy, sniffio, redis, PyYAML, python-dotenv, pydantic-core, propcache, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, idna, h11, frozenlist, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, requests, pydantic, jsonpatch, httpcore, anyio, aiosignal, requests-toolbelt, pydantic-settings, httpx, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: redis\n",
            "    Found existing installation: redis 3.5.3\n",
            "    Uninstalling redis-3.5.3:\n",
            "      Successfully uninstalled redis-3.5.3\n",
            "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.2.post1 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 dataclasses-json-0.6.7 frozenlist-1.4.1 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.3 langchain-community-0.3.2 langchain-core-0.3.12 langchain-text-splitters-0.3.0 langsmith-0.1.135 marshmallow-3.23.0 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.7 propcache-0.2.0 pydantic-2.9.2 pydantic-core-2.23.4 pydantic-settings-2.6.0 python-dotenv-1.0.1 redis-5.1.1 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-8.5.0 typing-inspect-0.9.0 urllib3-2.2.3 yarl-1.15.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -U langchain-community redis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k_Y5F7FmYEw"
      },
      "outputs": [],
      "source": [
        "import redis\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.vectorstores.redis import Redis\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Step 1: Connect to Redis Cloud (Replace with your Redis Cloud credentials)\n",
        "redis_host = \"redis-14406.c259.us-central1-2.gce.redns.redis-cloud.com\"\n",
        "redis_port = 14406  # Replace with your port\n",
        "redis_password = \"daZbJ7qv45e3WBwo8yzrSI8rXuN4foHX\"  # Replace with your password\n",
        "\n",
        "redis_conn = redis.StrictRedis(\n",
        "    host=redis_host,\n",
        "    port=redis_port,\n",
        "    password=redis_password,\n",
        "    decode_responses=True\n",
        ")\n",
        "redis_url = f\"redis://:{redis_password}@{redis_host}:{redis_port}\"\n",
        "\n",
        "# Step 2: Initialize the embedding model and vector store\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = Redis(\n",
        "    redis_url=redis_url,\n",
        "    embedding=embedding_model,\n",
        "    index_name=\"umiacs\"  # Optional: Specify an index name\n",
        ")\n",
        "\n",
        "\n",
        "# Scraping all the links through one page: URL of the Special:AllPages page on the wiki\n",
        "\n",
        "all_pages_url = 'https://wiki.umiacs.umd.edu/umiacs/index.php/Special:AllPages'\n",
        "response = requests.get(all_pages_url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Create a set of unique pages on the wiki\n",
        "pageset = set()\n",
        "for link in soup.find_all('a'):\n",
        "    href = link.get('href')\n",
        "    # Ensure the link is valid and points to an internal page\n",
        "    if href and href.startswith('/umiacs/index.php'):\n",
        "        # Build the full URL for the wiki page\n",
        "        full_url = 'https://wiki.umiacs.umd.edu' + href\n",
        "        # Add the unique URL to the set, following redirects\n",
        "        response = requests.get(full_url, allow_redirects=True)\n",
        "        response_url = response.url\n",
        "        pageset.add(response_url)\n",
        "\n",
        "# List of URLs for HTML pages\n",
        "html_links = list(pageset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVU6URZJs6xj",
        "outputId": "a06d911f-9475-4b38-ce41-ceaa472a4df0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2931 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch https://wiki.umiacs.umd.edu/umiacs/index.php/UMIACS:General_disclaimer: 404 Client Error: Not Found for url: https://wiki.umiacs.umd.edu/umiacs/index.php/UMIACS:General_disclaimer\n",
            "Failed to fetch https://wiki.umiacs.umd.edu/umiacs/index.php/UMIACS:Privacy_policy: 404 Client Error: Not Found for url: https://wiki.umiacs.umd.edu/umiacs/index.php/UMIACS:Privacy_policy\n",
            "All pages processed and embeddings stored in Redis!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "MAX_TOKENS = 2048  # Max token limit for the model\n",
        "OVERLAP = 100  # Token overlap for better context\n",
        "\n",
        "# Step 2: Token-based chunking with overlap (ensures token limits are respected)\n",
        "def chunk_text_with_overlap(text, max_tokens=MAX_TOKENS, overlap=OVERLAP):\n",
        "    \"\"\"Chunks text within token limits with overlap for context retention.\"\"\"\n",
        "    tokens = tokenizer.tokenize(text)  # Tokenize the input text\n",
        "    chunks = []\n",
        "\n",
        "    # Create sliding window chunks with overlap\n",
        "    for i in range(0, len(tokens), max_tokens - overlap):\n",
        "        chunk_tokens = tokens[i:i + max_tokens]\n",
        "        chunk = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Step 3: Function to fetch HTML content\n",
        "def fetch_html_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to fetch {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 4: Chunk HTML content into smaller pieces\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        yield \" \".join(words[i:i + chunk_size])\n",
        "\n",
        "# Step 5: Process each page, generate embeddings, and store them in Redis\n",
        "def process_and_store_page(url):\n",
        "    html_content = fetch_html_from_url(url)\n",
        "    if html_content:\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "        text_content = soup.get_text(separator=\" \", strip=True)\n",
        "        chunks = chunk_text_with_overlap(text_content)\n",
        "        for chunk in chunks:\n",
        "            embedding = embedding_model.embed_query(chunk)\n",
        "            vector_store.add_texts([chunk], embeddings=[embedding])\n",
        "\n",
        "# Step 6: Process all pages and store embeddings in Redis\n",
        "for url in html_links:\n",
        "    process_and_store_page(url)\n",
        "\n",
        "print(\"All pages processed and embeddings stored in Redis!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwJSjV5ywXW2",
        "outputId": "42b65f08-759c-4a5b-cacc-294a56860119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ": ss # sbatch - - qos = default # set qos, this will determine what resources can be requested # sbatch - - nodes = 2 # number of nodes to allocate for your job # sbatch - - ntasks = 4 # request 4 cpu cores be reserved for your node total # sbatch - - ntasks - per - node = 2 # request 2 cpu cores be reserved per node # sbatch - - mem = 1g # memory required by job ; if unit is not specified mb will be assumed. for multi - node jobs, this argument allocates this much memory * per node * srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # use srun to invoke commands within your job ; using an ' & ' srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # will background the process allowing them to run concurrently wait # wait for any background processes to complete # once the end of the batch script is reached your job allocation will be revoked another useful thing to know is that you can pass additional arguments into your sbatch scripts on the command line and reference them as $ { 1 } for the first argument and so on. more examples slurm / arrayjobs scancel the scancel command can be used to cancel job allocations or job steps that are no longer needed. it can be passed individual job ids or an option to delete all of your jobs or jobs that meet certain criteria. scancel 255 cancel job 255 scancel 255. 3 cancel job step 3 of job 255 scancel - - user username - - partition = tron cancel all jobs for username in the tron partition identifying resources and features the sinfo command can show you additional features of nodes in the cluster but you need to ask it to show some non - default options using a command like sinfo - o \" % 40n % 8c % 8m % 35f % 35g \". $ sinfo - o \" % 40n % 8c % 8m % 35f % 35g \" nodelist cpus memory avail _ features gres legacy00 48 125940 rhel8, zen, epyc - 7402 ( null ) legacy [ 01 - 11, 13 - 19, 22 - 28, 30 ] 12 + 61804 + rhel8, xeon, e5 - 2620 ( null ) cbcb [ 23 - 24 ], twist [ 02 - 05 ] 24 255150 rhel8, xeon, e5 - 2650 ( null ) cbcb26 128 513243 rhel8, zen, epyc - 7763, ampere gpu : rtxa5000 : 8 cbcb27 64 255167 rhel8, zen, epyc - 7513, ampere gpu : rtxa6000 : 8 cbcb [ 00 - 21 ] 32 2061175 rhel8, zen, epyc - 7313 ( null ) cbcb22, cmlcpu [ 00, 06 - 07 ], legacy20 24 + 384270 + rhel8, xeon, e5 - 2680 ( null ) cbcb25 24 255278 rhel8, xeon, e5 - 2650, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 1 legacy21 8 61746 rhel8, xeon, e5 - 2623 ( null ) tron [ 06 - 09, 12 - 15, 21 ] 16 126214 + rhel8, zen, epyc - 7302p, ampere gpu : rtxa4000 : 4 tron [ 10 - 11, 16 - 20, 34 ] 16 126217 rhel8, zen, epyc - 7313p, ampere gpu : rtxa4000 : 4 tron [ 22 - 33, 35 - 45 ] 16 126214 + rhel8, zen, epyc - 7302, ampere gpu : rtxa4000 : 4 clip11 16 126217 rhel8, zen, epyc - 7313, ampere gpu : rtxa4000 : 4 clip00 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 3 clip02 20 126255 rhel8, xeon, e5 - 2630, pascal gpu : gtx1080ti : 3 clip03 20 126243 rhel8, xeon, e5 - 2630, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 2 clip04 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtx3090 : 4 clip [ 05 - 06 ] 24 126216 rhel8, zen, epyc - 7352, ampere gpu : rtxa6000 : 2 clip07 8 255263 rhel8, xeon, e5 - 2623, pascal gpu : gtx1080ti : 3 clip09 32 383043 rhel8, xeon, 6130, pascal, turing gpu : rtx2080ti : 5, gpu : gtx1080ti : 3 clip13, cml30, vulcan [ 29 - 32 ] 32 255218 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 8 clip08, vulcan [ 08 - 22, 25 ] 32 255258 + rhel8, xeon, e5 - 2683, pascal gpu : gtx1080ti : 8 clip12, gammagpu [ 10 - 17 ] 16 126203 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 4 clip01 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 1, gpu : titanxp : 2 clip10 44 1029404 rhel8, xeon, e5 - 2699 ( null ) cml [ 00, 02 - 11, 13 - 14 ], tron [ 62 - 63, 65 - 66, 68 - 32 351530 + rhel8, xeon, 4216, turing gpu : rtx2080ti : 8 cml01 32 383030 rhel8, xeon, 4216, turing gpu : rtx2080ti : 6 cml12 32 383038 rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtxa4000 : 1 cml [ 15 - 16 ] 32 383038 rhel8, xeon, 4216, turing gpu : rtx2080ti : 7 cml [ 17 - 28 ], gammagpu05 32 255225 + rhel8, zen, epyc - 7282, ampere gpu : rtxa4000 : 8 cml31 32 384094 rhel8, zen, epyc - 9124, ampere gpu : a100 : 1 cml32 64 512999 rhel8, zen, epyc - 7543, ampere gpu : a100 : 4 cmlcpu [ 01 - 04 ] 20 384271 rhel8, xeon, e5 - 2660 ( null ) gammagpu00 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa5000 : 8 mbrc [ 00 - 01 ] 20 189498 rhel8, xeon, 4114, turing gpu : rtx2080ti : 8 twist [ 00 - 01 ] 8 61727 rhel8, xeon, e5 - 1660 ( null ) legacygpu08 20 513327 rhel8, xeon, e5 - 2640, maxwell gpu : m40 : 2 brigid [ 16 - 17 ] 48 512897 rhel8, zen, epyc - 7443 ( null ) brigid [ 18 - 19 ] 20 61739 rhel8, xeon, e5 - 2640 ( null ) legacygpu06 20 255249 rhel8, xeon, e5 - 2699, maxwell gpu : gtxtitanx : 4 tron [ 00 - 05 ] 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa6000 : 8 tron [ 46 - 61 ] 48 255232 rhel8, zen, epyc - 7352, ampere gpu : rtxa5000 : 8 tron [ 64, 67 ] 32 383028 + rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtx3070 : 1 vulcan00 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7, gpu : p100 : 1 vulcan [ 01 - 04, 06 - 07 ] 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 8 vulcan05 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7 janus [ 02 - 04 ] 40 3830\n",
            "; format = dd - hh : mm : ss # sbatch - - qos = default # set qos, this will determine what resources can be requested # sbatch - - nodes = 2 # number of nodes to allocate for your job # sbatch - - ntasks = 4 # request 4 cpu cores be reserved for your node total # sbatch - - ntasks - per - node = 2 # request 2 cpu cores be reserved per node # sbatch - - mem = 1g # memory required by job ; if unit is not specified mb will be assumed. for multi - node jobs, this argument allocates this much memory * per node * srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # use srun to invoke commands within your job ; using an ' & ' srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # will background the process allowing them to run concurrently wait # wait for any background processes to complete # once the end of the batch script is reached your job allocation will be revoked another useful thing to know is that you can pass additional arguments into your sbatch scripts on the command line and reference them as $ { 1 } for the first argument and so on. more examples slurm / arrayjobs scancel the scancel command can be used to cancel job allocations or job steps that are no longer needed. it can be passed individual job ids or an option to delete all of your jobs or jobs that meet certain criteria. scancel 255 cancel job 255 scancel 255. 3 cancel job step 3 of job 255 scancel - - user username - - partition = tron cancel all jobs for username in the tron partition identifying resources and features the sinfo command can show you additional features of nodes in the cluster but you need to ask it to show some non - default options using a command like sinfo - o \" % 40n % 8c % 8m % 35f % 35g \". $ sinfo - o \" % 40n % 8c % 8m % 35f % 35g \" nodelist cpus memory avail _ features gres legacy00 48 125940 rhel8, zen, epyc - 7402 ( null ) legacy [ 01 - 11, 13 - 19, 22 - 28, 30 ] 12 + 61804 + rhel8, xeon, e5 - 2620 ( null ) cbcb [ 23 - 24 ], twist [ 02 - 05 ] 24 255150 rhel8, xeon, e5 - 2650 ( null ) cbcb26 128 513243 rhel8, zen, epyc - 7763, ampere gpu : rtxa5000 : 8 cbcb27 64 255167 rhel8, zen, epyc - 7513, ampere gpu : rtxa6000 : 8 cbcb [ 00 - 21 ] 32 2061175 rhel8, zen, epyc - 7313 ( null ) cbcb22, cmlcpu [ 00, 06 - 07 ], legacy20 24 + 384270 + rhel8, xeon, e5 - 2680 ( null ) cbcb25 24 255278 rhel8, xeon, e5 - 2650, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 1 legacy21 8 61746 rhel8, xeon, e5 - 2623 ( null ) tron [ 06 - 09, 12 - 15, 21 ] 16 126214 + rhel8, zen, epyc - 7302p, ampere gpu : rtxa4000 : 4 tron [ 10 - 11, 16 - 20, 34 ] 16 126217 rhel8, zen, epyc - 7313p, ampere gpu : rtxa4000 : 4 tron [ 22 - 33, 35 - 45 ] 16 126214 + rhel8, zen, epyc - 7302, ampere gpu : rtxa4000 : 4 clip11 16 126217 rhel8, zen, epyc - 7313, ampere gpu : rtxa4000 : 4 clip00 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 3 clip02 20 126255 rhel8, xeon, e5 - 2630, pascal gpu : gtx1080ti : 3 clip03 20 126243 rhel8, xeon, e5 - 2630, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 2 clip04 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtx3090 : 4 clip [ 05 - 06 ] 24 126216 rhel8, zen, epyc - 7352, ampere gpu : rtxa6000 : 2 clip07 8 255263 rhel8, xeon, e5 - 2623, pascal gpu : gtx1080ti : 3 clip09 32 383043 rhel8, xeon, 6130, pascal, turing gpu : rtx2080ti : 5, gpu : gtx1080ti : 3 clip13, cml30, vulcan [ 29 - 32 ] 32 255218 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 8 clip08, vulcan [ 08 - 22, 25 ] 32 255258 + rhel8, xeon, e5 - 2683, pascal gpu : gtx1080ti : 8 clip12, gammagpu [ 10 - 17 ] 16 126203 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 4 clip01 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 1, gpu : titanxp : 2 clip10 44 1029404 rhel8, xeon, e5 - 2699 ( null ) cml [ 00, 02 - 11, 13 - 14 ], tron [ 62 - 63, 65 - 66, 68 - 32 351530 + rhel8, xeon, 4216, turing gpu : rtx2080ti : 8 cml01 32 383030 rhel8, xeon, 4216, turing gpu : rtx2080ti : 6 cml12 32 383038 rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtxa4000 : 1 cml [ 15 - 16 ] 32 383038 rhel8, xeon, 4216, turing gpu : rtx2080ti : 7 cml [ 17 - 28 ], gammagpu05 32 255225 + rhel8, zen, epyc - 7282, ampere gpu : rtxa4000 : 8 cml31 32 384094 rhel8, zen, epyc - 9124, ampere gpu : a100 : 1 cml32 64 512999 rhel8, zen, epyc - 7543, ampere gpu : a100 : 4 cmlcpu [ 01 - 04 ] 20 384271 rhel8, xeon, e5 - 2660 ( null ) gammagpu00 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa5000 : 8 mbrc [ 00 - 01 ] 20 189498 rhel8, xeon, 4114, turing gpu : rtx2080ti : 8 twist [ 00 - 01 ] 8 61727 rhel8, xeon, e5 - 1660 ( null ) legacygpu08 20 513327 rhel8, xeon, e5 - 2640, maxwell gpu : m40 : 2 brigid [ 16 - 17 ] 48 512897 rhel8, zen, epyc - 7443 ( null ) brigid [ 18 - 19 ] 20 61739 rhel8, xeon, e5 - 2640 ( null ) legacygpu06 20 255249 rhel8, xeon, e5 - 2699, maxwell gpu : gtxtitanx : 4 tron [ 00 - 05 ] 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa6000 : 8 tron [ 46 - 61 ] 48 255232 rhel8, zen, epyc - 7352, ampere gpu : rtxa5000 : 8 tron [ 64, 67 ] 32 383028 + rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtx3070 : 1 vulcan00 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7, gpu : p100 : 1 vulcan [ 01 - 04, 06 - 07 ] 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 8 vulcan05 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7 jan\n",
            "SLURM/ArrayJobs - UMIACS SLURM/ArrayJobs From UMIACS Jump to navigation Jump to search Here is an example to get you started using array jobs in SLURM . Array computation example job Save this code to a file called test.py . import time print('start at ' + time.strftime('%H:%M:%S')) print('sleep for 10 seconds ...') time.sleep(10) print('stop at ' + time.strftime('%H:%M:%S')) Submission script Save this to a file called array.sh and you should be able to submit the job as sbatch array.sh . #!/bin/bash ##################### # job-array example # ##################### #SBATCH --job-name=example #SBATCH --array=1-16 # run 16 jobs at the same time #SBATCH --time=0-00:05:00 # run for 5 minutes (d-hh:mm:ss) #SBATCH --mem-per-cpu=500MB # use 500MB per core # all bash commands must be after all SBATCH directives # define and create a unique scratch directory SCRATCH_DIRECTORY=/scratch0/${USER}/job-array-example/${SLURM_JOBID} mkdir -p ${SCRATCH_DIRECTORY} cd ${SCRATCH_DIRECTORY} cp ${SLURM_SUBMIT_DIR}/test.py ${SCRATCH_DIRECTORY} # each job will see a different ${SLURM_ARRAY_TASK_ID} echo \"now processing task id:: \" ${SLURM_ARRAY_TASK_ID} python test.py > output_${SLURM_ARRAY_TASK_ID}.txt # after the job is done we copy our output back to $SLURM_SUBMIT_DIR cp output_${SLURM_ARRAY_TASK_ID}.txt ${SLURM_SUBMIT_DIR} # we step out of the scratch directory and remove it cd ${SLURM_SUBMIT_DIR} rm -rf ${SCRATCH_DIRECTORY} # happy end exit 0 Retrieved from \" https://wiki.umiacs.umd.edu/umiacs/index.php?title=SLURM/ArrayJobs&oldid=11731 \" Navigation menu Personal tools Log in Namespaces Page Discussion English Views Read View source View history More Search Navigation Main Page Getting Started Core Services Lab Facilities Placing Orders Support Tools What links here Related changes Special pages Printable version Permanent link Page information This page was last edited on 28 March 2024, at 20:30. Privacy policy About UMIACS Disclaimers\n",
            "slurm / arrayjobs - umiacs slurm / arrayjobs from umiacs jump to navigation jump to search here is an example to get you started using array jobs in slurm. array computation example job save this code to a file called test. py. import time print ( ' start at ' + time. strftime ( ' % h : % m : % s ' ) ) print ( ' sleep for 10 seconds... ' ) time. sleep ( 10 ) print ( ' stop at ' + time. strftime ( ' % h : % m : % s ' ) ) submission script save this to a file called array. sh and you should be able to submit the job as sbatch array. sh. #! / bin / bash # # # # # # # # # # # # # # # # # # # # # # job - array example # # # # # # # # # # # # # # # # # # # # # # # sbatch - - job - name = example # sbatch - - array = 1 - 16 # run 16 jobs at the same time # sbatch - - time = 0 - 00 : 05 : 00 # run for 5 minutes ( d - hh : mm : ss ) # sbatch - - mem - per - cpu = 500mb # use 500mb per core # all bash commands must be after all sbatch directives # define and create a unique scratch directory scratch _ directory = / scratch0 / $ { user } / job - array - example / $ { slurm _ jobid } mkdir - p $ { scratch _ directory } cd $ { scratch _ directory } cp $ { slurm _ submit _ dir } / test. py $ { scratch _ directory } # each job will see a different $ { slurm _ array _ task _ id } echo \" now processing task id : : \" $ { slurm _ array _ task _ id } python test. py > output _ $ { slurm _ array _ task _ id }. txt # after the job is done we copy our output back to $ slurm _ submit _ dir cp output _ $ { slurm _ array _ task _ id }. txt $ { slurm _ submit _ dir } # we step out of the scratch directory and remove it cd $ { slurm _ submit _ dir } rm - rf $ { scratch _ directory } # happy end exit 0 retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / arrayjobs & oldid = 11731 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 28 march 2024, at 20 : 30. privacy policy about umiacs disclaimers\n",
            "--nodes=2 Requests 2 nodes be allocated to your job; if passed to srun, runs the given command concurrently on each node --nodelist=<NODENAME> Requests to run your job on the <NODENAME> node --time=dd-hh:mm:ss Requests your job run for dd days, hh hours, mm minutes, and ss seconds --error=<ERRNAME> Redirects stderr for your job to the <ERRNAME> file --partition=<PARTITIONNAME> Requests your job run in the <PARTITIONNAME> partition --qos=<QOSNAME>default Requests your job run with the <QOSNAME> QOS, to see the available QOS options on a cluster, run show_qos --account=<ACCOUNTNAME> Requests your job runs under the <ACCOUNTNAME> Slurm account, different accounts have different available partitions/QOS --output=<OUTNAME> Redirects stdout for your job to the <OUTNAME> file --requeue Requests your job be automatically requeued if it is preempted --exclusive Requests your job be the only one running on the node(s) it is assigned to. This requires that your job be allocated all of the resources on the node(s). The scheduler does not automatically give your job all of the node's/nodes' resources, however, so if you need more than the default, you still need to request these with --ntasks and --mem Interactive Shell Sessions An interactive shell session on a compute node can be useful for debugging or developing code that isn't ready to be run as a batch job. To get an interactive shell on a node, use srun with the --pty argument to invoke a shell: $ srun --pty --qos=default --mem=1g --time=01:00:00 bash $ hostname tron33.umiacs.umd.edu Please do not leave interactive shells running for long periods of time when you are not working. This blocks resources from being used by everyone else. salloc The salloc command can also be used to request resources be allocated without needing a batch script. Running salloc with a list of resources will allocate the resources you requested, create a job, and drop you into a subshell with the environment variables necessary to run commands in the newly created job allocation. When your time is up or you exit the subshell, your job allocation will be relinquished. $ salloc --qos=default -N 1 --mem=2g --time=01:00:00 salloc: Granted job allocation 159 $ srun /usr/bin/hostname tron33.umiacs.umd.edu $ exit exit salloc: Relinquishing job allocation 159 Please note that any commands not invoked with srun will be run locally on the submit node. Please be careful when using salloc. sbatch The sbatch command allows you to write a batch script to be submitted and run non-interactively on the compute nodes. To run a simple Hello World command on the compute nodes you could write a file, helloWorld.sh with the following contents: #!/bin/bash srun bash -c 'echo Hello World from `hostname`' Then you need to submit the script with sbatch and request resources: $ sbatch --qos=default --mem=1g --time=1:00:00 helloWorld.sh Submitted batch job 121 SLURM will return a job number that you can use to check the status of your job with squeue: $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 121 tron helloWor username R 0:01 1 tron32 Advanced Batch Scripts You can also\n"
          ]
        }
      ],
      "source": [
        "# Generate query embedding\n",
        "query = \"sbatch jobs\"\n",
        "query_embedding = embedding_model.embed_query(query)  # Ensure this is a valid vector\n",
        "\n",
        "# Step: Perform similarity search by vector\n",
        "try:\n",
        "    # Use the correct syntax for similarity search by vector\n",
        "    results = vector_store.similarity_search_by_vector(embedding=query_embedding, k=5)\n",
        "\n",
        "    # Correctly extract text from Document objects\n",
        "    for doc in results:\n",
        "        print(doc.page_content)  # Access the content of the Document object\n",
        "except Exception as e:\n",
        "    print(f\"Error during similarity search: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bA51j0pN2UB"
      },
      "source": [
        "### In case we want to delete the vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8KQyODR4OHB",
        "outputId": "d27097f3-9438-4c24-ee55-38a2ceb64d27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All vectors from index 'umiacs' have been deleted.\n"
          ]
        }
      ],
      "source": [
        "def delete_all_vectors(redis_conn, index_name):\n",
        "    \"\"\"Deletes all keys in the given Redis namespace.\"\"\"\n",
        "    try:\n",
        "        # Use SCAN to retrieve all keys that belong to the index namespace\n",
        "        keys = redis_conn.scan_iter(f\"{index_name}:*\")\n",
        "        for key in keys:\n",
        "            redis_conn.delete(key)  # Delete each key\n",
        "        print(f\"All vectors from index '{index_name}' have been deleted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting vectors: {e}\")\n",
        "\n",
        "# Example usage\n",
        "delete_all_vectors(redis_conn, \"umiacs\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
