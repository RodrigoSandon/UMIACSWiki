{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Parse the Data into a nested json structure where the key is the subject and value is the content scraped for that subject"
      ],
      "metadata": {
        "id": "FreB7c3ktsrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def fetch_content(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "def parse_content(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "def is_valid_url(url):\n",
        "    try:\n",
        "        result = urlparse(url)\n",
        "        # Check if the scheme (http or https) and netloc (domain) are present\n",
        "        return all([result.scheme, result.netloc])\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def extract_summary(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    summary_parts = []\n",
        "\n",
        "    # Add the title as part of the summary\n",
        "    title = soup.find('title')\n",
        "    if title:\n",
        "        summary_parts.append(title.get_text())\n",
        "\n",
        "    # Extract main headings (h1)\n",
        "    for h1 in soup.find_all('h1'):\n",
        "        summary_parts.append(h1.get_text(strip=True))\n",
        "\n",
        "    # Extract first paragraph after each main heading\n",
        "    for h1 in soup.find_all('h1'):\n",
        "        next_p = h1.find_next_sibling('p')\n",
        "        if next_p:\n",
        "            summary_parts.append(next_p.get_text(strip=True))\n",
        "\n",
        "    return ' '.join(summary_parts)\n",
        "\n",
        "def extract_content(soup):\n",
        "    try:\n",
        "        main_heading = soup.find('h1', id='firstHeading').text.strip()\n",
        "        content_div = soup.find('div', id='bodyContent')\n",
        "        sections = {}\n",
        "\n",
        "        for headline in content_div.find_all('span', class_='mw-headline'):\n",
        "          current_section = headline.text.strip()\n",
        "          section_content = []\n",
        "          codes = []\n",
        "\n",
        "          # Check the next siblings until another headline or end of div\n",
        "          for sibling in headline.parent.find_next_siblings():\n",
        "              if sibling.find('span', class_='mw-headline'):\n",
        "                  break  # Stop if another section starts\n",
        "\n",
        "              # Handle pre-formatted code blocks\n",
        "              if sibling.name == 'pre':\n",
        "                  codes.append(sibling.text.strip())\n",
        "\n",
        "              # Handle other types of content\n",
        "              elif sibling.text:\n",
        "                  # Go through each element inside the sibling (text or links)\n",
        "                  for element in sibling.descendants:\n",
        "                      if element.name == 'a' and element.get('href'):\n",
        "                          # Get the href link, and exclude .png links\n",
        "                          href = element['href']\n",
        "                          if not href.endswith('.png'):\n",
        "                              # Append link text and URL in the format [text](URL)\n",
        "                              section_content.append(f\"{element.get_text(strip=True)} [{href}]\")\n",
        "                      elif isinstance(element, str):\n",
        "                          # Append plain text directly\n",
        "                          section_content.append(element.strip())\n",
        "\n",
        "          # Join all section content into one string and handle further as needed\n",
        "          final_content = ' '.join(section_content)\n",
        "\n",
        "            # Now you can use final_content or store it for further processing\n",
        "\n",
        "          # Saving content, links, and codes for the current section\n",
        "          sections[current_section] = {\n",
        "              'content': final_content,\n",
        "              'code': codes\n",
        "          }\n",
        "\n",
        "        return main_heading, sections\n",
        "    except:\n",
        "      print(\"Error\")\n",
        "\n",
        "def extract_external_content(url):\n",
        "  summary = {}\n",
        "  baseUrl = \"https://wiki.umiacs.umd.edu/\"\n",
        "  cleaned_url = url.replace(baseUrl, \"\")\n",
        "  if cleaned_url in externalLinkDict:\n",
        "    externalLinks = externalLinkDict[cleaned_url]\n",
        "    for link in externalLinks:\n",
        "      if is_valid_url(link):\n",
        "        response = requests.get(link)\n",
        "        html = response.text\n",
        "        summary[link] = extract_summary(html)\n",
        "  return summary\n",
        "\n",
        "def scrape_wiki(url):\n",
        "    html = fetch_content(url)\n",
        "    soup = parse_content(html)\n",
        "    main_heading, sections = extract_content(soup)\n",
        "    externalContent = extract_external_content(url)\n",
        "    return {main_heading: sections,\n",
        "            'external_info': externalContent}\n",
        "\n",
        "# Example URL\n",
        "url = 'https://wiki.umiacs.umd.edu//umiacs/index.php/Nexus/MC2'\n",
        "wiki_content = scrape_wiki(url)\n",
        "print(wiki_content)\n",
        "\n"
      ],
      "metadata": {
        "id": "NvO5UtUYtvRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a master dictionary\n",
        "linkPage = {}\n",
        "baseURL = 'https://wiki.umiacs.umd.edu'\n",
        "i = 1\n",
        "master_dictionary = {}\n",
        "for link in links:\n",
        "    print(\"Parsing Link \" + str(i))\n",
        "    i += 1\n",
        "    href = link.get('href')\n",
        "\n",
        "    # If the link leads to a page on the wiki, we can parse the text\n",
        "    if href and href.startswith('/umiacs/index.php'):\n",
        "        pageUrl= baseURL + href\n",
        "        try:\n",
        "          wiki_content = scrape_wiki(pageUrl)\n",
        "        except:\n",
        "          print(pageUrl)\n",
        "        wiki_content['url'] = href # can index the pages based on their base url\n",
        "        master_dictionary[href] = wiki_content"
      ],
      "metadata": {
        "id": "NIYrDF7-t7Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the file path where you want to save the JSON\n",
        "file_path = 'wiki_content.json'\n",
        "\n",
        "# Open a file in write mode ('w') and save the dictionary as JSON\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(master_dictionary, json_file, indent=4)"
      ],
      "metadata": {
        "id": "cEldxNOBt9FZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}