{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findings from this are \n",
    "1. currently our chunk sizes are too big (or must be too small), so we need to pick an embedding model that has a larger context window\n",
    "2. if we still have to chunk our documents (longest one is 8.5k tokens, even if it fits in embedding model's context, it might overwhelm the actual LLM)\n",
    "    1. if you use langchain_text_splitters it'll do the splitting and tokenization by itself, and the returned chunks won't have messed up spacing and casing etc. you have to mess with the separator characters/strings though\n",
    "    2. chunking documents might also be desirable because \"python notebook nexus\" is closer to documents entirely about nexus than documents only partially about python notebooks (\"python notebook\" returns the right one). bigger embedding model doesn't fix this\n",
    "3. there's duplicate pages from the allpages page that are in the html folder and scrapedtext file (which means duplicate or near duplicate chunks)\n",
    "4. we'll also want to do some testing with seeing what range of similarity scores are relevant for us, cuz a lot of the time there's only a few sentences of context that answers a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(file, sep=\" \"):\n",
    "    with open(\"./dataset/raw_html/\" + file, \"r\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        text = soup.get_text(separator=sep, strip=True)\n",
    "        # text = soup\n",
    "        return text\n",
    "    return \"wtf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(N_A)Network_VPN_OSX.html',\n",
       " 'Accounts.html',\n",
       " 'Accounts_Collaborator.html',\n",
       " 'ActiveDirectory.html',\n",
       " 'AddingUMIACSCertificateAuthority.html',\n",
       " 'Adobe.html',\n",
       " 'AlternativePickup.html',\n",
       " 'ApplicationResource.html',\n",
       " 'Apptainer.html',\n",
       " 'Archives.html',\n",
       " 'ATL_ConferenceRooms.html',\n",
       " 'Automounter.html',\n",
       " 'Backups.html',\n",
       " 'BarracudaSpamFirewall.html',\n",
       " 'BarracudaSpamFirewall_QuarantinePassthrough.html',\n",
       " 'BarracudaSpamFirewall_Scoring.html',\n",
       " 'BarracudaSpamFirewall_SearchingQuarantine.html',\n",
       " 'Barracuda_Spam_Firewall.html',\n",
       " 'Bash.html',\n",
       " 'BashForWindows.html',\n",
       " 'BashForWindows10.html',\n",
       " 'BitLocker.html',\n",
       " 'BitLocker_PersonalUse.html',\n",
       " 'Branding.html',\n",
       " 'Caffe.html',\n",
       " 'CBCB.html',\n",
       " 'CCompilers.html',\n",
       " 'CDebuggers.html',\n",
       " 'CIFS.html',\n",
       " 'ClassAccounts.html',\n",
       " 'ClassAccounts_Manage.html',\n",
       " 'Clear_Cache.html',\n",
       " 'CMake.html',\n",
       " 'CML.html',\n",
       " 'CollaboratorAccount.html',\n",
       " 'CompromisedPasswordFiltering.html',\n",
       " 'ComputationalResource.html',\n",
       " 'Compute_DataLocality.html',\n",
       " 'ConferenceRooms.html',\n",
       " 'ConferenceRooms_ATL.html',\n",
       " 'ConferenceRooms_ATL3100A.html',\n",
       " 'ConferenceRooms_ATL3100C.html',\n",
       " 'ConferenceRooms_ATL3100D.html',\n",
       " 'ConferenceRooms_ATL3373.html',\n",
       " 'ConferenceRooms_IRB.html',\n",
       " 'ConferenceRooms_Recording.html',\n",
       " 'ConferenceRooms_Solstice.html',\n",
       " 'ConferenceRooms_TouchPanel.html',\n",
       " 'ConferenceRooms_Zoom.html',\n",
       " 'ContentManagement.html',\n",
       " 'CoreServices.html',\n",
       " 'Creating_service_admin_user_on_macOS.html',\n",
       " 'Creating_service_admin_user_on_OSX.html',\n",
       " 'CUDA.html',\n",
       " 'CUPS.html',\n",
       " 'Datasets.html',\n",
       " 'Data_Compression___Archival.html',\n",
       " 'Data_Transfer.html',\n",
       " 'DigitalLoanerForm.html',\n",
       " 'Docker.html',\n",
       " 'Duo.html',\n",
       " 'Email.html',\n",
       " 'EmailMigration.html',\n",
       " 'EmailMigration_SelfMigration.html',\n",
       " 'Email_Barracuda.html',\n",
       " 'EnvironmentalVariables.html',\n",
       " 'EuclidCluster.html',\n",
       " 'FileTransferProtocol.html',\n",
       " 'Fixing_Stuck_Standing_Desk.html',\n",
       " 'FTP.html',\n",
       " 'FTP_macOS_Finder.html',\n",
       " 'FTP_OSX_Finder.html',\n",
       " 'FTP_Windows_Explorer.html',\n",
       " 'GettingStarted.html',\n",
       " 'GitLab.html',\n",
       " 'Google_Drive.html',\n",
       " 'Google_Drive_Backup_and_Sync.html',\n",
       " 'Google_Drive_Drive_for_Desktop.html',\n",
       " 'Google_Drive_Migrating_from_Backup_and_Sync_to_Drive_for_Desktop.html',\n",
       " 'Gurobi.html',\n",
       " 'HelpDesk.html',\n",
       " 'HPC.html',\n",
       " 'InputMethodEditors.html',\n",
       " 'IntelCompilers.html',\n",
       " 'Intranet.html',\n",
       " 'IOSJuniperVPN.html',\n",
       " 'Iribe.html',\n",
       " 'Iribe_ConferenceRooms.html',\n",
       " 'Iribe_ConferenceRooms_AutoAccept.html',\n",
       " 'Iribe_ConferenceRooms_HuddleRoom.html',\n",
       " 'Iribe_ConferenceRooms_List.html',\n",
       " 'Iribe_ConferenceRooms_Moderated.html',\n",
       " 'Iribe_ConferenceRooms_Moderation.html',\n",
       " 'Iribe_ConferenceRooms_Recording.html',\n",
       " 'Iribe_ConferenceRooms_Reserve.html',\n",
       " 'Iribe_ConferenceRooms_Solstice.html',\n",
       " 'Iribe_ConferenceRooms_Solstice_Solstice_App.html',\n",
       " 'Iribe_ConferenceRooms_TouchPanel.html',\n",
       " 'Iribe_ConferenceRooms_View.html',\n",
       " 'Iribe_ConferenceRooms_Zoom.html',\n",
       " 'Iribe_Faxing.html',\n",
       " 'Iribe_Mailroom.html',\n",
       " 'Iribe_OfficeTV.html',\n",
       " 'Iribe_PacketFence_Network.html',\n",
       " 'Iribe_Wired_Network.html',\n",
       " 'Java.html',\n",
       " 'JavaDevelEnvironment.html',\n",
       " 'JavaDisableBrowser.html',\n",
       " 'JavaVersions.html',\n",
       " 'Jekyll.html',\n",
       " 'Jira.html',\n",
       " 'Kerberos.html',\n",
       " 'KVM.html',\n",
       " 'LabFacilities.html',\n",
       " 'Lamsub.sh.e127.html',\n",
       " 'Lamsub.sh.html',\n",
       " 'Lamsub.sh.o127.html',\n",
       " 'Lamsub2.sh.html',\n",
       " 'LANDesk.html',\n",
       " 'LaTeX.html',\n",
       " 'LDAP.html',\n",
       " 'LinuxFAQ.html',\n",
       " 'LocalDataStorage.html',\n",
       " 'MacOSDisplayModes.html',\n",
       " 'MacOSPrinting.html',\n",
       " 'MailingLists.html',\n",
       " 'MailmanFAQ.html',\n",
       " 'MailmanListAdmin.html',\n",
       " 'Main_Page.html',\n",
       " 'MalwareRecovery.html',\n",
       " 'Malware_virus_removal.html',\n",
       " 'Mathematica.html',\n",
       " 'MathematicaActivation.html',\n",
       " 'Matlab.html',\n",
       " 'Mattermost.html',\n",
       " 'MBRC.html',\n",
       " 'MediaSanitization.html',\n",
       " 'MFA.html',\n",
       " 'MFA_Duo_Recovery.html',\n",
       " 'Microsoft_Office_Activation.html',\n",
       " 'Modules.html',\n",
       " 'MongoDB.html',\n",
       " 'MonthlyMaintenanceWindow.html',\n",
       " 'MPATH.html',\n",
       " 'Mpich1.csh.html',\n",
       " 'Mpich1sub.sh.e167.html',\n",
       " 'Mpich1sub.sh.html',\n",
       " 'Mpich1sub.sh.o167.html',\n",
       " 'NAGWareCompiler.html',\n",
       " 'NAS.html',\n",
       " 'NASProjects.html',\n",
       " 'NASUsers.html',\n",
       " 'NautilusThumbnails.html',\n",
       " 'Network.html',\n",
       " 'Network_Troubleshooting.html',\n",
       " 'Network_Troubleshooting_DNS.html',\n",
       " 'Network_VPN.html',\n",
       " 'Network_VPN_Android.html',\n",
       " 'Network_VPN_IOS.html',\n",
       " 'Network_VPN_Ivanti.html',\n",
       " 'Network_VPN_Linux.html',\n",
       " 'Network_VPN_macOS.html',\n",
       " 'Network_VPN_MFA.html',\n",
       " 'Network_VPN_Mobile.html',\n",
       " 'Network_VPN_OSX.html',\n",
       " 'Network_VPN_Troubleshooting.html',\n",
       " 'Network_VPN_Ubuntu.html',\n",
       " 'Network_VPN_Windows.html',\n",
       " 'Nexus.html',\n",
       " 'Nexus_Accounts.html',\n",
       " 'Nexus_Apptainer.html',\n",
       " 'Nexus_CBCB.html',\n",
       " 'Nexus_CLIP.html',\n",
       " 'Nexus_CML.html',\n",
       " 'Nexus_Gamma.html',\n",
       " 'Nexus_GPUs.html',\n",
       " 'Nexus_MBRC.html',\n",
       " 'Nexus_MC2.html',\n",
       " 'Nexus_Tron.html',\n",
       " 'Nexus_Vulcan.html',\n",
       " 'Nexus_Vulcan_GPUs.html',\n",
       " 'NFS.html',\n",
       " 'NFShomes.html',\n",
       " 'NightlyBackups.html',\n",
       " 'OBJ.html',\n",
       " 'OBJbox.html',\n",
       " 'OBJ_WebHosting.html',\n",
       " 'Obj_Website.html',\n",
       " 'OpenCVVersions.html',\n",
       " 'OpenLAB.html',\n",
       " 'OptLocal.html',\n",
       " 'OrderingEquipment.html',\n",
       " 'Orders.html',\n",
       " 'OSSupport.html',\n",
       " 'OSXJuniperVPN.html',\n",
       " 'OSXPrinting.html',\n",
       " 'Outage.html',\n",
       " 'Password.html',\n",
       " 'PATH.html',\n",
       " 'Perl.html',\n",
       " 'PerlDevelEnvironment.html',\n",
       " 'PerlLocalLib.html',\n",
       " 'Perl_Environment.html',\n",
       " 'PGICompilers.html',\n",
       " 'Phishing.html',\n",
       " 'Podman.html',\n",
       " 'PrinterQueueNaming.html',\n",
       " 'PrinterTroubleshooting.html',\n",
       " 'Printing.html',\n",
       " 'PrintingDoubleSidedUSB.html',\n",
       " 'Programming.html',\n",
       " 'Publishing_Data.html',\n",
       " 'Python.html',\n",
       " 'PythonVirtualEnv.html',\n",
       " 'QuICS.html',\n",
       " 'Quota.html',\n",
       " 'R.html',\n",
       " 'Rclone.html',\n",
       " 'RedHat.html',\n",
       " 'Remote_Desktop.html',\n",
       " 'RevisionControl.html',\n",
       " 'RHEL.html',\n",
       " 'RHEL7.html',\n",
       " 'RHELDisplayModes.html',\n",
       " 'RStudio.html',\n",
       " 'S3Clients.html',\n",
       " 'SCM.html',\n",
       " 'SCP.html',\n",
       " 'Screen.html',\n",
       " 'SearchingQuarantine.html',\n",
       " 'SecGroups.html',\n",
       " 'SecureCopy.html',\n",
       " 'SecureShell.html',\n",
       " 'SecureShellTunneling.html',\n",
       " 'SecureShell_MFA.html',\n",
       " 'Security.html',\n",
       " 'ServiceDeskMigration.html',\n",
       " 'Services.html',\n",
       " 'Services_Collaboration.html',\n",
       " 'Services_CommonPool.html',\n",
       " 'Services_Compute.html',\n",
       " 'Services_Compute_HPC.html',\n",
       " 'Services_Compute_LabSpecific.html',\n",
       " 'Services_Compute_UserSupported.html',\n",
       " 'Services_Data.html',\n",
       " 'Services_EMail.html',\n",
       " 'Services_EquipmentLoans.html',\n",
       " 'Services_Logistics.html',\n",
       " 'Services_OnSite.html',\n",
       " 'Services_Support.html',\n",
       " 'Services_Web.html',\n",
       " 'Service_Compute_Virtualization.html',\n",
       " 'SetGID.html',\n",
       " 'Setting_File_Permissions_in_Windows.html',\n",
       " 'SetUID.html',\n",
       " 'SFTP.html',\n",
       " 'Shell.html',\n",
       " 'Show_available_nodes.html',\n",
       " 'Singularity.html',\n",
       " 'SLURM.html',\n",
       " 'SLURM_ArrayJobs.html',\n",
       " 'SLURM_ClusterStatus.html',\n",
       " 'SLURM_JobStatus.html',\n",
       " 'SLURM_JobSubmission.html',\n",
       " 'SLURM_Preemption.html',\n",
       " 'SLURM_Priority.html',\n",
       " 'Snapshots-Example.html',\n",
       " 'Snapshots.html',\n",
       " 'Special-AllPages.html',\n",
       " 'Special-SpecialPages.html',\n",
       " 'SSH.html',\n",
       " 'SSHFileTransferProtocol.html',\n",
       " 'SSH_Jumphosts.html',\n",
       " 'SSH_Keys.html',\n",
       " 'SSH_MFA.html',\n",
       " 'Stow.html',\n",
       " 'Subversion.html',\n",
       " 'Tcsh.html',\n",
       " 'Tensorflow.html',\n",
       " 'Tmux.html',\n",
       " 'Torch.html',\n",
       " 'Tron.html',\n",
       " 'TSM.html',\n",
       " 'Ubuntu.html',\n",
       " 'UbuntuPrinting.html',\n",
       " 'Ubuntu_SoftwareCenter.html',\n",
       " 'Umask.html',\n",
       " 'UMIACS-About.html',\n",
       " 'UMIACS-General_disclaimer.html',\n",
       " 'UMIACS-Privacy_policy.html',\n",
       " 'UMIACS_Account.html',\n",
       " 'UMIACS_Public_Printers.html',\n",
       " 'UMobj.html',\n",
       " 'UMobj_Example.html',\n",
       " 'UNIXDataPolicies.html',\n",
       " 'UNIXPrinting.html',\n",
       " 'Unix_groups.html',\n",
       " 'UpdatingMozillaSoftware.html',\n",
       " 'URL.html',\n",
       " 'UserReport.html',\n",
       " 'UsrLocal.html',\n",
       " 'Visual_Studio.html',\n",
       " 'VPN.html',\n",
       " 'VPN_OSX.html',\n",
       " 'VPN_PulseSecure.html',\n",
       " 'VPN_Windows.html',\n",
       " 'VScode.html',\n",
       " 'VS_Code.html',\n",
       " 'Vtune.html',\n",
       " 'Vulcan.html',\n",
       " 'Web.html',\n",
       " 'WebCrawling.html',\n",
       " 'WebSensitiveInformation.html',\n",
       " 'WebSpace.html',\n",
       " 'Windows.html',\n",
       " 'WindowsDisplayModes.html',\n",
       " 'WindowsJuniperVPN.html',\n",
       " 'WindowsMigration.html',\n",
       " 'WindowsPermissions.html',\n",
       " 'WindowsPrinting.html',\n",
       " 'WindowsPrintingUMIACS.html',\n",
       " 'WindowsPython.html',\n",
       " 'WindowsServicing.html',\n",
       " 'WindowsSubsystemForLinux.html',\n",
       " 'Windows_10_Servicing.html',\n",
       " 'Windows_Account_Migration_March_2018.html',\n",
       " 'Windows_Activation.html',\n",
       " 'Windows_LaptopSupport.html',\n",
       " 'Windows_Patch_Management.html',\n",
       " 'Windows_Personal_Backups.html',\n",
       " 'Wireless.html',\n",
       " 'Working_with_External_Collaborators.html']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(\"./dataset/raw_html\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nexus - UMIACS Nexus From UMIACS Jump to navigation Jump to search The Nexus is the combined scheduler of resources in UMIACS.  The resource manager for Nexus is SLURM .  Resources are arranged into partitions where users are able to schedule computational jobs.  Users are arranged into a number of SLURM accounts based on faculty, lab, or center investments. Contents 1 Getting Started 1.1 Access 1.2 Jobs 1.2.1 Interactive 1.2.2 Batch 2 Partitions 3 Quality of Service (QoS) 3.1 Job QoS 3.2 Partition QoS 4 Storage 4.1 Home Directories 4.2 Scratch Directories 4.2.1 Network Scratch Directories 4.2.2 Local Scratch Directories 4.3 Faculty Allocations 4.4 Project Allocations 4.5 Datasets Getting Started All accounts in UMIACS are sponsored.  If you don\\'t already have a UMIACS account, please see Accounts for information on getting one.  You need a full UMIACS account (not a collaborator account ) in order to access Nexus. Access Your access to submission nodes (alternatively called login nodes) for Nexus computational resources is determined by your account sponsor\\'s department, center, or lab affiliation.  You can log into the UMIACS Directory CR application and select the Computational Resource (CR) in the list that has the prefix nexus .  The Hosts section lists your available submission nodes - generally a pair of nodes of the format nexus<department, lab, or center abbreviation>[00,01] , e.g., nexusgroup00 and nexusgroup01 . Note - UMIACS requires multi-factor authentication through our Duo instance.  This is completely discrete from both UMD\\'s and CSD\\'s Duo instances.  You will need to enroll one or more devices to access resources in UMIACS, and will be prompted to enroll when you log into the Directory application for the first time. Once you have identified your submission nodes, you can SSH directly into them.  From there, you are able to submit to the cluster via our SLURM workload manager.  You need to make sure that your submitted jobs have the correct account, partition, and qos. Jobs SLURM jobs are submitted by either srun or sbatch depending if you are doing an interactive job or batch job, respectively.  You need to provide the where/how/who to run the job and specify the resources you need to run with. For the who/where/how, you may be required to specify --account , --partition , and/or --qos (respectively) to be able to adequately submit jobs to the Nexus. For resources, you may need to specify --time for time, --ntasks for CPUs, --mem for RAM, and --gres=gpu for GPUs in your submission arguments to meet your requirements.  There are defaults for all four, so if you don\\'t specify something, you may be scheduled with a very minimal set of time and resources (e.g., by default, NO GPUs are included if you do not specify --gres=gpu ).  For more information about submission flags for GPU resources, see here .  You can also can run man srun on your submission node for a complete list of available submission arguments. For a list of available GPU types on Nexus and their specs, please see Nexus/GPUs . Interactive Once logged into a submission node, you can run simple interactive jobs.  If your session is interrupted from the submission node, the job will be killed.  As such, we encourage use of a terminal multiplexer such as Tmux . $ srun --pty --ntasks=4 --mem=2gb --gres=gpu:1 nvidia-smi -L\\nGPU 0: NVIDIA RTX A4000 (UUID: GPU-ae5dc1f5-c266-5b9f-58d5-7976e62b3ca1) Batch Batch jobs are scheduled with a script file with an optional ability to embed job scheduling parameters via variables that are defined by #SBATCH lines at the top of the file.  You can find some examples in our SLURM/JobSubmission documentation. Partitions The SLURM resource manager uses partitions to act as job queues which can restrict size, time and user limits.  The Nexus has a number of different partitions of resources.  Different Centers, Labs, and Faculty are able to invest in computational resources that are restricted to approved users through these partitions. Partitions usable by all non- class account users: Nexus/Tron - Pool of resources available to all UMIACS and CSD faculty and graduate students. Scavenger - Preemption partition that supports nodes from multiple other partitions.  More resources are available to schedule simultaneously than in other partitions, however jobs are subject to preemption rules.  You are responsible for ensuring your jobs handle this preemption correctly.  The SLURM scheduler will simply restart a preempted job with the same submission arguments when it is available to run again. For an overview of things you can check within scripts to determine if your job was preempted/resumed, see SLURM/Preemption . Partitions usable by ClassAccounts : Class - Pool available for UMIACS class accounts sponsored by either UMIACS or CSD faculty. Partitions usable by specific lab/center users: Nexus/CBCB - CBCB lab pool available for CBCB lab members. Nexus/CLIP - CLIP lab pool available for CLIP lab members. Nexus/CML - CML lab pool available for CML lab members. Nexus/GAMMA - GAMMA lab pool available for GAMMA lab members. Nexus/MBRC - MBRC lab pool available for MBRC lab members. Nexus/MC2 - MC2 lab pool available for MC2 lab members. Nexus/Vulcan - Vulcan lab pool available for Vulcan lab members. Quality of Service (QoS) SLURM uses Quality of Service (QoS) both to provide limits on job sizes (termed by us as \"job QoS\") as well as to limit resources used by all jobs running in a partition, either per user or per group (termed by us as \"partition QoS\"). Job QoS Job QoS are used to provide limits on the size of job that you can run. You should try to allocate only the resources your job actually needs, as resources that each of your jobs schedules are counted against your fair-share priority in the future. default - Default job QoS. Limited to 4 CPU cores, 1 GPU, and 32GB RAM per job.  The maximum wall time per job is 3 days. medium - Limited to 8 CPU cores, 2 GPUs, and 64GB RAM per job.  The maximum wall time per job is 2 days. high - Limited to 16 CPU cores, 4 GPUs, and 128GB RAM per job.  The maximum wall time per job is 1 day. scavenger - No resource limits per job, only a maximum wall time per job of 3 days.  You are responsible for ensuring your job requests multiple nodes if it requests resources beyond what any one node is capable of.  576 total CPU cores, 72 total GPUs, and 2304GB total RAM are permitted simultaneously across all of your jobs running with this job QoS.  This job QoS is both only available in the scavenger partition and the only job QoS available in the scavenger partition. To use this job QoS, include --partition=scavenger and --account=scavenger in your submission arguments.  Do not include any job QoS argument other than --qos=scavenger (optional) or submission will fail. You can display these job QoS from the command line using the show_qos command.  By default, the command will only show job QoS that you can access.  The above four job QoS are the ones that everyone can access. $ show_qos\\n                Name     MaxWall                        MaxTRES MaxJobsPU                      MaxTRESPU \\n-------------------- ----------- ------------------------------ --------- ------------------------------ \\n             default  3-00:00:00       cpu=4,gres/gpu=1,mem=32G                                          \\n                high  1-00:00:00     cpu=16,gres/gpu=4,mem=128G                                                                        \\n              medium  2-00:00:00       cpu=8,gres/gpu=2,mem=64G                                          \\n           scavenger  3-00:00:00                                           cpu=576,gres/gpu=72,mem=2304G If you want to see all job QoS, including those that you do not have access to, you can use the show_qos --all command. $ show_qos --all\\n                Name     MaxWall                        MaxTRES MaxJobsPU                      MaxTRESPU \\n-------------------- ----------- ------------------------------ --------- ------------------------------ \\n             cml-cpu  7-00:00:00                                        8                                \\n         cml-default  7-00:00:00       cpu=4,gres/gpu=1,mem=32G         2                                \\n            cml-high  1-12:00:00     cpu=16,gres/gpu=4,mem=128G         2                                \\n       cml-high_long 14-00:00:00              cpu=32,gres/gpu=8         8                     gres/gpu=8 \\n          cml-medium  3-00:00:00       cpu=8,gres/gpu=2,mem=64G         2                                \\n       cml-scavenger  3-00:00:00                                                             gres/gpu=24 \\n       cml-very_high  1-12:00:00     cpu=32,gres/gpu=8,mem=256G         8                    gres/gpu=12 \\n             default  3-00:00:00       cpu=4,gres/gpu=1,mem=32G                                          \\n                high  1-00:00:00     cpu=16,gres/gpu=4,mem=128G                                          \\n             highmem 21-00:00:00                  cpu=32,mem=2T                                          \\n           huge-long 10-00:00:00     cpu=32,gres/gpu=8,mem=256G                                          \\n              medium  2-00:00:00       cpu=8,gres/gpu=2,mem=64G                                          \\n           scavenger  3-00:00:00                                           cpu=576,gres/gpu=72,mem=2304G \\n          vulcan-cpu  2-00:00:00                cpu=1024,mem=4T         4                                \\n      vulcan-default  7-00:00:00       cpu=4,gres/gpu=1,mem=32G         2                                \\n       vulcan-exempt  7-00:00:00     cpu=32,gres/gpu=8,mem=256G         2                                \\n         vulcan-high  1-12:00:00     cpu=16,gres/gpu=4,mem=128G         2                                \\n        vulcan-janus  3-00:00:00    cpu=32,gres/gpu=10,mem=256G                                          \\n       vulcan-medium  3-00:00:00       cpu=8,gres/gpu=2,mem=64G         2                                \\n       vulcan-sailon  3-00:00:00     cpu=32,gres/gpu=8,mem=256G                              gres/gpu=48 \\n    vulcan-scavenger  3-00:00:00     cpu=32,gres/gpu=8,mem=256G To find out what accounts and partitions you have access to, first use the show_assoc command to show your account/job QoS combinations. Then, use the scontrol show partition command and note the AllowAccounts entry for each listed partition. You are able to submit to any partition that allows an account that you have. If you need to use an account other than the default account nexus , you will need to specify it via the --account submission argument. Partition QoS Partition QoS are used to limit resources used by all jobs running in a partition, either per user (MaxTRESPU) or per group (GrpTRES). To view partition QoS, use the show_partition_qos command. $ show_partition_qos\\n                Name MaxSubmitPU                      MaxTRESPU              GrpTRES \\n-------------------- ----------- ------------------------------ -------------------- \\n           scavenger         500  cpu=576,gres/gpu=72,mem=2304G                      \\n                tron         500     cpu=32,gres/gpu=4,mem=256G If you want to see all partition QoS, including those that you do not have access to, you can use the show_partition_qos --all command. $ show_partition_qos --all\\n                Name MaxSubmitPU                      MaxTRESPU              GrpTRES \\n-------------------- ----------- ------------------------------ -------------------- \\n                Name MaxSubmitPU                      MaxTRESPU              GrpTRES\\n-------------------- ----------- ------------------------------ --------------------\\n                cbcb         500                                 cpu=1260,mem=50016G\\n           cbcb-heng         500\\n               class         500     cpu=32,gres/gpu=4,mem=256G\\n                clip         500                                   cpu=564,mem=5590G\\n                 cml         500                                    cpu=1128,mem=11T\\n         cml-furongh         500\\n       cml-scavenger         500                    gres/gpu=24\\n            cml-zhou         500\\n               gamma         500                                   cpu=648,mem=5454G\\n                mbrc         500                                   cpu=240,mem=2345G\\n                 mc2         500                                   cpu=312,mem=3092G\\n               oasis         500\\n               quics         500                                   cpu=328,mem=3484G\\n           scavenger         500  cpu=576,gres/gpu=72,mem=2304G\\n                tron         500     cpu=32,gres/gpu=4,mem=256G\\n              vulcan         500                                 cpu=1392,mem=12833G\\n       vulcan-ampere         500\\n          vulcan-cpu         500\\n       vulcan-ramani         500\\n    vulcan-scavenger         500 NOTE : These QoS cannot be used directly when submitting jobs, with the exception of the scavenger QoS (i.e., they are not in the AllowQos field for their respective partition). Partition QoS limits apply to all jobs running on a given partition, regardless of what job QoS is used. For example, in the default non-preemption partition ( tron ), you are restricted to 32 total CPU cores, 4 total GPUs, and 256GB total RAM at once across all jobs you have running in the partition. Lab/group-specific partitions may also have their own user limits, and/or may also have group limits on the total number of resources consumed simultaneously by all users that are using their partition, codified by the line in the output above that matches their lab/group name. Note that the values listed above in the two \"TRES\" columns are not fixed and may fluctuate per-partition as more resources are added to or removed from each partition. All partitions also only allow a maximum of 500 submitted (running (R) or pending (PD)) jobs per user in the partition simultaneously. This is to prevent excess pending jobs causing backfill issues with the SLURM scheduler. If you need to submit more than 500 jobs in batch at once, you can develop and run an \"outer submission script\" that repeatedly attempts to run an \"inner submission script\" (your original submission script) to submit jobs in the batch periodically, until all job submissions are successful. The outer submission script should use looping logic to check if you are at the max job limit and should then retry submission after waiting for some time interval. An example outer submission script is as follows. In this example, example_inner.sh is your inner submission script and is not an array job , and you want to run 1000 jobs. If your inner submission script is an array job, adjust the number of jobs accordingly. Array jobs must be of size 500 or less. #!/bin/bash\\nnumjobs=1000\\ni=0\\nwhile [ $i -lt $numjobs ]\\ndo\\n  while [[ \"$(sbatch example_inner.sh 2>&1)\" =~ \"QOSMaxSubmitJobPerUserLimit\" ]]\\n  do\\n    echo \"Currently at maximum job submissions allowed.\"\\n    echo \"Waiting for 5 minutes before trying to submit more jobs.\"\\n    sleep 300\\n  done\\n  i=$(( $i + 1 ))\\n  echo \"Submitted job $i of $numjobs\"\\ndone It is suggested that you run the outer submission script in a Tmux session to keep the terminal window executing it from being interrupted. Storage All network storage available in Nexus is currently NFS based, and comes in a few different flavors. Compute nodes also have local storage that can be used. Home Directories You have 30GB of home directory storage available at /nfshomes/<username> .  It has both Snapshots and Backups enabled. Home directories are intended to store personal or configuration files only.  We encourage you to not share any data in your home directory.  You are encouraged to utilize our GitLab infrastructure to host your code repositories. NOTE : To check your quota on this directory, use the command df -h ~ . Scratch Directories Scratch data has no data protection including no snapshots and the data is not backed up. There are two types of scratch directories in the Nexus compute infrastructure: Network scratch directories Local scratch directories Please note that class accounts do not have network scratch directories. Network Scratch Directories You are allocated 200GB of scratch space via NFS from /fs/nexus-scratch/<USERNAME> where <USERNAME> is your UMIACS username. It is not backed up or protected in any way. This directory is automounted ; you will need to cd into the directory or request/specify a fully qualified file path to access it. You can view your quota usage by running df -h /fs/nexus-scratch/<USERNAME> . You may request a permanent increase of up to 400GB total space without any faculty approval by contacting staff .  If you need space beyond 400GB, you will need faculty approval and/or a project allocation for this. If you choose to increase your scratch space beyond 400GB, the increased space is also subject to the 270 TB days limit mentioned in the project allocation section before we check back in for renewal. For example, if you request 1.4TB total space, you may have this for 270 days (1TB beyond the 400GB permanent increase). The amount increased beyond 400GB will also count against your faculty member\\'s 20TB total storage limit mentioned below. This file system is available on all submission, data management, and computational nodes within the cluster. Local Scratch Directories Each computational node that you can schedule compute jobs on also has one or more local scratch directories.  These are always named /scratch0 , /scratch1 , etc. and are not backed up or protected in any way. These directories are almost always more performant than any other storage available to the job as they are mounted from disks directly attached to the compute node.  However, you must stage your data within the confines of your job and extract the relevant resultant data elsewhere before the end of your job. These local scratch directories have a tmpwatch job which will delete unaccessed data after 90 days , scheduled via maintenance jobs to run once a month during our monthly maintenance windows .  Please make sure you secure any resultant data you wish to keep from these directories at the end of your job. Faculty Allocations Each faculty member can be allocated 1TB of permanent lab space upon request.  We can also support grouping these individual allocations together into larger center, lab, or research group allocations if desired by the faculty.  Please contact staff to inquire. Lab space storage is fully protected.  It has snapshots enabled and is backed up nightly . Project Allocations Project allocations are available per user for 270 TB days; you can have a 1TB allocation for up to 270 days, a 3TB allocation for 90 days, etc.. A single faculty member can not have more than 20TB of project allocations across all of their sponsored accounts active simultaneously. Network scratch allocation space increases beyond the 400GB permanent maximum also have the increase count against this limit (i.e., a 1TB network scratch allocation would have 600GB counted towards this limit). Project storage is fully protected.  It has snapshots enabled and is backed up nightly . The maximum allocation length you can request is 540 days (500GB space) and the maximum storage space you can request is 9TB (30 day length). To request an allocation, please contact staff with the faculty member(s) that the project is under involved in the conversation.  Please include the following details: Project Name (short) Description Size (1TB, 2TB, etc.) Length in days (270 days, 135 days, etc.) Other user(s) that need to access the allocation, if any These allocations are available via /fs/nexus-projects/<project name> . Renewal is not guaranteed to be available due to limits on the amount of total storage. Near the end of the allocation period, staff will contact you and ask if you are still in need of the storage allocation.  If renewal is available, you can renew for up to another 270 TB days with reapproval from the original faculty approver. If you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period.  Staff will then remove the allocation. If you do not respond to staff\\'s request by the end of the allocation period, staff will make the allocation temporarily inaccessible. If you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible. If one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation. Datasets We have read-only dataset storage available at /fs/nexus-datasets .  If there are datasets that you would like to see curated and available, please see this page . The list of Nexus datasets we currently host can be viewed here . Retrieved from \" https://wiki.umiacs.umd.edu/umiacs/index.php?title=Nexus&oldid=12024 \" Navigation menu Personal tools Log in Namespaces Page Discussion English Views Read View source View history More Search Navigation Main Page Getting Started Core Services Lab Facilities Placing Orders Support Tools What links here Related changes Special pages Printable version Permanent link Page information This page was last edited on 13 September 2024, at 13:21. Privacy policy About UMIACS Disclaimers'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_html(\"Nexus.html\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nexus.html',\n",
       " 'Nexus_Accounts.html',\n",
       " 'Nexus_Apptainer.html',\n",
       " 'Nexus_CBCB.html',\n",
       " 'Nexus_CLIP.html',\n",
       " 'Nexus_CML.html',\n",
       " 'Nexus_Gamma.html',\n",
       " 'Nexus_GPUs.html',\n",
       " 'Nexus_MBRC.html',\n",
       " 'Nexus_MC2.html',\n",
       " 'Nexus_Tron.html',\n",
       " 'Nexus_Vulcan.html',\n",
       " 'Nexus_Vulcan_GPUs.html',\n",
       " 'SLURM.html',\n",
       " 'SLURM_ArrayJobs.html',\n",
       " 'SLURM_ClusterStatus.html',\n",
       " 'SLURM_JobStatus.html',\n",
       " 'SLURM_JobSubmission.html',\n",
       " 'SLURM_Preemption.html',\n",
       " 'SLURM_Priority.html']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosenfiles = [file for file in (file if \"Nexus\" in file or \"SLURM\" in file else None for file in files) if file is not None] # ? \n",
    "chosenfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_tokenizer': Tokenizer(version=\"1.0\", truncation=TruncationParams(direction=Right, max_length=128, strategy=LongestFirst, stride=0), padding=PaddingParams(strategy=Fixed(128), direction=Right, pad_to_multiple_of=None, pad_id=0, pad_type_id=0, pad_token=\"[PAD]\"), added_tokens=[{\"id\":0, \"content\":\"[PAD]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":100, \"content\":\"[UNK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":101, \"content\":\"[CLS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":102, \"content\":\"[SEP]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":103, \"content\":\"[MASK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=BertNormalizer(clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True), pre_tokenizer=BertPreTokenizer(), post_processor=TemplateProcessing(single=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, type_id=0), SpecialToken(id=\"[SEP]\", type_id=0)], pair=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, type_id=0), SpecialToken(id=\"[SEP]\", type_id=0), Sequence(id=B, type_id=1), SpecialToken(id=\"[SEP]\", type_id=1)], special_tokens={\"[CLS]\":SpecialToken(id=\"[CLS]\", ids=[101], tokens=[\"[CLS]\"]), \"[SEP]\":SpecialToken(id=\"[SEP]\", ids=[102], tokens=[\"[SEP]\"])}), decoder=WordPiece(prefix=\"##\", cleanup=True), model=WordPiece(unk_token=\"[UNK]\", continuing_subword_prefix=\"##\", max_input_chars_per_word=100, vocab={\"[PAD]\":0, \"[unused0]\":1, \"[unused1]\":2, \"[unused2]\":3, \"[unused3]\":4, \"[unused4]\":5, \"[unused5]\":6, \"[unused6]\":7, \"[unused7]\":8, \"[unused8]\":9, \"[unused9]\":10, \"[unused10]\":11, \"[unused11]\":12, \"[unused12]\":13, \"[unused13]\":14, \"[unused14]\":15, \"[unused15]\":16, \"[unused16]\":17, \"[unused17]\":18, \"[unused18]\":19, \"[unused19]\":20, \"[unused20]\":21, \"[unused21]\":22, \"[unused22]\":23, \"[unused23]\":24, \"[unused24]\":25, \"[unused25]\":26, \"[unused26]\":27, \"[unused27]\":28, \"[unused28]\":29, \"[unused29]\":30, \"[unused30]\":31, \"[unused31]\":32, \"[unused32]\":33, \"[unused33]\":34, \"[unused34]\":35, \"[unused35]\":36, \"[unused36]\":37, \"[unused37]\":38, \"[unused38]\":39, \"[unused39]\":40, \"[unused40]\":41, \"[unused41]\":42, \"[unused42]\":43, \"[unused43]\":44, \"[unused44]\":45, \"[unused45]\":46, \"[unused46]\":47, \"[unused47]\":48, \"[unused48]\":49, \"[unused49]\":50, \"[unused50]\":51, \"[unused51]\":52, \"[unused52]\":53, \"[unused53]\":54, \"[unused54]\":55, \"[unused55]\":56, \"[unused56]\":57, \"[unused57]\":58, \"[unused58]\":59, \"[unused59]\":60, \"[unused60]\":61, \"[unused61]\":62, \"[unused62]\":63, \"[unused63]\":64, \"[unused64]\":65, \"[unused65]\":66, \"[unused66]\":67, \"[unused67]\":68, \"[unused68]\":69, \"[unused69]\":70, \"[unused70]\":71, \"[unused71]\":72, \"[unused72]\":73, \"[unused73]\":74, \"[unused74]\":75, \"[unused75]\":76, \"[unused76]\":77, \"[unused77]\":78, \"[unused78]\":79, \"[unused79]\":80, \"[unused80]\":81, \"[unused81]\":82, \"[unused82]\":83, \"[unused83]\":84, \"[unused84]\":85, \"[unused85]\":86, \"[unused86]\":87, \"[unused87]\":88, \"[unused88]\":89, \"[unused89]\":90, \"[unused90]\":91, \"[unused91]\":92, \"[unused92]\":93, \"[unused93]\":94, \"[unused94]\":95, \"[unused95]\":96, \"[unused96]\":97, \"[unused97]\":98, ...})),\n",
       " '_decode_use_source_tokenizer': False,\n",
       " 'init_inputs': (),\n",
       " 'init_kwargs': {'do_lower_case': True,\n",
       "  'unk_token': AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "  'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "  'pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "  'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "  'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "  'tokenize_chinese_chars': True,\n",
       "  'strip_accents': None,\n",
       "  'name_or_path': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'do_basic_tokenize': True,\n",
       "  'never_split': None,\n",
       "  'model_max_length': 512,\n",
       "  'max_length': 128,\n",
       "  'truncation_side': 'right',\n",
       "  'stride': 0,\n",
       "  'truncation_strategy': 'longest_first',\n",
       "  'pad_token_type_id': 0,\n",
       "  'padding_side': 'right',\n",
       "  'pad_to_multiple_of': None},\n",
       " 'name_or_path': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       " '_processor_class': None,\n",
       " 'model_max_length': 512,\n",
       " 'padding_side': 'right',\n",
       " 'truncation_side': 'right',\n",
       " 'model_input_names': ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'clean_up_tokenization_spaces': False,\n",
       " 'split_special_tokens': False,\n",
       " 'deprecation_warnings': {},\n",
       " '_in_target_context_manager': False,\n",
       " 'chat_template': None,\n",
       " '_bos_token': None,\n",
       " '_eos_token': None,\n",
       " '_unk_token': AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '_sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '_pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '_cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '_mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '_pad_token_type_id': 0,\n",
       " '_additional_special_tokens': [],\n",
       " 'verbose': False,\n",
       " 'do_lower_case': True}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 2048 \n",
    "OVERLAP = 100\n",
    "\n",
    "# original model we were using has max context length 512, and was trained with only 128 token long sequences\n",
    "def chunk_text_with_overlap(text, max_tokens=MAX_TOKENS, overlap=OVERLAP, tokenizer=tokenizer):\n",
    "    \"\"\"Chunks text within token limits with overlap for context retention.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)  # Tokenize the input text\n",
    "    chunks = []\n",
    "\n",
    "    # Create sliding window chunks with overlap\n",
    "    for i in range(0, len(tokens), max_tokens - overlap):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore(embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5242 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "for file in chosenfiles:\n",
    "    chunks = chunk_text_with_overlap(get_html(file))\n",
    "    for chunk in chunks:\n",
    "        # embedding = embedding_model.embed_query(chunk) # inmemoryvectorstore doesn't check if you set embeddings when adding documents/texts.\n",
    "        # the redis one fortunately does\n",
    "        vector_store.add_texts([chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_store.store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7f7412ce-d1e8-45c3-97ef-4cbf39559e32', page_content=\"nexus / cml - umiacs nexus / cml from umiacs jump to navigation jump to search the compute nodes from cml ' s previous standalone cluster have folded into nexus as of the scheduled maintenance window for august 2023 ( thursday 08 / 17 / 2023, 5 - 8pm ). the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 usage 2 partitions 3 accounts 4 qos 5 storage 5. 1 home directories 5. 2 project directories 5. 3 scratch directories 5. 3. 1 network scratch directory 5. 3. 2 local scratch directories 5. 4 datasets 5. 5 models usage you can ssh to nexuscml. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexuscml00. umiacs. umd. edu nexuscml01. umiacs. umd. edu all partitions, qoses, and account names from the standalone cml cluster have been moved over to nexus. however, please note that cml - is prepended to all of the values that were present in the standalone cml cluster to distinguish them from existing values in nexus. the lone exception is the base account that was named cml in the standalone cluster ( it is also named just cml in nexus ). here are some before / after examples of job submission with various parameters : standalone cml cluster submission command nexus cluster submission command srun - - partition = dpart - - qos = medium - - account = tomg - - gres = gpu : rtx2080ti : 2 - - pty bash srun - - partition = cml - dpart - - qos = cml - medium - - account = cml - tomg - - gres = gpu : rtx2080ti : 2 - - pty bash srun - - partition = cpu - - qos = cpu - - pty bash srun - - partition = cml - cpu - - qos = cml - cpu - - account = cml - - pty bash srun - - partition = scavenger - - qos = scavenger - - account = scavenger - - gres = gpu : 4 - - pty bash srun - - partition = cml - scavenger - - qos = cml - scavenger - - account = cml - scavenger - - gres = gpu : 4 - - pty bash cml users ( exclusively ) can schedule non - interruptible jobs on cml nodes with any non - scavenger job parameters. please note that the cml - dpart partition has a grptres limit of 100 % of the available cores / ram on all cml # # nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. it also has a max submission limit of 500 jobs per user simultaneously so as to not overload the cluster. this is codified by the partition qos named cml. please note that the cml compute nodes are also in the institute - wide scavenger partition in nexus. cml users still have scavenging priority over these nodes via the cml - scavenger partition ( i. e., all cml - partition jobs ( other than cml - scavenger ) can preempt both cml - scavenger and scavenger partition jobs, and cml - scavenger partition jobs can preempt scavenger partition jobs ). partitions there are three partitions available to general cml slurm users. you must specify a partition when submitting your job. cml - dpart - this is the default partition. job allocations are guaranteed. cml - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs in other cml - partitions are ready to be scheduled. cml - cpu - this partition is for cpu focused jobs. job allocations are guaranteed. there are two additional partitions available solely to specific faculty members and their sponsored accounts. cml - furongh - this partition is for exclusive priority access to dr. furong huang ' s purchased a6000 node. job allocations are guaranteed. cml - zhou - this partition is for exclusive priority access to dr. tianyi zhou ' s purchased nodes. job allocations are guaranteed. accounts the center has a base slurm account cml which has a modest number of guaranteed billing resources available to all cluster users at any given time. other faculty that have invested in the cluster have an additional account provided to their sponsored accounts on the cluster, which provides a number of guaranteed billing resources corresponding to the amount that they invested. if you do not specify an account when submitting your job, you will receive the cml account, which only has access to the cml - cpu, cml - default, and cml - medium qoses ( see below section ). if you need access to a different qos, or if the cml account is at its billing limit ( see below in this section ), please use your faculty sponsor ' s account if they have one available. however, keep in mind that if you use your faculty sponsor has their own named partition ( see previous section ), using the faculty - specific account in the cml - dpart partition may block access to resources in the faculty - specific partition, since the billing limit for the account is charged regardless of what partition is being used. the current faculty accounts are : cml - abhinav cml - cameron cml - furongh cml - hajiagha cml - john cml - ramani cml - sfeizi cml - tokekar cml - tomg cml - zhou $ sacctmgr show account format = account % 20, description % 30, organization % 10 account descr org - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -......... cml cml cml cml - abhinav cml - abhinav shrivastava cml cml - cameron cml - maria cameron cml cml - furongh cml - furong huang cml cml - hajiagha cml - mohammad hajiaghayi cml cml - john cml - john dickerson cml cml - ramani cml - ramani duraiswami cml cml - scavenger cml - scavenger cml cml - sfeizi cml - soheil feizi cml cml - tokekar cml - pratap tokekar cml cml - tomg cml - tom goldstein cml cml - zhou cml - tianyi zhou cml......... faculty can manage this list of users via our directory application in the security groups section. the security group that controls access has the prefix cml _ and then the faculty username. it will also list slurm : / / nexusctl. umiacs. umd. edu as the associated uri. you can check your account associations by running the show _ assoc command to see the accounts you are associated with. please contact staff and include your faculty member in the conversation if you do not see the appropriate association. $ show _ assoc user account maxjobs grptres qos - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -......... tomg cml cml - cpu, cml - default, cml - medium tomg cml - scavenger cml - scavenger tomg cml - tomg cml - default, cml - high, cml - medium......... you can also see the total number of track - able resources ( tres ) allowed for each account by running the following command. please make sure you give the appropriate account that you are looking for. the billing number displayed here is the sum of resource weightings for all nodes appropriated to that account. $ sacctmgr show assoc account = cml format = user, account, qos, grptres user account qos grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - cml billing = 6481...... qos cml currently has 5 qos for the cml - d\"),\n",
       " Document(id='61e7a3ca-77b8-4b2c-9d77-8c4fb5884afe', page_content='nexus - umiacs nexus from umiacs jump to navigation jump to search the nexus is the combined scheduler of resources in umiacs. the resource manager for nexus is slurm. resources are arranged into partitions where users are able to schedule computational jobs. users are arranged into a number of slurm accounts based on faculty, lab, or center investments. contents 1 getting started 1. 1 access 1. 2 jobs 1. 2. 1 interactive 1. 2. 2 batch 2 partitions 3 quality of service ( qos ) 3. 1 job qos 3. 2 partition qos 4 storage 4. 1 home directories 4. 2 scratch directories 4. 2. 1 network scratch directories 4. 2. 2 local scratch directories 4. 3 faculty allocations 4. 4 project allocations 4. 5 datasets getting started all accounts in umiacs are sponsored. if you don \\' t already have a umiacs account, please see accounts for information on getting one. you need a full umiacs account ( not a collaborator account ) in order to access nexus. access your access to submission nodes ( alternatively called login nodes ) for nexus computational resources is determined by your account sponsor \\' s department, center, or lab affiliation. you can log into the umiacs directory cr application and select the computational resource ( cr ) in the list that has the prefix nexus. the hosts section lists your available submission nodes - generally a pair of nodes of the format nexus < department, lab, or center abbreviation > [ 00, 01 ], e. g., nexusgroup00 and nexusgroup01. note - umiacs requires multi - factor authentication through our duo instance. this is completely discrete from both umd \\' s and csd \\' s duo instances. you will need to enroll one or more devices to access resources in umiacs, and will be prompted to enroll when you log into the directory application for the first time. once you have identified your submission nodes, you can ssh directly into them. from there, you are able to submit to the cluster via our slurm workload manager. you need to make sure that your submitted jobs have the correct account, partition, and qos. jobs slurm jobs are submitted by either srun or sbatch depending if you are doing an interactive job or batch job, respectively. you need to provide the where / how / who to run the job and specify the resources you need to run with. for the who / where / how, you may be required to specify - - account, - - partition, and / or - - qos ( respectively ) to be able to adequately submit jobs to the nexus. for resources, you may need to specify - - time for time, - - ntasks for cpus, - - mem for ram, and - - gres = gpu for gpus in your submission arguments to meet your requirements. there are defaults for all four, so if you don \\' t specify something, you may be scheduled with a very minimal set of time and resources ( e. g., by default, no gpus are included if you do not specify - - gres = gpu ). for more information about submission flags for gpu resources, see here. you can also can run man srun on your submission node for a complete list of available submission arguments. for a list of available gpu types on nexus and their specs, please see nexus / gpus. interactive once logged into a submission node, you can run simple interactive jobs. if your session is interrupted from the submission node, the job will be killed. as such, we encourage use of a terminal multiplexer such as tmux. $ srun - - pty - - ntasks = 4 - - mem = 2gb - - gres = gpu : 1 nvidia - smi - l gpu 0 : nvidia rtx a4000 ( uuid : gpu - ae5dc1f5 - c266 - 5b9f - 58d5 - 7976e62b3ca1 ) batch batch jobs are scheduled with a script file with an optional ability to embed job scheduling parameters via variables that are defined by # sbatch lines at the top of the file. you can find some examples in our slurm / jobsubmission documentation. partitions the slurm resource manager uses partitions to act as job queues which can restrict size, time and user limits. the nexus has a number of different partitions of resources. different centers, labs, and faculty are able to invest in computational resources that are restricted to approved users through these partitions. partitions usable by all non - class account users : nexus / tron - pool of resources available to all umiacs and csd faculty and graduate students. scavenger - preemption partition that supports nodes from multiple other partitions. more resources are available to schedule simultaneously than in other partitions, however jobs are subject to preemption rules. you are responsible for ensuring your jobs handle this preemption correctly. the slurm scheduler will simply restart a preempted job with the same submission arguments when it is available to run again. for an overview of things you can check within scripts to determine if your job was preempted / resumed, see slurm / preemption. partitions usable by classaccounts : class - pool available for umiacs class accounts sponsored by either umiacs or csd faculty. partitions usable by specific lab / center users : nexus / cbcb - cbcb lab pool available for cbcb lab members. nexus / clip - clip lab pool available for clip lab members. nexus / cml - cml lab pool available for cml lab members. nexus / gamma - gamma lab pool available for gamma lab members. nexus / mbrc - mbrc lab pool available for mbrc lab members. nexus / mc2 - mc2 lab pool available for mc2 lab members. nexus / vulcan - vulcan lab pool available for vulcan lab members. quality of service ( qos ) slurm uses quality of service ( qos ) both to provide limits on job sizes ( termed by us as \" job qos \" ) as well as to limit resources used by all jobs running in a partition, either per user or per group ( termed by us as \" partition qos \" ). job qos job qos are used to provide limits on the size of job that you can run. you should try to allocate only the resources your job actually needs, as resources that each of your jobs schedules are counted against your fair - share priority in the future. default - default job qos. limited to 4 cpu cores, 1 gpu, and 32gb ram per job. the maximum wall time per job is 3 days. medium - limited to 8 cpu cores, 2 gpus, and 64gb ram per job. the maximum wall time per job is 2 days. high - limited to 16 cpu cores, 4 gpus, and 128gb ram per job. the maximum wall time per job is 1 day. scavenger - no resource limits per job, only a maximum wall time per job of 3 days. you are responsible for ensuring your job requests multiple nodes if it requests resources beyond what any one node is capable of. 576 total cpu cores, 72 total gpus, and 2304gb total ram are permitted simultaneously across all of your jobs running with this job qos. this job qos is both only available in the scavenger partition and the only job qos available in the scavenger partition. to use this job qos, include - - partition = scavenger and - - account = scavenger in your submission arguments. do not include any job qos argument other than - - qos = scavenger ( optional ) or submission will fail. you can display these job qos from the command line using the show _ qos command. by default, the command will only show job qos that you can access. the above four job qos are the ones that everyone can access. $ show _ qos name maxwall maxtres maxjobspu maxtrespu - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - default 3 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g high 1 - 00 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g medium 2 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g scavenger 3 - 00 : 00 : 00 cpu = 576, gres / gpu = 72, mem = 2304g if you want to see all job qos, including those that you do not have access to, you can use the show _ qos - - all command. $ show _ qos - - all name maxwall maxtres maxjobspu maxtrespu - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -'),\n",
       " Document(id='55f1a7a6-f2fe-4208-abe5-5113f8a49eb6', page_content=\"nexus / vulcan - umiacs nexus / vulcan from umiacs jump to navigation jump to search the compute nodes from vulcan ' s previous standalone cluster have folded into nexus as of the scheduled maintenance window for august 2023 ( thursday 08 / 17 / 2023, 5 - 8pm ). the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 usage 2 nodes 3 partitions 4 accounts 5 qos 6 storage 6. 1 home directories 6. 2 scratch directories 6. 2. 1 network scratch directory 6. 2. 2 local scratch directories 6. 3 datasets 6. 4 project storage 6. 5 object storage 7 migration 7. 1 home directories usage you can ssh to nexusvulcan. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexusvulcan00. umiacs. umd. edu nexusvulcan01. umiacs. umd. edu all partitions, qoses, and account names from the standalone vulcan cluster have been moved over to nexus. however, please note that vulcan - is prepended to all of the values that were present in the standalone vulcan cluster to distinguish them from existing values in nexus. the lone exception is the base account that was named vulcan in the standalone cluster ( it is also named just vulcan in nexus ). here are some before / after examples of job submission with various parameters : standalone vulcan cluster submission command nexus cluster submission command srun - - partition = dpart - - qos = medium - - account = abhinav - - gres = gpu : gtx1080ti : 2 - - pty bash srun - - partition = vulcan - dpart - - qos = vulcan - medium - - account = vulcan - abhinav - - gres = gpu : gtx1080ti : 2 - - pty bash srun - - partition = cpu - - qos = cpu - - pty bash srun - - partition = vulcan - cpu - - qos = vulcan - cpu - - account = vulcan - - pty bash srun - - partition = scavenger - - qos = scavenger - - account = vulcan - - gres = gpu : 4 - - pty bash srun - - partition = vulcan - scavenger - - qos = vulcan - scavenger - - account = vulcan - - gres = gpu : 4 - - pty bash vulcan users ( exclusively ) can schedule non - interruptible jobs on vulcan nodes with any non - scavenger job parameters. please note that the vulcan - dpart partition has a grptres limit of 100 % of the available cores / ram on all vulcan # # in aggregate nodes plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. it also has a max submission limit of 500 jobs per user simultaneously so as to not overload the cluster. this is codified by the partition qos named vulcan. please note that the vulcan compute nodes are also in the institute - wide scavenger partition in nexus. vulcan users still have scavenging priority over these nodes via the vulcan - scavenger partition ( i. e., all vulcan - partition jobs ( other than vulcan - scavenger ) can preempt both vulcan - scavenger and scavenger partition jobs, and vulcan - scavenger partition jobs can preempt scavenger partition jobs ). nodes there are currently 46 gpu nodes available running a mixture of nvidia rtx a6000, nvidia rtx a5000, nvidia rtx a4000, nvidia quadro p6000, nvidia geforce gtx 1080 ti, nvidia geforce rtx 2080 ti, and nvidia tesla p100 cards. there are also 4 cpu - only nodes available. all nodes are scheduled with the slurm resource manager. partitions there are three partitions available to general vulcan slurm users. you must specify a partition when submitting your job. vulcan - dpart - this is the default partition. job allocations are guaranteed. only nodes with gpus from architectures before nvidia ' s ampere architecture are included in this partition. vulcan - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs in other vulcan - partitions are ready to be scheduled. vulcan - cpu - this partition is for cpu focused jobs. job allocations are guaranteed. there are a few additional partitions available to subsets of vulcan users based on specific requirements. vulcan - ampere - this partition contains nodes with gpus from nvidia ' s ampere architecture. job allocations are guaranteed. as of thursday 02 / 29 / 2024 at 12pm, there is a 4 hour time limit on interactive jobs in this partition. if you need to run longer jobs, you will need to modify your workflow into a job that can be submitted as a batch script. as of thursday 03 / 21 / 2024 at 5pm, there is a limit of 4 cpus and 48g memory maximum per gpu requested by a job. if you need to run jobs with more cpus / memory, you will either need to request more gpus in the job or use a different partition. submission is restricted to the slurm accounts of the faculty who invested in these nodes : abhinav shrivastava ( vulcan - abhinav ) jia - bin huang ( vulcan - jbhuang ) christopher metzler ( vulcan - metzler ) matthias zwicker ( vulcan - zwicker ) accounts vulcan has a base slurm account vulcan which has a modest number of guaranteed billing resources available to all cluster users at any given time. other faculty that have invested in vulcan compute infrastructure have an additional account provided to their sponsored accounts on the cluster. if you do not specify an account when submitting your job, you will receive the vulcan account. if your faculty sponsor has their own account, it is recommended to use that account for job submission. the current faculty accounts are : vulcan - abhinav vulcan - djacobs vulcan - jbhuang vulcan - lsd vulcan - metzler vulcan - rama vulcan - ramani vulcan - yaser vulcan - zwicker $ sacctmgr show account format = account % 20, description % 30, organization % 10 account descr org - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -......... vulcan vulcan vulcan vulcan - abhinav vulcan - abhinav shrivastava vulcan vulcan - djacobs vulcan - david jacobs vulcan vulcan - jbhuang vulcan - jia - bin huang vulcan vulcan - lsd vulcan - larry davis vulcan vulcan - metzler vulcan - chris metzler vulcan vulcan - rama vulcan - rama chellappa vulcan vulcan - ramani vulcan - ramani duraiswami vulcan vulcan - yaser vulcan - yaser yacoob vulcan vulcan - zwicker vulcan - matthias zwicker vulcan......... faculty can manage this list of users via our directory application in the security groups section. the security group that controls access has the prefix vulcan _ and then the faculty username. it will also list slurm : / / nexusctl. umiacs. umd. edu as the associated uri. you can check your account associations by running the show _ assoc command to see the accounts you are associated with. please contact staff and include your faculty member in the conversation if you do not see the appropriate association. $ show _ assoc user account maxjobs grptres qos - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -............ abhinav vulcan 48 vulcan - cpu, vulcan - default, vulcan - medium, vulcan - scavenger abhinav vulcan - abhinav 48 vulcan - cpu, vulcan - default, vulcan - high, vulcan - medium, vulcan - scavenger............ you can also see the total number of track - able resources ( tres ) allowed for each account by running the following command. please make sure you give the appropriate account that you are looking for. as shown below, there is a concurrent limit of 64 total gpus for all users not in a contributing faculty group. $ sacctmgr show assoc account = vulcan format = user, account, qos, grptres user account qos grptres\"),\n",
       " Document(id='17484e30-be16-4e0e-9c71-a760bcd89456', page_content=\"nexus / gamma - umiacs nexus / gamma from umiacs jump to navigation jump to search the gamma lab has a partition of gpu nodes available in the nexus. only gamma lab members are able to run non - interruptible jobs on these nodes. contents 1 access 2 quality of service 3 hardware 4 example 5 storage 5. 1 home directories 5. 2 project directories 5. 3 scratch directories 5. 3. 1 network scratch directory 5. 3. 2 local scratch directories 5. 4 datasets access you can always find out what hosts you have access to submit via the nexus # access page. the gamma lab in particular has a special submission host that has additional local storage available. nexusgamma00. umiacs. umd. edu please do not run anything on the login node. always allocate yourself machines on the compute nodes ( see instructions below ) to run any job. quality of service gamma users have access to all of the standard job qoses in the gamma partition using the gamma account. the additional job qoses for the gamma partition specifically are : huge - long : allows for longer jobs using higher overall resources. please note that the partition has a grptres limit of 100 % of the available cores / ram on the partition - specific nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. hardware nodenames type quantity cpu cores per node memory per node gpus per node gammagpu [ 00 - 04, 06 - 09 ] a5000 gpu node 9 32 256gb 8 gammagpu05 a4000 gpu node 1 32 256gb 8 total 10 320 2560gb 80 example from nexusgamma00. umiacs. umd. edu you can run the following example to submit an interactive job. please note that you need to specify the - - partition and - - account. please refer to our slurm documentation about about how to further customize your submissions including making a batch submission. the following command will allocate 8 gpus for 2 days in an interactive session. change parameters accordingly to your needs. we discourage use of srun and promote use of sbatch for fair use of gpus. $ srun - - pty - - gres = gpu : 8 - - account = gamma - - partition = gamma - - qos = huge - long bash $ hostname gammagpu01. umiacs. umd. edu $ nvidia - smi - l gpu 0 : nvidia rtx a5000 ( uuid : gpu - cdfb2e0c - d69f - 354b - 02f4 - 15161dc7fa66 ) gpu 1 : nvidia rtx a5000 ( uuid : gpu - be53e7a1 - b8fd - 7089 - 3cac - 7a2fbf4ec7dd ) gpu 2 : nvidia rtx a5000 ( uuid : gpu - 774efbb1 - d7ec - a0bb - e992 - da9d1fa6b193 ) gpu 3 : nvidia rtx a5000 ( uuid : gpu - d1692181 - c7de - e273 - 5f95 - 53ad381614c3 ) gpu 4 : nvidia rtx a5000 ( uuid : gpu - ba51fd6c - 37bf - 1b95 - 5f68 - 987c18a6292a ) gpu 5 : nvidia rtx a5000 ( uuid : gpu - c1224a2a - 4a3b - ff16 - 0308 - 4f36205b9859 ) gpu 6 : nvidia rtx a5000 ( uuid : gpu - 8d20d6cd - abf5 - 2630 - ab88 - 6bba438c55fe ) gpu 7 : nvidia rtx a5000 ( uuid : gpu - 93170910 - 5d94 - 6da5 - 8a24 - f561d7da1e2d ) you can also use sbatch to submit your job. here are two examples on how to do that. $ sbatch - - pty - - gres = gpu : 8 - - account = gamma - - partition = gamma - - qos = huge - long - - time = 1 - 23 : 00 : 00 script. sh or $ sbatch script. sh / / script. sh / / #! / bin / bash # sbatch - - gres = gpu : 8 # sbatch - - account = gamma # sbatch - - partition = gamma # sbatch - - qos = huge - long # sbatch - - time = 1 - 23 : 00 : 00 python your _ file. py storage there are 3 types of user storage available to users in gamma : home directories project directories scratch directories there is also read - only storage available for dataset directories. gamma users can also request nexus project allocations. home directories you have 30gb of home directory storage available at / nfshomes / < username >. it has both snapshots and backups enabled. home directories are intended to store personal or configuration files only. we encourage you to not share any data in your home directory. you are encouraged to utilize our gitlab infrastructure to host your code repositories. note : to check your quota on this directory, use the command df - h ~. project directories you can request project based allocations for up to 8tb and up to 180 days with approval from a gamma faculty member. to request an allocation, please contact staff with the faculty member ( s ) that approved the project in the conversation. please include the following details : project name ( short ) description size ( 1tb, 2tb, etc. ) length in days ( 30 days, 90 days, etc. ) other user ( s ) that need to access the allocation, if any these allocations will be available from / fs / gamma - projects under a name that you provide when you request the allocation. near the end of the allocation period, staff will contact you and ask if you would like to renew the allocation ( requires re - approval from a gamma faculty member ). if you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period. staff will then remove the allocation. if you do not respond to staff ' s request by the end of the allocation period, staff will make the allocation temporarily inaccessible. if you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible. if one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation. this data is backed up nightly. scratch directories scratch data has no data protection, there are no snapshots and the data is not backed up. there are two types of scratch directories : network scratch directory local scratch directories network scratch directory you are allocated 100gb of scratch space via nfs from / gammascratch / $ username. it is not backed up or protected in any way. this directory is automounted so you may not see your directory if you run ls / gammascratch but it will be mounted when you cd into your / gammascratch directory. you may request a permanent increase of up to 200gb total space without any faculty approval by contacting staff. if you need space beyond 200gb, you will need faculty approval. this file system is available on all submission, data management, and computational nodes within the cluster. local scratch directories these file systems are not available over nfs and there are no backups or snapshots available for these file systems. each computational node that you can schedule compute jobs on has one or more local scratch directories. these are always named / scratch0, / scratch1, etc. these directories are local to each node, ie. the / scratch0 on two different nodes are completely separate. these directories are almost always more performant than any other storage available to the job. however, you must stage data to these directories within the confines of your jobs and stage the data out before the end of your jobs. these local scratch directories have a tmpwatch job which will delete unaccessed data after 90 days, scheduled via maintenance jobs to run once a month during our monthly maintenance windows. again, please make sure you secure any data you write to these directories at the end of your job. gamma has invested in a 20tb nvme scratch file system on nexusgamma00. umiacs. umd. edu that is available as / scratch1. to utilize this space, you will need to copy data from / to this over ssh from a compute node. to make this easier, you may want to setup ssh keys that will allow you to copy data without prompting for passwords. the / scratch1 directory on nexusgamma00. umiacs. umd. edu doesn ' t have a tmpwatch. the files in this directory need to be manually removed once they are no longer needed. datasets we have read - only dataset storage available at / fs / gamma - datasets. if there are datasets that you would like to see\"),\n",
       " Document(id='41992650-a6f7-435a-8ad3-5a091c6318f7', page_content='of $ numjobs \" done it is suggested that you run the outer submission script in a tmux session to keep the terminal window executing it from being interrupted. storage all network storage available in nexus is currently nfs based, and comes in a few different flavors. compute nodes also have local storage that can be used. home directories you have 30gb of home directory storage available at / nfshomes / < username >. it has both snapshots and backups enabled. home directories are intended to store personal or configuration files only. we encourage you to not share any data in your home directory. you are encouraged to utilize our gitlab infrastructure to host your code repositories. note : to check your quota on this directory, use the command df - h ~. scratch directories scratch data has no data protection including no snapshots and the data is not backed up. there are two types of scratch directories in the nexus compute infrastructure : network scratch directories local scratch directories please note that class accounts do not have network scratch directories. network scratch directories you are allocated 200gb of scratch space via nfs from / fs / nexus - scratch / < username > where < username > is your umiacs username. it is not backed up or protected in any way. this directory is automounted ; you will need to cd into the directory or request / specify a fully qualified file path to access it. you can view your quota usage by running df - h / fs / nexus - scratch / < username >. you may request a permanent increase of up to 400gb total space without any faculty approval by contacting staff. if you need space beyond 400gb, you will need faculty approval and / or a project allocation for this. if you choose to increase your scratch space beyond 400gb, the increased space is also subject to the 270 tb days limit mentioned in the project allocation section before we check back in for renewal. for example, if you request 1. 4tb total space, you may have this for 270 days ( 1tb beyond the 400gb permanent increase ). the amount increased beyond 400gb will also count against your faculty member \\' s 20tb total storage limit mentioned below. this file system is available on all submission, data management, and computational nodes within the cluster. local scratch directories each computational node that you can schedule compute jobs on also has one or more local scratch directories. these are always named / scratch0, / scratch1, etc. and are not backed up or protected in any way. these directories are almost always more performant than any other storage available to the job as they are mounted from disks directly attached to the compute node. however, you must stage your data within the confines of your job and extract the relevant resultant data elsewhere before the end of your job. these local scratch directories have a tmpwatch job which will delete unaccessed data after 90 days, scheduled via maintenance jobs to run once a month during our monthly maintenance windows. please make sure you secure any resultant data you wish to keep from these directories at the end of your job. faculty allocations each faculty member can be allocated 1tb of permanent lab space upon request. we can also support grouping these individual allocations together into larger center, lab, or research group allocations if desired by the faculty. please contact staff to inquire. lab space storage is fully protected. it has snapshots enabled and is backed up nightly. project allocations project allocations are available per user for 270 tb days ; you can have a 1tb allocation for up to 270 days, a 3tb allocation for 90 days, etc.. a single faculty member can not have more than 20tb of project allocations across all of their sponsored accounts active simultaneously. network scratch allocation space increases beyond the 400gb permanent maximum also have the increase count against this limit ( i. e., a 1tb network scratch allocation would have 600gb counted towards this limit ). project storage is fully protected. it has snapshots enabled and is backed up nightly. the maximum allocation length you can request is 540 days ( 500gb space ) and the maximum storage space you can request is 9tb ( 30 day length ). to request an allocation, please contact staff with the faculty member ( s ) that the project is under involved in the conversation. please include the following details : project name ( short ) description size ( 1tb, 2tb, etc. ) length in days ( 270 days, 135 days, etc. ) other user ( s ) that need to access the allocation, if any these allocations are available via / fs / nexus - projects / < project name >. renewal is not guaranteed to be available due to limits on the amount of total storage. near the end of the allocation period, staff will contact you and ask if you are still in need of the storage allocation. if renewal is available, you can renew for up to another 270 tb days with reapproval from the original faculty approver. if you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period. staff will then remove the allocation. if you do not respond to staff \\' s request by the end of the allocation period, staff will make the allocation temporarily inaccessible. if you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible. if one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation. datasets we have read - only dataset storage available at / fs / nexus - datasets. if there are datasets that you would like to see curated and available, please see this page. the list of nexus datasets we currently host can be viewed here. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus & oldid = 12024 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 13 september 2024, at 13 : 21. privacy policy about umiacs disclaimers'),\n",
       " Document(id='d1bdf0ec-28ef-43eb-a8a6-e9d4528a33e8', page_content='##3269042969 1699 10. 129261016845703 1799 9. 685370445251465 1899 9. 391674041748047 1999 9. 19735336303711 result : y = 0. 0022362577728927135 + 0. 837898313999176 x + - 0. 0003857926349155605 x ^ 2 + - 0. 09065020829439163 x ^ 3 bind mounts to get data into the container you need to pass some bind mounts. apptainer containers will not automatically mount data from the outside operating system other than your home directory. users need to manually bind mounts for other file paths. - - bind / fs / nexus - scratch / < username > / < projectname > : / mnt in this example, we will exec an interactive session with gpus and binding our nexus scratch directory which allows us to specify the command we want to run inside the container. apptainer exec - - nv - - bind / fs / nexus - scratch / username : / fs / nexus - scratch / username / fs / nexus - containers / pytorch / pytorch _ 1. 13. 0 + cu117. sif bash you can now write / run your own pytorch python code interactively within the container or just make a python script that you can call directly from the apptainer exec command for batch processing. shared containers portable images called singularity image format or. sif files can be copied and shared. nexus maintains some shared containers in / fs / nexus - containers. these are arranged by the application ( s ) that are installed. docker workflow example we have a pytorch _ docker example workflow using our gitlab as a docker registry. you can clone the repository and further customize this to your needs. the workflow is : run docker on a laptop or personal desktop on to create the image, or use podman on a umiacs - supported system. tag the image and and push it to your repository ( this can be any docker registry ) pull the image down onto one of our workstations / clusters and run it with your data. $ apptainer pull pytorch _ docker. sif docker : / / registry. umiacs. umd. edu / derek / pytorch _ docker info : converting oci blobs to sif format info : starting build... getting image source signatures copying blob 85386706b020 done... 2022 / 10 / 14 10 : 58 : 36 info unpack layer : sha256 : b6f46848806c8750a68edc4463bf146ed6c3c4af18f5d3f23281dcdfb1c65055 2022 / 10 / 14 10 : 58 : 43 info unpack layer : sha256 : 44845dc671f759820baac0376198141ca683f554bb16a177a3cfe262c9e368ff info : creating sif file... $ apptainer exec - - nv pytorch _ docker. sif python3 - c \\' from _ _ future _ _ import print _ function ; import torch ; print ( torch. cuda. current _ device ( ) ) ; x = torch. rand ( 5, 3 ) ; print ( x ) \\' 0 tensor ( [ [ 0. 3273, 0. 7174, 0. 3587 ], [ 0. 2250, 0. 3896, 0. 4136 ], [ 0. 3626, 0. 0383, 0. 6274 ], [ 0. 6241, 0. 8079, 0. 2950 ], [ 0. 0804, 0. 9705, 0. 0030 ] ] ) retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = apptainer & oldid = 11997 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 13 august 2024, at 17 : 51. privacy policy about umiacs disclaimers'),\n",
       " Document(id='ffa18264-970a-4d9b-b305-da4bd727b154', page_content='nexus / tron - umiacs nexus / tron from umiacs jump to navigation jump to search the tron partition is a subset of resources available in the nexus. it was purchased using college - level funding for umiacs and csd faculty. hardware the full configuration includes 70 nodes with specs as detailed below. nodenames type quantity cpu cores per node memory per node gpus per node tron [ 00 - 05 ] a6000 gpu node 6 32 256gb 8 tron [ 06 - 44 ] a4000 gpu node 39 16 128gb 4 tron [ 46 - 61 ] a5000 gpu node 16 48 256gb 8 tron [ 62 - 69 ] rtx 2080 ti gpu node 8 32 384gb 8 tron [ 00 - 44, 46 - 69 ] total 69 1840 13282gb 396 retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / tron & oldid = 11884 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 12 june 2024, at 15 : 56. privacy policy about umiacs disclaimers'),\n",
       " Document(id='0fddf1e7-7477-486c-a2ad-ff69ef371d46', page_content='nexus / mbrc - umiacs nexus / mbrc from umiacs jump to navigation jump to search the compute nodes from mbrc \\' s previous standalone cluster have folded into nexus as of mid 2023. the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 submission nodes 2 resources 3 qos 4 jobs 5 storage 5. 1 project directories submission nodes you can ssh to nexusmbrc. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexusmbrc00. umiacs. umd. edu nexusmbrc01. umiacs. umd. edu resources the mbrc partition has nodes brought over from the previous standalone mbrc slurm scheduler. the compute nodes are named mbrc # #. qos mbrc users have access to all of the standard job qoses in the mbrc partition using the mbrc account. the additional job qoses for the mbrc partition specifically are : huge - long : allows for longer jobs using higher overall resources. please note that the partition has a grptres limit of 100 % of the available cores / ram on the partition - specific nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. jobs you will need to specify - - partition = mbrc and - - account = mbrc to be able to submit jobs to the mbrc partition. [ username @ nexusmbrc00 : ~ ] $ srun - - pty - - ntasks = 4 - - mem = 8g - - qos = default - - partition = mbrc - - account = mbrc - - time 1 - 00 : 00 : 00 bash srun : job 218874 queued and waiting for resources srun : job 218874 has been allocated resources [ username @ mbrc00 : ~ ] $ scontrol show job 218874 jobid = 218874 jobname = bash userid = username ( 1000 ) groupid = username ( 21000 ) mcs _ label = n / a priority = 897 nice = 0 account = mbrc qos = default jobstate = running reason = none dependency = ( null ) requeue = 1 restarts = 0 batchflag = 0 reboot = 0 exitcode = 0 : 0 runtime = 00 : 00 : 06 timelimit = 1 - 00 : 00 : 00 timemin = n / a submittime = 2022 - 11 - 18t11 : 13 : 56 eligibletime = 2022 - 11 - 18t11 : 13 : 56 accruetime = 2022 - 11 - 18t11 : 13 : 56 starttime = 2022 - 11 - 18t11 : 13 : 56 endtime = 2022 - 11 - 19t11 : 13 : 56 deadline = n / a preempteligibletime = 2022 - 11 - 18t11 : 13 : 56 preempttime = none suspendtime = none secspresuspend = 0 lastschedeval = 2022 - 11 - 18t11 : 13 : 56 scheduler = main partition = mbrc allocnode : sid = nexusmbrc00 : 25443 reqnodelist = ( null ) excnodelist = ( null ) nodelist = mbrc00 batchhost = mbrc00 numnodes = 1 numcpus = 4 numtasks = 4 cpus / task = 1 reqb : s : c : t = 0 : 0 : * : * tres = cpu = 4, mem = 8g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 8g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage in addition to storage types available to all nexus users, mbrc users can also request mbrc project directories. project directories for this cluster we have decided to allocate network storage on a project by project basis. jonathan heagerty will be the point of contact as it pertains to allocating the requested / required storage for each project. as a whole, the mbrc cluster has limited network storage and for this there will be limits to how much and how long network storage can be appropriated. if the requested storage size is significantly large relative to the total allotted amount, the request will be relayed from jonathan heagerty to the mbrc cluster faculty for approval. two other situations that would need approval from the mbrc cluster faculty would be : to request an increase to a projects current storage allotment or to request a time extension for a projects storage. when making a request for storage please provide the following information when contacting staff : - name of user requesting storage : example : jheager2 - name of project : example : foveated rendering - collaborators working on the project : example : sida li - storage size : example : 1tb - length of time for storage : example : 6 - 8 months retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / mbrc & oldid = 12040 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 17 september 2024, at 16 : 40. privacy policy about umiacs disclaimers')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the documents are >512 tokens long, so I think it's only looking at the first 512\n",
    "vector_store.similarity_search(query=\"python notebook nexus\", k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c5132a82-b07a-4623-84db-40162fc71757', page_content=\"slurm - umiacs slurm from umiacs jump to navigation jump to search contents 1 simple linux utility for resource management ( slurm ) 1. 1 documentation 1. 2 commands 1. 2. 1 srun 1. 2. 2 salloc 1. 2. 3 sbatch 1. 2. 4 squeue 1. 2. 5 scancel 1. 2. 6 sacct 1. 2. 7 sstat 1. 3 modules 1. 4 running jupyter notebook on a compute node 1. 4. 1 setting up your python virtual environment 1. 4. 2 running jupyter notebook 2 quick guide to translate pbs / torque to slurm simple linux utility for resource management ( slurm ) slurm is an open - source workload manager designed for linux clusters of all sizes. it provides three key functions. first, it allocates exclusive or non - exclusive access to resources ( computer nodes ) to users for some duration of time so they can perform work. second, it provides a framework for starting, executing, and monitoring work ( typically a parallel job ) on a set of allocated nodes. finally, it arbitrates contention for resources by managing a queue of pending work. documentation submitting jobs checking job status checking cluster status understanding job priority job preemption overview official documentation faq related documentation : using vs code commands below are some of the common commands used in slurm. further information on how to use these commands is found in the documentation linked above. to see all flags available for a command, please check the command ' s manual by using man < command > on the command line. srun srun runs a parallel job on a cluster managed by slurm. if necessary, it will first create a resource allocation in which to run the parallel job. salloc salloc allocates a slurm job allocation, which is a set of resources ( nodes ), possibly with some set of constraints ( e. g. number of processors per node ). when salloc successfully obtains the requested allocation, it then runs the command specified by the user. finally, when the user specified command is complete, salloc relinquishes the job allocation. if no command is specified, salloc runs the user ' s default shell. sbatch sbatch submits a batch script to slurm. the batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. the batch script may contain options preceded with # sbatch before any executable commands in the script. squeue squeue views job and job step information for jobs managed by slurm. scancel scancel signals or cancels jobs, job arrays, or job steps. an arbitrary number of jobs or job steps may be signaled using job specification filters or a space separated list of specific job and / or job step ids. sacct sacct displays job accounting data stored in the job accounting log file or slurm database in a variety of forms for your analysis. the sacct command displays information on jobs, job steps, status, and exitcodes by default. you can tailor the output with the use of the - - format = option to specify the fields to be shown. sstat sstat displays job status information for your analysis. the sstat command displays information pertaining to cpu, task, node, resident set size ( rss ) and virtual memory ( vm ). you can tailor the output with the use of the - - fields = option to specify the fields to be shown. modules if you are trying to use gnu modules in a slurm job, please read the section of our modules documentation on non - interactive shell sessions. this also needs to be done if the os version of the compute node you are scheduled on is different from the os version of the submission node you are submitting the job from. running jupyter notebook on a compute node the steps to run a jupyter notebook from a compute node are listed below. setting up your python virtual environment create a python virtual environment on the compute node you are assigned and activate it. next, install jupyter using pip by following the steps here. you may also use other environment management systems such as conda if desired. running jupyter notebook after you ' ve set up the python virtual environment, submit a job, activate the environment within the job, and run the following command on the compute node you are assigned : jupyter notebook - - no - browser - - port = 8889 - - ip = 0. 0. 0. 0 this will start running the notebook on port 8889. note : you must keep this shell window open to be able to connect. if the submission node for the cluster you are using is not accessible via the public internet, you must also be on a machine connected to the umiacs network or connected to our vpn in order to access the jupyter notebook once you start the ssh tunnel, so ensure this is the case before starting the tunnel. then, on your local machine, run ssh - n - f - l localhost : 8888 : < nodename > : 8889 < username > @ < submissionnode >. umiacs. umd. edu this will tunnel port 8889 from the compute node to port 8888 on your local machine, using < submissionnode > as an intermediate node. make sure to replace < username > with your username, < submissionnode > with the name of the submission node you want to use, and < nodename > with the name of the compute node you are assigned. note that this command will not display any output if the connection is successful due to the included ssh flags. you must also keep this shell window open to be able to connect. for example, assuming your username is username and that you are using the nexus cluster, have been assigned the nexusgroup submission nodes, and are assigned compute node tron00. umiacs. umd. edu : ssh - n - f - l localhost : 8888 : tron00. umiacs. umd. edu : 8889 username @ nexusgroup. umiacs. umd. edu you can then open a web browser and type in localhost : 8888 to access the notebook. notes : later versions of jupyter have token authentication enabled by default - you will need to prepend the /? token = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx part of the url provided by the terminal output after starting the notebook in order to connect if this is the case. e. g. localhost : 8888 /? token = fcc6bd0f996e7aa89376c33cb34f7b80890502aacc97d98e if the port on the compute node mentioned in the example above ( 8889 ) is not working, it may be that someone else has already started a process ( jupyter notebook or otherwise ) using that specific port number on that specific compute node. the port number can be replaced with any other ephemeral port number you ' d like, just make sure to change it in both the command you run on the compute node and the ssh command from your local machine. quick guide to translate pbs / torque to slurm pbs / torque was the previous workload manager and job submission framework used at umiacs prior to slurm ' s adoption. below is a quick guide of how to translate some common pbs / torque commands to slurm ones. user commands pbs / torque slurm job submission qsub [ filename ] sbatch [ filename ] job deletion qdel [ job _ id ] scancel [ job _ id ] job status ( by job ) qstat [ job _ id ] squeue - - job [ job _ id ] full job status ( by job ) qstat - f [ job _ id ] scontrol show job [ job _ id ] job status ( by user ) qstat - u [ username ] squeue - - user = [ username ] environment variables pbs / torque slurm job id $ pbs _ jobid $ slurm _ jobid submit directory $ pbs _ o _ workdir $ slurm _ submit _ dir node list $ pbs _ nodefile $ slurm _ job _ nodelist job specification pbs / torque slurm script directive # pbs # sbatch job name - n [ name ] - - job - name = [ name ] or - j [ name ] node count - l nodes = [ count ] - - nodes = [ min [ - max ] ] or - n [ min [ - max ] ] cpu count - l ppn = [ count ] - - ntasks - per - node = [ count ] cpus per task - - cpus - per - task = [ count ] memory size - l mem = [ mb ] - - mem = [ mb ] or - - mem - per - cpu = [ mb ] wall clock limit - l walltime = [ hh : mm : ss ] - - time = [ min ] or - - time = [ days - hh : mm : ss ] node properties - l nodes = 4 : ppn = 8 : [ property ] - - constraint = [ list ] standard output file - o [ file _ name ] - - output = [ file _ name ] or - o [ file _ name ] standard error file - e [ file _ name ] - - error = [ file _ name ] or\"),\n",
       " Document(id='6787c945-e429-4f4d-a29a-0b59a00c8ad4', page_content='slurm / arrayjobs - umiacs slurm / arrayjobs from umiacs jump to navigation jump to search here is an example to get you started using array jobs in slurm. array computation example job save this code to a file called test. py. import time print ( \\' start at \\' + time. strftime ( \\' % h : % m : % s \\' ) ) print ( \\' sleep for 10 seconds... \\' ) time. sleep ( 10 ) print ( \\' stop at \\' + time. strftime ( \\' % h : % m : % s \\' ) ) submission script save this to a file called array. sh and you should be able to submit the job as sbatch array. sh. #! / bin / bash # # # # # # # # # # # # # # # # # # # # # # job - array example # # # # # # # # # # # # # # # # # # # # # # # sbatch - - job - name = example # sbatch - - array = 1 - 16 # run 16 jobs at the same time # sbatch - - time = 0 - 00 : 05 : 00 # run for 5 minutes ( d - hh : mm : ss ) # sbatch - - mem - per - cpu = 500mb # use 500mb per core # all bash commands must be after all sbatch directives # define and create a unique scratch directory scratch _ directory = / scratch0 / $ { user } / job - array - example / $ { slurm _ jobid } mkdir - p $ { scratch _ directory } cd $ { scratch _ directory } cp $ { slurm _ submit _ dir } / test. py $ { scratch _ directory } # each job will see a different $ { slurm _ array _ task _ id } echo \" now processing task id : : \" $ { slurm _ array _ task _ id } python test. py > output _ $ { slurm _ array _ task _ id }. txt # after the job is done we copy our output back to $ slurm _ submit _ dir cp output _ $ { slurm _ array _ task _ id }. txt $ { slurm _ submit _ dir } # we step out of the scratch directory and remove it cd $ { slurm _ submit _ dir } rm - rf $ { scratch _ directory } # happy end exit 0 retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / arrayjobs & oldid = 11731 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 28 march 2024, at 20 : 30. privacy policy about umiacs disclaimers'),\n",
       " Document(id='24bc4eb6-c6a6-42dc-ac1d-535edc6b389f', page_content='page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 17 september 2024, at 16 : 37. privacy policy about umiacs disclaimers'),\n",
       " Document(id='cee3797a-38e8-411e-968f-fc70ed9afdd7', page_content='want to setup ssh keys that will allow you to copy data without prompting for passwords. the / scratch1 directory on nexusgamma00. umiacs. umd. edu doesn \\' t have a tmpwatch. the files in this directory need to be manually removed once they are no longer needed. datasets we have read - only dataset storage available at / fs / gamma - datasets. if there are datasets that you would like to see curated and available, please see this page. the list of gamma datasets we currently host can be viewed here. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / gamma & oldid = 11906 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 25 june 2024, at 18 : 25. privacy policy about umiacs disclaimers'),\n",
       " Document(id='0692a555-edb0-4c6b-ae1b-d292e5024bdb', page_content=\"##eon, e5 - 2683, pascal gpu : p6000 : 7, gpu : p100 : 1 vulcan [ 01 - 04, 06 - 07 ] 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 8 vulcan05 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7 janus [ 02 - 04 ] 40 383025 rhel8, xeon, 6248, turing gpu : rtx2080ti : 10 legacygpu00 20 255249 rhel8, xeon, e5 - 2650, pascal gpu : titanxp : 4 legacygpu [ 01 - 02, 07 ] 20 255249 + rhel8, xeon, e5 - 2650, maxwell gpu : gtxtitanx : 4 legacygpu [ 03 - 04 ] 16 255268 rhel8, xeon, e5 - 2630, maxwell gpu : gtxtitanx : 2 legacygpu05 44 513193 rhel8, xeon, e5 - 2699, pascal gpu : gtx1080ti : 4 vulcan23 32 383030 rhel8, xeon, 4612, turing gpu : rtx2080ti : 8 vulcan26 24 770126 rhel8, xeon, 6146, pascal gpu : titanxp : 10 vulcan [ 27 - 28 ] 56 770093 rhel8, xeon, 8280, turing gpu : rtx2080ti : 10 vulcan24 16 126216 rhel8, zen, 7282, ampere gpu : rtxa6000 : 4 gammagpu [ 01 - 04, 06 - 09 ], vulcan [ 33 - 37 ] 32 255215 + rhel8, zen, epyc - 7313, ampere gpu : rtxa5000 : 8 vulcan [ 38 - 44 ] 32 255215 rhel8, zen, epyc - 7313, ampere gpu : rtxa4000 : 8 note that all of the nodes shown by this may not necessarily be in a partition you are able to submit to. you can identify further specific information about a node using scontrol with various flags. there are also two command aliases developed by umiacs staff to show various node information in aggregate. they are show _ nodes and show _ available _ nodes. show _ nodes the show _ nodes command alias shows each node ' s name, number of cpus, memory, { os, cpu architecture, cpu type, gpu architecture ( if the node has gpus ) } ( as avail _ features ), gres ( gpus ), and state. it essentially wraps the sinfo command with some pre - determined output format options and shows each node on its own line, in alphabetical order. to only view nodes in a specific partition, append - p < partition name > to the command alias. examples $ show _ nodes nodelist cpus memory avail _ features gres state brigid16 48 512897 rhel8, x86 _ 64, zen, epyc - 7443 ( null ) idle brigid17 48 512897 rhel8, x86 _ 64, zen, epyc - 7443 ( null ) idle.................. vulcan45 32 513250 rhel8, x86 _ 64, zen, epyc - 7313, ampere gpu : rtxa6000 : 8 idle ( specific partition ) $ show _ nodes - p tron nodelist cpus memory avail _ features gres state tron00 32 255233 rhel8, x86 _ 64, zen, epyc - 7302, ampere gpu : rtxa6000 : 8 idle tron01 32 255233 rhel8, x86 _ 64, zen, epyc - 7302, ampere gpu : rtxa6000 : 8 idle.................. tron69 32 383030 rhel8, x86 _ 64, xeon, 4216, turing gpu : rtx2080ti : 8 idle show _ available _ nodes the show _ available _ nodes command alias takes zero or more arguments that correspond to slurm constructs, resources, or features that you are looking to request a job with and tells you what nodes could theoretically [ 0, 1 ] run a job with these arguments immediately. it assumes your job is a single - node job. these arguments are : - - partition : only include nodes in the specified partition ( s ). - - account : only include nodes from partitions that can use the specified account ( s ). - - qos : only include nodes from partitions that can use the specified qos ( es ). - - cpus : only include nodes with at least this many cpus free. - - mem : only include nodes with at least this much memory free. the default unit is mb if unspecified, but any of { k, m, g, t } can be suffixed to the number provided ( will then be interpreted as kb, mb, gb, or tb, respectively ). gres - related arguments : - - gres, - - and - gres : only include nodes whose list of gres contains all of the specified gres type / quantity pairings. - - or - gres : only include nodes whose list of gres contains any of the specified gres type / quantity pairings. functionally identical to - - and - gres if only one gres type / quantity pairing is specified. gpu - related arguments : - - gpus, - - and - gpus : only include nodes whose list of gpus ( a subset of gres ) contains all of the specified gpu type / quantity pairings. - - or - gpus : only include nodes whose list of gpus ( a subset of gres ) contains any of the specified gpu type / quantity pairings. functionally identical to - - and - gpus if only one gpu type / quantity pairing is specified. feature - related arguments : - - feature, - - and - feature : only include nodes whose list of features contains all of the specified feature ( s ). - - or - feature : only include nodes whose list of features contains any of the specified feature ( s ). functionally identical to - - and - feature if only one feature is specified. these arguments are also viewable by running show _ available _ nodes - h. if your passed argument set does not contain any resource - based arguments ( cpus / ram / gres or gpus ), a node is defined as available if it has at least 1 cpu and 1mb of ram available. if there are no nodes available that meet your passed argument set, you will receive the message there are no nodes that have currently free resources that meet this argument set. footnotes [ 0 ] - as of now, this command alias does not factor in resources occupied by jobs that could be preempted ( based on the partition ( s ) passed to it, if present ). this is soon to come. [ 1 ] - this command alias also does not factor in jobs with higher priority values requesting more resources, in the same partition ( s ), blocking execution of a job submitted with the arguments checked by the command alias. this is due to the complexity of calculating a job ' s priority value before it is actually submitted. examples show all available nodes : $ show _ available _ nodes brigid17 cpus = 16, mem = 414593m brigid18 cpus = 8, mem = 24875m... show nodes available in the tron partition : $ show _ available _ nodes - - partition tron tron00 cpus = 14, mem = 50433m, gres = gpu : rtxa6000 : 1 tron01 cpus = 10, mem = 17665m, gres = gpu : rtxa6000 : 2... show nodes with one or more rtx a5000 or rtx a6000 gpus available to the vulcan account : $ show _ available _ nodes - - account vulcan - - or - gpus rtxa5000 : 1, rtxa6000 : 1 vulcan32 cpus = 16, mem = 193778m, gres = gpu : rtxa6000 : 4 vulcan33 cpus = 15, mem = 181499m, gres = gpu : rtxa5000 : 3... show nodes with 4 or more cpus, 48g or more memory, and one or more rtx a6000 gpus available in the scavenger partition : $ show _ available _ nodes - - partition = scavenger - - cpus = 4 - - mem = 48g - - or - gpus = rtxa6000 : 1 cbcb27 cpus = 59, mem = 218303m, gres = gpu : rtxa6000 : 6 clip06 cpus = 20, mem = 93448m, gres = gpu : rtxa6000 : 1... show nodes with turing or ampere architecture gpus available in the scavenger partition : $ show _ available _ nodes - - partition = scavenger - - or - feature = ampere, turing cbcb25 cpus = 24, mem = 255278m, gres = gpu : rtx2080ti : 1, gpu : gtx1080ti : 1 cbcb26\"),\n",
       " Document(id='b69df911-c9b6-4731-bee1-9ed82e741a6a', page_content='name maxwall maxtres maxjobspu maxtrespu - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - cml - cpu 7 - 00 : 00 : 00 8 cml - default 7 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g 2 cml - high 1 - 12 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g 2 cml - high _ long 14 - 00 : 00 : 00 cpu = 32, gres / gpu = 8 8 gres / gpu = 8 cml - medium 3 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g 2 cml - scavenger 3 - 00 : 00 : 00 gres / gpu = 24 cml - very _ high 1 - 12 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g 8 gres / gpu = 12 default 3 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g high 1 - 00 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g highmem 21 - 00 : 00 : 00 cpu = 32, mem = 2t huge - long 10 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g medium 2 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g scavenger 3 - 00 : 00 : 00 cpu = 576, gres / gpu = 72, mem = 2304g vulcan - cpu 2 - 00 : 00 : 00 cpu = 1024, mem = 4t 4 vulcan - default 7 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g 2 vulcan - exempt 7 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g 2 vulcan - high 1 - 12 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g 2 vulcan - janus 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 10, mem = 256g vulcan - medium 3 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g 2 vulcan - sailon 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g gres / gpu = 48 vulcan - scavenger 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g to find out what accounts and partitions you have access to, first use the show _ assoc command to show your account / job qos combinations. then, use the scontrol show partition command and note the allowaccounts entry for each listed partition. you are able to submit to any partition that allows an account that you have. if you need to use an account other than the default account nexus, you will need to specify it via the - - account submission argument. partition qos partition qos are used to limit resources used by all jobs running in a partition, either per user ( maxtrespu ) or per group ( grptres ). to view partition qos, use the show _ partition _ qos command. $ show _ partition _ qos name maxsubmitpu maxtrespu grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - scavenger 500 cpu = 576, gres / gpu = 72, mem = 2304g tron 500 cpu = 32, gres / gpu = 4, mem = 256g if you want to see all partition qos, including those that you do not have access to, you can use the show _ partition _ qos - - all command. $ show _ partition _ qos - - all name maxsubmitpu maxtrespu grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - name maxsubmitpu maxtrespu grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - cbcb 500 cpu = 1260, mem = 50016g cbcb - heng 500 class 500 cpu = 32, gres / gpu = 4, mem = 256g clip 500 cpu = 564, mem = 5590g cml 500 cpu = 1128, mem = 11t cml - furongh 500 cml - scavenger 500 gres / gpu = 24 cml - zhou 500 gamma 500 cpu = 648, mem = 5454g mbrc 500 cpu = 240, mem = 2345g mc2 500 cpu = 312, mem = 3092g oasis 500 quics 500 cpu = 328, mem = 3484g scavenger 500 cpu = 576, gres / gpu = 72, mem = 2304g tron 500 cpu = 32, gres / gpu = 4, mem = 256g vulcan 500 cpu = 1392, mem = 12833g vulcan - ampere 500 vulcan - cpu 500 vulcan - ramani 500 vulcan - scavenger 500 note : these qos cannot be used directly when submitting jobs, with the exception of the scavenger qos ( i. e., they are not in the allowqos field for their respective partition ). partition qos limits apply to all jobs running on a given partition, regardless of what job qos is used. for example, in the default non - preemption partition ( tron ), you are restricted to 32 total cpu cores, 4 total gpus, and 256gb total ram at once across all jobs you have running in the partition. lab / group - specific partitions may also have their own user limits, and / or may also have group limits on the total number of resources consumed simultaneously by all users that are using their partition, codified by the line in the output above that matches their lab / group name. note that the values listed above in the two \" tres \" columns are not fixed and may fluctuate per - partition as more resources are added to or removed from each partition. all partitions also only allow a maximum of 500 submitted ( running ( r ) or pending ( pd ) ) jobs per user in the partition simultaneously. this is to prevent excess pending jobs causing backfill issues with the slurm scheduler. if you need to submit more than 500 jobs in batch at once, you can develop and run an \" outer submission script \" that repeatedly attempts to run an \" inner submission script \" ( your original submission script ) to submit jobs in the batch periodically, until all job submissions are successful. the outer submission script should use looping logic to check if you are at the max job limit and should then retry submission after waiting for some time interval. an example outer submission script is as follows. in this example, example _ inner. sh is your inner submission script and is not an array job, and you want to run 1000 jobs. if your inner submission script is an array job, adjust the number of jobs accordingly. array jobs must be of size 500 or less. #! / bin / bash numjobs = 1000 i = 0 while [ $ i - lt $ numjobs ] do while [ [ \" $ ( sbatch example _ inner. sh 2 > & 1 ) \" = ~ \" qosmaxsubmitjobperuserlimit \" ] ] do echo \" currently at maximum job submissions allowed. \" echo \" waiting for 5 minutes before trying to submit more jobs. \" sleep 300 done i = $ ( ( $ i + 1 ) ) echo \" submitted job $ i of $ numjobs \" done it is suggested that you run the outer submission script in a tmux session to keep the terminal window executing it from being interrupted. storage all network storage available in nexus is currently nfs based, and comes in a few different flavors. compute nodes also have local storage that can be used. home directories you have 30gb of home directory storage available at / nfshomes / < username >. it has both snapshots and backups'),\n",
       " Document(id='e4918136-449b-4767-9270-e3d0e898a02d', page_content='| n / a | + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + | processes : gpu memory | | gpu pid type process name usage | | = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = | | no running processes found | + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + as with all other flags, the - - gres flag may also be passed to sbatch and salloc rather than directly to srun. mpi example to run mpi jobs, you will need to include the - - mpi = pmix flag in your submission arguments. #! / usr / bin / bash # sbatch - - job - name = mpi _ test # job name # sbatch - - nodes = 4 # number of nodes # sbatch - - ntasks = 8 # number of mpi ranks # sbatch - - ntasks - per - node = 2 # number of mpi ranks per node # sbatch - - ntasks - per - socket = 1 # number of tasks per processor socket on the node # sbatch - - time = 00 : 30 : 00 # time limit hrs : min : sec srun - - mpi = pmix / nfshomes / username / testing / mpi / a. out retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / jobsubmission & oldid = 11991 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 13 august 2024, at 16 : 21. privacy policy about umiacs disclaimers'),\n",
       " Document(id='f9fb823e-12ff-470b-b3d9-ee6c7536d701', page_content=': ss # sbatch - - qos = default # set qos, this will determine what resources can be requested # sbatch - - nodes = 2 # number of nodes to allocate for your job # sbatch - - ntasks = 4 # request 4 cpu cores be reserved for your node total # sbatch - - ntasks - per - node = 2 # request 2 cpu cores be reserved per node # sbatch - - mem = 1g # memory required by job ; if unit is not specified mb will be assumed. for multi - node jobs, this argument allocates this much memory * per node * srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # use srun to invoke commands within your job ; using an \\' & \\' srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # will background the process allowing them to run concurrently wait # wait for any background processes to complete # once the end of the batch script is reached your job allocation will be revoked another useful thing to know is that you can pass additional arguments into your sbatch scripts on the command line and reference them as $ { 1 } for the first argument and so on. more examples slurm / arrayjobs scancel the scancel command can be used to cancel job allocations or job steps that are no longer needed. it can be passed individual job ids or an option to delete all of your jobs or jobs that meet certain criteria. scancel 255 cancel job 255 scancel 255. 3 cancel job step 3 of job 255 scancel - - user username - - partition = tron cancel all jobs for username in the tron partition identifying resources and features the sinfo command can show you additional features of nodes in the cluster but you need to ask it to show some non - default options using a command like sinfo - o \" % 40n % 8c % 8m % 35f % 35g \". $ sinfo - o \" % 40n % 8c % 8m % 35f % 35g \" nodelist cpus memory avail _ features gres legacy00 48 125940 rhel8, zen, epyc - 7402 ( null ) legacy [ 01 - 11, 13 - 19, 22 - 28, 30 ] 12 + 61804 + rhel8, xeon, e5 - 2620 ( null ) cbcb [ 23 - 24 ], twist [ 02 - 05 ] 24 255150 rhel8, xeon, e5 - 2650 ( null ) cbcb26 128 513243 rhel8, zen, epyc - 7763, ampere gpu : rtxa5000 : 8 cbcb27 64 255167 rhel8, zen, epyc - 7513, ampere gpu : rtxa6000 : 8 cbcb [ 00 - 21 ] 32 2061175 rhel8, zen, epyc - 7313 ( null ) cbcb22, cmlcpu [ 00, 06 - 07 ], legacy20 24 + 384270 + rhel8, xeon, e5 - 2680 ( null ) cbcb25 24 255278 rhel8, xeon, e5 - 2650, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 1 legacy21 8 61746 rhel8, xeon, e5 - 2623 ( null ) tron [ 06 - 09, 12 - 15, 21 ] 16 126214 + rhel8, zen, epyc - 7302p, ampere gpu : rtxa4000 : 4 tron [ 10 - 11, 16 - 20, 34 ] 16 126217 rhel8, zen, epyc - 7313p, ampere gpu : rtxa4000 : 4 tron [ 22 - 33, 35 - 45 ] 16 126214 + rhel8, zen, epyc - 7302, ampere gpu : rtxa4000 : 4 clip11 16 126217 rhel8, zen, epyc - 7313, ampere gpu : rtxa4000 : 4 clip00 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 3 clip02 20 126255 rhel8, xeon, e5 - 2630, pascal gpu : gtx1080ti : 3 clip03 20 126243 rhel8, xeon, e5 - 2630, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 2 clip04 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtx3090 : 4 clip [ 05 - 06 ] 24 126216 rhel8, zen, epyc - 7352, ampere gpu : rtxa6000 : 2 clip07 8 255263 rhel8, xeon, e5 - 2623, pascal gpu : gtx1080ti : 3 clip09 32 383043 rhel8, xeon, 6130, pascal, turing gpu : rtx2080ti : 5, gpu : gtx1080ti : 3 clip13, cml30, vulcan [ 29 - 32 ] 32 255218 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 8 clip08, vulcan [ 08 - 22, 25 ] 32 255258 + rhel8, xeon, e5 - 2683, pascal gpu : gtx1080ti : 8 clip12, gammagpu [ 10 - 17 ] 16 126203 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 4 clip01 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 1, gpu : titanxp : 2 clip10 44 1029404 rhel8, xeon, e5 - 2699 ( null ) cml [ 00, 02 - 11, 13 - 14 ], tron [ 62 - 63, 65 - 66, 68 - 32 351530 + rhel8, xeon, 4216, turing gpu : rtx2080ti : 8 cml01 32 383030 rhel8, xeon, 4216, turing gpu : rtx2080ti : 6 cml12 32 383038 rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtxa4000 : 1 cml [ 15 - 16 ] 32 383038 rhel8, xeon, 4216, turing gpu : rtx2080ti : 7 cml [ 17 - 28 ], gammagpu05 32 255225 + rhel8, zen, epyc - 7282, ampere gpu : rtxa4000 : 8 cml31 32 384094 rhel8, zen, epyc - 9124, ampere gpu : a100 : 1 cml32 64 512999 rhel8, zen, epyc - 7543, ampere gpu : a100 : 4 cmlcpu [ 01 - 04 ] 20 384271 rhel8, xeon, e5 - 2660 ( null ) gammagpu00 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa5000 : 8 mbrc [ 00 - 01 ] 20 189498 rhel8, xeon, 4114, turing gpu : rtx2080ti : 8 twist [ 00 - 01 ] 8 61727 rhel8, xeon, e5 - 1660 ( null ) legacygpu08 20 513327 rhel8, xeon, e5 - 2640, maxwell gpu : m40 : 2 brigid [ 16 - 17 ] 48 512897 rhel8, zen, epyc - 7443 ( null ) brigid [ 18 - 19 ] 20 61739 rhel8, xeon, e5 - 2640 ( null ) legacygpu06 20 255249 rhel8, xeon, e5 - 2699, maxwell gpu : gtxtitanx : 4 tron [ 00 - 05 ] 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa6000 : 8 tron [ 46 - 61 ] 48 255232 rhel8, zen, epyc - 7352, ampere gpu : rtxa5000 : 8 tron [ 64, 67 ] 32 383028 + rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtx3070 : 1 vulcan00 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7, gpu : p100 : 1 vulcan [ 01 - 04, 06 - 07 ] 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 8 vulcan05 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7 janus [ 02 - 04 ] 40 3830')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we also see that it seems to focus a lot on the link title. (python notebook is only in the body of SLURM, the first result)\n",
    "# adding nexus to the end of our query in the previous cell just gave us a bunch of nexus pages\n",
    "vector_store.similarity_search(query=\"python notebook\", k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_store.store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with chunking modified to 128 tokens (and overlap adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['f00b972f-0564-4d7a-95cf-72f3af65bfa3',\n",
       " '06b5772d-b5ec-45ca-b95b-939c2019a9f9',\n",
       " 'a76d677f-c77a-41b7-88bc-c98df6f879be',\n",
       " 'aaa57ca0-e8c5-4d10-86ae-94a61d0026b5',\n",
       " 'd3fe756b-3981-494b-9d14-f40bc91bc6c3',\n",
       " 'a51571a4-7af9-46e8-a7af-5f1b328f9467',\n",
       " 'cd0c4011-409c-4656-99c6-183ceaa36d7c',\n",
       " 'd4d2aa37-ec68-432e-8088-898f6d9c042e',\n",
       " '2afd21fa-5cdb-4c4a-a306-93165215e41b',\n",
       " '36cfa6d1-cfde-4b2b-8bfc-87b4db753179',\n",
       " '9c0b45dc-25f7-49f8-9057-f82a042effe6',\n",
       " '302fc279-bff2-4f94-9997-83440f039e99',\n",
       " '55e327b8-695d-4ff4-b9dd-ee755c4accce',\n",
       " 'bb99a095-1103-4c0c-aec7-91407bf9ca7d',\n",
       " '430b9edf-145e-4490-ae5a-275f2216badd',\n",
       " 'e64cf6e5-5f1d-4f10-af79-956ce3486932',\n",
       " '484de1f3-223c-4648-bdbe-58809848fb7e',\n",
       " '0455a596-8721-476f-bed2-1c731e16cd05',\n",
       " '5a83b9d6-5747-408b-b871-a4a6bc05caad',\n",
       " 'fcbfbdb9-bb19-4bdd-a73a-8b6c51231922',\n",
       " 'a4867f5b-e552-4191-b9b7-7c5744ca8d47',\n",
       " '8f412f51-db3e-4375-8cfc-dd8f95c2f7ca',\n",
       " '0ff11fcf-8156-4814-b111-e318c2f2f566',\n",
       " 'f42dd7c5-38e0-419f-96f9-5b1539c96fe1',\n",
       " '34b411a0-aa63-4ad5-8d92-f4c3cf3eee71',\n",
       " 'd0464032-2194-4e8f-8fa6-f52fb599b0db',\n",
       " '9b4235f5-346a-4d9e-989a-f4dc0e9c7bdd',\n",
       " '79c58f52-d93a-43a3-ae13-2ced5971956b',\n",
       " '20ea3c9d-a1ff-4269-9fcc-b0cd88a996c2',\n",
       " '4361c902-040e-4600-b776-a741fa8f0658',\n",
       " 'eed01d04-21a8-4ac1-87fa-a47b31e11485',\n",
       " 'ac658d1d-9d5a-4094-b033-112a2823e3fd',\n",
       " 'b3e7a19c-cbed-4b84-82ae-dc0203d3f185',\n",
       " 'c31c3a5a-a4ed-4898-818f-e050a721c4a4',\n",
       " 'fd047dfb-7f2d-491a-9bdd-0cddc4f71015',\n",
       " 'dd3f10ba-d3fc-41c1-8362-92aa925cf9eb',\n",
       " 'ab1d3c6b-b583-4d8e-b4db-c34c725e7ece',\n",
       " 'd085f43b-ecf7-4e39-8b5b-1f3d784b8143',\n",
       " '2aa6d168-68e7-43cf-a81a-d74c7fc77e73',\n",
       " '9aaaa26b-ce9a-48bf-a9ab-2ed5503025bc',\n",
       " 'b2eb1511-12a0-40bf-924f-aa056cdf20df',\n",
       " '6ee80845-841f-4dc6-8a96-45f5a8f34de3',\n",
       " '0db4c7b8-9ad3-44df-affa-7c172efb4a60',\n",
       " '0c072798-14f1-4e6a-bdb5-f4ba1761a8c2',\n",
       " 'af39c198-ca85-4207-acf2-1586af64ecb4',\n",
       " 'b021885f-06cd-47b9-8c68-369a779047a0',\n",
       " 'fc3b05e0-dd02-4058-a3f4-598d562c8233',\n",
       " '4b7f6b1a-85e0-4f27-8f69-cf0abf4e1e13',\n",
       " 'c576ea51-2903-4809-9082-c1cb49d748b2',\n",
       " 'f89753d5-074d-451a-8939-645902f41da7',\n",
       " 'af851e76-b312-4312-a683-fc2cf9612a4c',\n",
       " '6eba0884-c57f-476e-b2b8-67ac8cdfcc35',\n",
       " '188d6069-8b2c-45c1-866a-b207550bb503',\n",
       " 'ed4d5d45-30e6-4f1f-b12b-578d313e04fe',\n",
       " 'fafc834f-e05f-4341-9096-e71d24136e72',\n",
       " 'bd1f5cae-9d5f-4a7b-9668-52ed97f5a517',\n",
       " 'd0567765-2d88-4baa-ac58-899af9c54ddd',\n",
       " '88867da5-b298-48ba-ad7f-dd334b926704',\n",
       " 'bd8ba38f-14df-4eaa-801a-15309e5ec041',\n",
       " 'b6dd4019-b036-4527-9d87-ceb48168b1e4',\n",
       " 'a6509f09-3bf4-41da-b465-735dff2d9f8b',\n",
       " 'c114af04-5121-43b7-acc4-cd92c27cf522',\n",
       " '0db39a83-3fc7-40bc-b89d-d61a92eaed7e',\n",
       " '427176c7-1f21-4789-8b17-9632124f8112',\n",
       " 'd53fe193-bd9b-46fb-bdb7-13c283756827',\n",
       " '31369cfb-b1dd-4916-88c2-f63427367ad4',\n",
       " '230703ee-4c4c-4738-b952-44e7056357b3',\n",
       " 'd15d69f4-fa4c-413c-8113-e31641c70e3c',\n",
       " '6ebdfd47-d9af-4a22-8be4-dba60bb4f619',\n",
       " '92a902d5-fcb7-461f-905e-1a1688ab92e9',\n",
       " '666f216a-976c-43a9-bb33-9b2660d63708',\n",
       " '01b4c898-6f24-4ebd-84a8-9865cdcc99ff',\n",
       " '662747f7-1656-4dc8-9e51-b824e0c8766b',\n",
       " 'e69fcce9-3deb-411a-9b19-a0ef281c2bdf',\n",
       " 'e4c895c3-bc7d-4084-844d-0cf0664e8d5b',\n",
       " '1192f8b8-0a39-433a-ac56-ca862ccc8baf',\n",
       " '52f1f49c-6040-46a9-aa9a-9f2481b0cbfb',\n",
       " 'acf47805-7c5e-44dc-a245-d6ee7d1a4a6a',\n",
       " '5bc47a56-d886-42e8-a23a-3f47632998fc',\n",
       " 'c937f5f4-4920-4b04-bbe4-1240e831d2f9',\n",
       " '3c0a9824-8f61-4a94-be66-6de57857e804',\n",
       " '1707fe57-abbb-41d0-815e-4a2b6d02d3f0',\n",
       " 'a9b3bef9-e83e-4fd8-9a34-ae044bcff8a6',\n",
       " '54fa5a08-000b-49d7-be03-882a278f7117',\n",
       " 'a1772c69-f61d-4ae9-9341-63190e8bc88c',\n",
       " 'd61a31e2-d752-490e-ad56-659f54db5b43',\n",
       " 'e8f2b2b5-1847-4a33-b98f-89e7236d9860',\n",
       " '04340da8-fab8-4652-be02-8a6cccbfdefe',\n",
       " '2fc5cdd2-8b5a-448e-884e-cf6beb10074e',\n",
       " '97678931-7fcb-46e2-9380-5cda2e6b1501',\n",
       " '1a523424-f4eb-4348-9820-2e83e141b5a2',\n",
       " 'aa4260fc-b790-4ed5-9ea1-1b98a8972074',\n",
       " '8dc03832-91cf-414d-b00a-dadcabea963f',\n",
       " '3a19429d-ebd8-4e1a-acac-68acffcfffdd',\n",
       " '03709110-f69d-4474-bc34-1284017c257b',\n",
       " '05fafa7b-a503-4e61-b6c3-1408df2600f8',\n",
       " '5af395ed-6dd6-4c5a-80e1-9eaf9a703c96',\n",
       " '947be522-af55-4b3f-bac6-7c175f352aaa',\n",
       " '953281f4-62a9-418c-9571-f234fb076e25',\n",
       " '53d4ce02-2d37-411c-8718-d34cd23abc17',\n",
       " 'ce3f3a7b-9227-4ba9-a25b-1c08383770fc',\n",
       " 'e1bfd458-1738-40af-a41f-eed2821e6ca1',\n",
       " '10090da8-0aa7-4492-abb3-4fa401f00a0f',\n",
       " 'faefbe14-f85f-4538-b1bf-a6c2126999c0',\n",
       " '272ec923-42e6-42f1-9c95-6f468b0d0f3d',\n",
       " 'c5ae7989-b35f-4583-8017-d72ecd679515',\n",
       " '2c436eb1-4468-485d-bd98-c4f3b08e236c',\n",
       " '4353bf4a-d3ef-4af2-8e9b-b72d13ecd129',\n",
       " '4f595772-e8dd-4f7a-90e4-2dc91bc6c05b',\n",
       " 'f3138f5b-b63f-410c-b933-61196ff856f9',\n",
       " '9da6aa62-a381-4633-9bbb-1a03dab97225',\n",
       " '11550655-060a-4e66-9cf8-e5727b6505a6',\n",
       " '77d98365-46f2-4ce9-9be1-624687427cfc',\n",
       " 'be56ecd8-2947-4a12-a22e-75286e970926',\n",
       " '673930d1-7e41-47df-b43a-5c91dc528f40',\n",
       " '4e412421-c473-451e-ab6f-f9f9201adb01',\n",
       " '223d6005-3891-4c91-aecf-905f778ed25d',\n",
       " 'a5d75bbb-d445-46c9-bcab-86fc01edf66c',\n",
       " 'fc0a0fca-bbb9-4391-8445-5ab7216aca17',\n",
       " '804588d8-6fda-452a-809d-59f470de05a0',\n",
       " '62d8139b-1ef3-43eb-821e-4fc32b678554',\n",
       " '508dec9a-d732-4b0b-a0a1-fa81d58c3207',\n",
       " '8200e262-efb4-4925-9531-ffb284dd0dc5',\n",
       " '28c42238-92d5-4263-a7ac-c8a18fcf33ce',\n",
       " '4135f0fe-861e-4409-a605-d09a6173cb98',\n",
       " '7b84f8fc-c418-4c40-a4fb-e34a94809bcb',\n",
       " '30ecfebd-1c21-4481-bc07-10361f2f56ed',\n",
       " 'a3d65fed-35e5-47d6-b99e-de81bde9a0c8',\n",
       " '0c738013-ab78-4bb1-b4d9-ef055daf1c50',\n",
       " '971c112c-99d4-4648-8100-61692b2faf79',\n",
       " '617a0083-8f56-480b-ae15-e80b2f0ff98d',\n",
       " '14f53c84-3d0a-4be4-8f07-baed755cd0f3',\n",
       " 'ce51be40-4fe5-4c35-98bc-5c56e8e57d81',\n",
       " '5c9a801b-b8f7-4b8f-8ef3-5d64b7649165',\n",
       " '5ae0c75d-3c3e-4422-96da-0c9385fa7919',\n",
       " 'f8008f7d-adbe-4e6d-ac77-34e650a53abc',\n",
       " '409f8aee-287e-450b-a253-b0405a3e18ca',\n",
       " '4b16aed5-67d1-477d-b1e8-5e45f77a6273',\n",
       " 'e3c64f58-2565-4fa6-af55-6b9423614f61',\n",
       " '31b869ad-b10a-4047-949c-a4b26f651d24',\n",
       " '96ff5b06-0201-4134-8d83-2f93579fc26c',\n",
       " '5f427dc3-cb40-49de-b424-a9fbe8b07507',\n",
       " 'ca441d17-7368-440b-9409-383e91665b3e',\n",
       " 'efd7d3f7-af92-4afd-8767-c5d8470817b0',\n",
       " 'd890db16-803e-45e7-a7ea-d244c3db5398',\n",
       " '0417fbf3-980c-4e1b-894d-2095e20f3e8c',\n",
       " '31f6c202-4e5f-4c46-b7d8-e255dd3e1997',\n",
       " '7248dc3c-4750-4682-aead-86c3757c0abb',\n",
       " 'fcef0be1-5045-4fa2-b77a-ca844b8c2fc0',\n",
       " 'e7b70289-7874-4ee1-a0b4-542d4d1ac9d8',\n",
       " '635e182d-7262-4d07-bf40-5ce0813c6695',\n",
       " '45c2e4d9-9762-4ceb-8fca-70a549ce8c39',\n",
       " 'c578d91a-338e-4c42-8861-51c52d4daf97',\n",
       " 'b2e4187c-34df-4efa-b280-eeac77eb8875',\n",
       " '58913e41-8724-4f47-be19-04f5f5ba8fc9',\n",
       " 'ccf4038c-96c9-419f-b9fe-75ea2ed5fcf5',\n",
       " 'c6e3e0e2-e5d0-4a9d-82a4-1b495b01e557',\n",
       " 'eba999ed-0391-4350-868b-7245f26506bd',\n",
       " '5c906966-fa04-446b-9060-20d0b7af03b6',\n",
       " 'ef609430-20f8-4f42-9aee-4a338aa431a5',\n",
       " '1a579d57-c182-40c0-86f1-3fcd7bdc58b0',\n",
       " '22ddbee8-8b96-4653-b578-b946e9c4bef9',\n",
       " 'dce9a2ca-a6b0-4432-be61-be088dc95c91',\n",
       " '0d67f9c8-1c07-461c-ac84-1352dddd2ee0',\n",
       " 'eedb9fae-f749-4fcc-9433-bf3917a85b7e',\n",
       " '66218f45-e2fb-4b3b-8b8a-e8a6dc2f9367',\n",
       " '376f7fce-1924-4cd9-aeca-5a9d5becce3f',\n",
       " 'a1e7dd80-afa4-4bf6-9ff4-9e35112c0910',\n",
       " 'ab985d4f-7ed7-4ee3-96d3-652872d73eb5',\n",
       " 'b4e37754-6d9e-412b-a0ae-fe627e20e87b',\n",
       " '7fd56927-6acd-4da9-88c4-3077d648f78a',\n",
       " '8aa93ede-c329-496d-a530-d73f9e479338',\n",
       " '44f5463d-05d3-4706-9160-93178109b1ea',\n",
       " 'e18591a3-fcab-43ee-95e5-ad85c38e5526',\n",
       " '6ccfca63-8596-49ca-b267-fd10847f64e7',\n",
       " '8ae29133-cc9d-4088-a9ad-a0425850b161',\n",
       " '7eaace95-d250-436d-b435-92a032828c04',\n",
       " 'c6790414-d22b-471d-ab31-74909098240c',\n",
       " 'a4285e16-e598-4ee6-a5c8-aa5f1bd70317',\n",
       " '8301f347-5d61-496c-8525-a84832107376',\n",
       " '1de6b222-8314-4bc1-bc60-4288ae4d928a',\n",
       " '1628275f-6ce9-4b8b-99a5-97a50c8d60fa',\n",
       " '3d5b5b16-b97a-4f4f-9d8c-83fe3b24dbf8',\n",
       " '1ebfdf26-63f1-4347-ab70-0f3235ee2179',\n",
       " 'b3d0a11d-251d-4c06-9cac-5224c3b5f687',\n",
       " 'adb24286-38ab-4298-ae76-5c002a7ee0c0',\n",
       " '5d972edf-b65e-44cd-a7b5-bf9f7595e15c',\n",
       " 'd53d8a13-a084-4e6e-a221-4fa4c26282a4',\n",
       " '88185266-25d7-4870-97d4-ce9ecd5fb0c5',\n",
       " '5668e364-fdf1-424d-9e3f-a0760352998b',\n",
       " '0c65ba37-06de-438e-b946-ed46ace59e02',\n",
       " '65c3a149-3b89-4b21-950c-a5e039f710b8',\n",
       " 'bfaeb607-f7eb-42d9-a0df-792b073b56e4',\n",
       " '274524e4-baea-44c3-8468-c83365f68926',\n",
       " '86cfc2d0-6334-45a5-b500-d5f8da712457',\n",
       " '66e93f0a-06d2-4018-b550-bb8643ac51fe',\n",
       " 'b89386f5-ec80-4674-a965-23c3c1f031f2',\n",
       " '67da6777-ea59-456e-888e-ea7484deb09e',\n",
       " 'c4374fd4-5f46-4fdb-bc8c-191883a999a7',\n",
       " 'aa0f7a24-f8e0-4e49-9046-9c79ce08dd09',\n",
       " 'e7a1a1ca-6008-44a4-989e-d152852a28bb',\n",
       " 'a45ba737-31e3-4dd0-836d-76db058e17d6',\n",
       " 'e79938c3-b6e9-4edd-8b29-da6d42b2e6ad',\n",
       " 'bdcd8ca9-659f-4caa-bd0d-410e3fdcb09b',\n",
       " '357331ca-d14f-462e-aa4b-0084046d7525',\n",
       " '97c8b990-4f3a-46ea-93dd-69f23c2dd06f',\n",
       " '8a472c7e-24f7-4b91-9bb8-7f920bbbe47c',\n",
       " '938972db-f859-456e-b335-1d5e3273c958',\n",
       " '6723c4b0-497e-438a-bec8-e80c1d1ed788',\n",
       " 'd67445e7-c0e8-4111-ac04-463b03811d89',\n",
       " '0e2cefc5-8428-4ee2-b68a-326729ca5d28',\n",
       " '8a17d2c5-b103-45fa-ab39-6299536c87ae',\n",
       " '014c9606-4cbe-47cf-a57b-6c45476c72e3',\n",
       " 'b1562063-f476-49a0-8618-cac10e558c6f',\n",
       " '51907a91-530d-4ddd-a638-770e7a49341a',\n",
       " 'e95ae4ef-4c65-493d-a457-f8c07e611f8f',\n",
       " '6291425a-2107-40fb-80f4-9dec40e624cf',\n",
       " 'bd243311-38bd-419e-99d0-52d28d755be1',\n",
       " '9c58bd09-cc9a-4a32-aa04-6a548bdbbdcc',\n",
       " 'e6f3ad36-0386-4147-844f-930b7d3a4239',\n",
       " '9bd33f8d-a6ba-4362-9a44-8d7d7abce2ed',\n",
       " '95d59176-c0f8-4cdf-89a3-23e6721c72c5',\n",
       " 'a4024cea-574a-4553-a062-137fc04a34aa',\n",
       " 'a896aafa-9481-4d8d-ab6d-12a23fe9d817',\n",
       " '74214629-4f57-4720-9649-3ec8fa96e735',\n",
       " 'bb90dd94-64b7-430a-b064-88112f6c6128',\n",
       " '2ee7e235-fcab-4fb2-802f-e7fb56d84f00',\n",
       " 'f927d7d4-1a66-49e1-b4fa-e8e5034b4a53',\n",
       " 'cd1bdf4e-a0d2-4963-be6c-bd3e40312a82',\n",
       " 'c09f364f-5ea5-4348-aa74-6840d6edc9d3',\n",
       " '06822d2d-2a13-429a-a776-0d3c859d929e',\n",
       " 'adef604c-9126-4a53-b10d-fbb055cf2b33',\n",
       " '625b8c1f-17a5-4b06-9fce-278260158746',\n",
       " '1d639759-0cb6-4432-9fd2-9f0658139f96',\n",
       " '2f16ff3a-24ed-464d-b2ce-64c940daf9c4',\n",
       " '2e2c05b3-dee0-404e-a5f0-a4d98f028d61',\n",
       " 'd969304a-e2b5-45fc-82f1-0fdd378d133c',\n",
       " 'ad116291-0105-45f3-b716-de7ace7d3a89',\n",
       " '26adeff8-d767-454c-9fb1-8e8c2e3291ff',\n",
       " '15abc646-fe40-42ab-a680-7c4d0a79e4b9',\n",
       " '08f52353-b332-45da-bfff-f9a52a4c1006',\n",
       " 'cdf457ed-d9d1-404a-a6b5-1fa3f610b7ca',\n",
       " '9b4a9ede-f8ff-45c0-9a94-d91d8b1895b7',\n",
       " '9dd972ee-1a4d-4103-8df0-c9d08c56db1c',\n",
       " '468bd9d4-9782-4d51-8266-fc3ab23e8470',\n",
       " 'e0c268e4-5582-41d2-b3c9-b02a6c2c8c58',\n",
       " 'eb6b406d-7430-4a64-8beb-d43863ad3ebd',\n",
       " '21989d0a-7c8a-40d0-8cd8-fd2131409627',\n",
       " 'd4734767-2ff5-4adf-94f3-12f2e16bfe4b',\n",
       " '1eb83063-939d-4c10-8c75-99acca6e021e',\n",
       " '8eb269d4-fbcd-4aca-aef5-79aec9474045',\n",
       " '28b0fd81-3591-4a13-be40-91bb6d86894b',\n",
       " '3414fbf8-11fc-4101-8dfe-340a716e2876',\n",
       " '9364a6c2-e2e6-46fb-9391-9f71aeea74a7',\n",
       " '521ae04c-75ac-44cd-95aa-e62ec7ee376f',\n",
       " '6543e535-3f9a-4b0b-b6c3-e3409dd8f390',\n",
       " '1a90a7c3-5f17-424b-87c9-430d7baf1f7b',\n",
       " 'e7bc89ce-ee6d-448a-beb1-5100d12e5411',\n",
       " '7b7173a7-27ad-4e57-a21c-4b0f11a9e2f1',\n",
       " '45d757ed-1a77-479f-9773-ba71b0eac90a',\n",
       " '304b7efc-c95e-4940-b006-a7b2fdd31d7a',\n",
       " '1a9bf477-10b3-4dca-b11f-5244682ee25a',\n",
       " 'f2bc7258-7724-4984-bab1-7894f63801c6',\n",
       " '73f01831-4913-4f41-ad5f-39072c3e3867',\n",
       " '4510d3bb-741c-4bac-a669-381eed11e768',\n",
       " '8ffc3be4-546a-4af2-bc88-e393facd07d8',\n",
       " 'f06a951c-ea01-4a1d-ae77-c45f9fd5e3fc',\n",
       " 'fdefe468-ed78-42bd-86d5-e08695e82ebb',\n",
       " '387c90f5-88ab-409d-b4fd-a16715c3d3bd',\n",
       " '233a753d-3bb0-4dfb-b9d9-6e9db4bae527',\n",
       " '8d412284-024c-4a55-8459-77b0a272fb95',\n",
       " '1819b07f-4528-4c58-a547-efc43be29052',\n",
       " '6144dea3-4ba2-4cdc-86c4-845ae777a134',\n",
       " 'dbcd03ce-5cb8-473d-8801-276ddafa60ad',\n",
       " '18f3cce4-468a-451e-aba8-b911a20f8f9c',\n",
       " 'c4492896-2a5b-49dc-8978-240790696a06',\n",
       " '137f2e27-659d-4748-a14b-b1f536f6ce4d',\n",
       " '1f4d7122-7133-48f5-90de-61b3841333be',\n",
       " '91d71f7f-1031-4e89-ba02-e15fb9e64f80',\n",
       " 'a6b30d42-2ef3-44f2-b56c-65f5f17e598a',\n",
       " '0c9ab9c2-e23c-4f1f-9cc3-801ea842ae80',\n",
       " '07220f37-efad-4fa2-b0aa-966101e5da5b',\n",
       " 'eefb4bf2-fc93-42e6-9890-352eeda04dee',\n",
       " '259f8181-ca8e-4613-87ee-9c121505d323',\n",
       " '4fd2f8d1-a3c2-4014-9bfc-2833eff00e7a',\n",
       " 'e2def92a-fa71-4a0d-936b-a9db9eb1f182',\n",
       " '308ca72d-d066-4d96-a212-1f1a71d16473',\n",
       " '40b99ab8-9024-4da1-9ad6-fc6de40a8dc2',\n",
       " '698e25bb-a3a0-4073-b48f-78530b869963',\n",
       " '351ec738-e53d-4273-a9cb-e45ce37ccbf9',\n",
       " 'd4ec4193-9e45-4cc2-9397-6bfb607a5655',\n",
       " 'b909f3d5-c5ac-4b9b-8808-2db3b131ed22',\n",
       " '5b3dcf91-8dc9-494a-9b15-258f4db3ce0c',\n",
       " 'a2231bac-d4ff-4565-be42-a75108a87d33',\n",
       " '83dd5a0e-6d03-4cc1-9f14-28b6c1794b00',\n",
       " 'b926e090-541c-45bf-a2d9-b1fcc4aaa543',\n",
       " '1a78300a-5289-4d5c-955c-731880cef9af',\n",
       " '778710e4-2d9a-4b64-aa2f-00ccd857ecff',\n",
       " '99d9dc83-c75b-4608-81b8-f3b366a3735c',\n",
       " '4abd39cd-9268-4135-9ae0-d8551a6c397c',\n",
       " '64a23240-712e-492d-afeb-d07105630d9d',\n",
       " '4ef92dd7-89dc-4dd3-b908-c3f7bdb86804',\n",
       " 'ce46e9fd-7b79-49c6-bfdd-1d4c86491c9e',\n",
       " '0b0950be-99c6-4b1f-b6eb-ffdd5311c941',\n",
       " '359dfde0-cbcf-4c96-a8c5-7ccc6280045f',\n",
       " '96e5af28-19e9-43a6-9a9a-13f00227c9dd',\n",
       " '43a3dd9d-7b3a-4096-a404-2a13994b7eb9',\n",
       " '8205f9e9-403b-4a28-aa6c-5239ed735cfc',\n",
       " 'fe1f71b9-a2f4-487a-bf45-11076639884b',\n",
       " '29466f6c-710f-45d0-ac89-21911c9a0606',\n",
       " 'cd3f8906-df81-43b4-bf70-fa3dac647185',\n",
       " 'aefe54e1-42cf-4238-9a7b-833725d01f56',\n",
       " '9794d103-7ff3-4672-96df-1648d15deb53',\n",
       " '59fa7492-d3ad-42a3-a8bc-acac7158259d',\n",
       " 'ecbb5411-edc2-4e94-8dd6-93c990b20103',\n",
       " '5f079aeb-c768-4ad0-9130-56e6e5370499',\n",
       " '9e62a429-4c59-48f4-9b6f-ebb11d0c1987',\n",
       " '639ef334-ff68-48df-bfcd-553b844310a0',\n",
       " 'd006af0c-dee4-4a16-90b9-37197bd589ce',\n",
       " '63810301-6f2f-4508-a357-87d784b2cbaf',\n",
       " 'dfa7c3c3-206b-4fe5-b914-6514a7168b92',\n",
       " '288dc62a-3aaf-4ed4-a871-b0980e63591e',\n",
       " 'b7e2192d-0fd9-4098-b08c-926530298071',\n",
       " '3b9d8773-4984-4815-9118-ed49348f7802',\n",
       " '45db4126-c6b7-45c6-a4ce-aa53947ae672',\n",
       " '93f0e810-e400-4736-92c1-c06bf918c6d5',\n",
       " 'f83c603d-95aa-4932-8e07-9596dfe0f4a6',\n",
       " '466b4868-f4dd-4986-bf3f-724eb9889fdc',\n",
       " 'c13f66a6-3e4e-4a53-bc91-611a8eb4714e',\n",
       " 'a0f20021-e91b-4627-8e42-c74d531d64a7',\n",
       " '58b59ff4-5090-432e-8d53-74545268f091',\n",
       " '8df03a56-c218-47d3-9849-96a031e0962c',\n",
       " '72136995-6473-4371-872d-e3ad8a40ac9b',\n",
       " '004838a3-1b51-4d82-97b7-51e77636a6d6',\n",
       " '1f9da242-3008-4e09-bc29-ff157e9231dd',\n",
       " '4c41b9b3-964e-4390-badb-d29be4aaab54',\n",
       " '2d81ec12-b038-4bd5-8ca3-81ee7b190425',\n",
       " 'e2bddaf8-dcaa-42cb-9443-98b0a6a8a1b9',\n",
       " '6da3fb6a-39e9-46bc-b5df-c23b0f863b58',\n",
       " '24bb256a-5e6e-4990-beb8-5455b1d9a156',\n",
       " '8be47b67-0f0e-4d20-82d4-1540fb635d4d',\n",
       " '49aba7d4-77bd-4372-8500-39acc07e677c',\n",
       " '3dacd981-bd4b-431b-8c69-5e9999d762a3',\n",
       " '57ff3d47-62b8-4dc2-8331-db74b4bffe37',\n",
       " '1e8c0ed0-40d0-447c-8b66-ffe0f756c736',\n",
       " '6f4f4a80-65e1-413b-853c-477161613ed9',\n",
       " '597d0ab2-8780-4ec7-bb21-0fe7a895ce57',\n",
       " 'a7bbecc8-8155-4a8c-8e12-83574e0219d2',\n",
       " '493a83c1-022a-49bb-8bf6-3defa98977d6',\n",
       " 'eaff5a72-20f0-4250-83b0-555cd22bf91f',\n",
       " 'a1bc6265-bee0-4164-8300-36cd3ad94a56',\n",
       " '37f588ad-9e37-4688-aeba-653632490d16',\n",
       " '98437636-20e3-4260-8831-65751aa14627',\n",
       " '9a47f8d1-7b64-4ff3-b03a-8f8193b9bd72',\n",
       " '5a3514df-8b53-48f0-914d-9c34fc12ca1a',\n",
       " 'b8623142-b349-4c34-873d-abff6d080e49',\n",
       " '40f0be12-3129-4f7e-8b67-a84878fa7bbd',\n",
       " '000f9e50-c986-43e0-b30d-ac68c9b5ed5d',\n",
       " 'd3bf54dc-e338-4be3-9e07-40773158de60',\n",
       " '7de4732c-938f-4459-879a-fff7c12f59eb',\n",
       " '064db956-5649-4529-a524-2a828c2667fe',\n",
       " '091acb13-6e65-46d5-9103-f4d80538944e',\n",
       " '4f44f0b3-8d07-41ac-9a04-444ed99afe9c',\n",
       " '7cd66adc-f335-4f6a-a342-d44b12536002',\n",
       " '4afb572e-7d9d-4d8f-8f95-cdb23a6928c3',\n",
       " '6df82476-0633-418f-ba42-01c3b6e9c6a9',\n",
       " 'dfa49428-3be6-4edf-8b8a-aa1e72105dea',\n",
       " '1457ceac-8b7d-4f73-938b-9d764f95b0a7',\n",
       " 'f9bf1a54-0476-49e8-83cb-427d877bdf63',\n",
       " '0e26e849-2ce1-46ce-ab8f-cb1ce8792ad8',\n",
       " '75382324-494f-456a-969a-a000ea5cc17b',\n",
       " 'f8c1c68d-3846-4959-b442-ec7c32be669a',\n",
       " 'c806102d-09b1-4a42-8fbe-c4ab6b3a32fa',\n",
       " '69709e95-4f26-45f7-ad4b-43cf140509df',\n",
       " 'e9f08897-30d2-4d8b-8e56-7ae590b34f8e',\n",
       " '1a301878-436c-40b0-a9cf-141fa19b30f0',\n",
       " '19b56151-84e4-4671-b964-ebdb0d77c43c',\n",
       " 'f0ae51be-83d5-43eb-8cfc-68ce88aafb55',\n",
       " 'ed3a783d-07ea-4c3c-a923-11647a349a8a',\n",
       " '75dcde6c-3dab-483e-bd10-54841d1de772',\n",
       " 'bc9d4103-ae6e-4058-a265-cb01c21ec419',\n",
       " 'f8285e88-2981-4c53-841f-9761b57bb109',\n",
       " '57c13e01-82ec-4e9b-869e-4a649c7290ad',\n",
       " '5baf4ff0-3c81-4c90-88bc-807001786895',\n",
       " '1b49408c-a910-459b-a2ca-e6e67bd55c92',\n",
       " '461eb7ea-c1cf-4353-be41-0337a8b7bf56',\n",
       " '89e48aec-d5c7-4cdb-992d-10d04d2eb95b',\n",
       " '5bb120c9-a970-4345-88c8-7813fc68e4a4',\n",
       " '0e42ee1c-9ec9-41db-9d2a-4d2c98207486',\n",
       " '809cf259-a555-4946-8eaf-0bcd971e2914',\n",
       " '2466e76c-37a3-4bca-895b-15284c1d9a1a',\n",
       " '9398c73b-b79e-4005-a94c-fa7d17862d52',\n",
       " '73918c1f-f6c5-4622-b6f6-6b9ae2884344',\n",
       " 'b5819f22-5c32-4804-942b-192688f51fe6',\n",
       " '9e4485ba-a359-4b2c-b650-b64188056047',\n",
       " '456b5c22-7cad-43a4-b4da-83260a13a438',\n",
       " 'a2a3b0c8-ccbc-4744-821d-41612dd1ca33',\n",
       " 'af8c68b8-ced8-46c3-acb2-ea8c7e95c7de',\n",
       " '8576c341-508e-4372-a82f-87d64834c6d9',\n",
       " '11650053-e17d-4e5c-96bc-4e2f224cb87c',\n",
       " 'b7f79f93-a8e0-4a1b-ab98-0c29293830d2',\n",
       " 'a6bcf12f-4499-40ef-a04b-2d89b5cc71ef',\n",
       " '86e8c5f6-cb4d-400b-8368-89034a4b3fdc',\n",
       " 'abee0237-34c1-4e6d-9ca7-848e71a42b82',\n",
       " '880f643f-abe9-4faa-9cb5-704512cd90c9',\n",
       " '5bf2e35b-e0c7-4a8a-9248-419e5607dfa7',\n",
       " '08ecc948-4a10-4bf5-8776-14fdf7459f24',\n",
       " '2a4876ba-b559-4a1f-a51a-8d3bb705f796',\n",
       " '6e4598f1-49a9-434e-8101-a954243af8fd',\n",
       " 'f81d38fd-8fd3-480d-b768-a67b48818df1',\n",
       " '120a91f0-f0fc-4f8d-bfae-818f95e7cf33',\n",
       " 'd9f9850b-fb8d-4958-9896-48198f11e4b1',\n",
       " '46898890-c9e7-4617-838e-2a3591b9c6df',\n",
       " 'b4854d80-57a0-4ef8-b142-afc4ae447276',\n",
       " 'e9d11e8c-c175-40c6-983f-400228168e68',\n",
       " '659f9ba4-2f24-412d-9c82-651f30f82018',\n",
       " 'b729830b-79e1-4646-bd41-16f520e503ce',\n",
       " '40cb29c9-89a2-4b87-8a49-c00729c98c4e',\n",
       " '7ceee4c0-5647-4a96-84c3-97fa9c215e16',\n",
       " 'af717d02-1095-4ec2-b2f8-d9d293528ea4',\n",
       " '62433c8d-a11a-406d-8cfd-86e38ea520b7',\n",
       " '028f6bdd-1724-4e48-8aa4-d90650550e27',\n",
       " 'fe8c06ed-1347-4521-9629-dcaa3cf14c47',\n",
       " '88db7042-7aa6-4b8e-b3bc-613b2933b6a7',\n",
       " '50f74d21-97f8-48f9-a9fe-978a0920efa6',\n",
       " 'f3cce890-6bab-4d86-a426-f938db289da3',\n",
       " 'df7ef338-6ba5-4ebf-8a1f-bbbb1f562718',\n",
       " '5c95ec22-92f8-414a-af94-5187c823ce80',\n",
       " '716dbe7b-2e2d-4715-abba-462c18d9795f',\n",
       " 'a055e809-a844-4525-8498-7ef615507349',\n",
       " '52284ae4-1feb-491f-96c0-ff4edc4fd106',\n",
       " '0791d251-731e-4bc5-915f-e8eb8741da04',\n",
       " '27d9ef31-476d-426c-b90d-cafb2c830bbe',\n",
       " 'd77f7b33-870b-40e5-8797-fc1c304d3f00',\n",
       " '16ab1f33-5afe-4938-8e57-85ed2e49da7a',\n",
       " '8c59e5ab-7df1-4f02-bf3e-e3f38abefd81',\n",
       " '4b64a0ca-dfc4-4d5a-9af6-424d9c96e4e2',\n",
       " '744815b8-4d90-4959-b304-5f3f6535a124',\n",
       " '4ff27894-f435-4a54-ac01-5abfdaffe509',\n",
       " '596abd13-049d-4016-8616-b091dd89b872',\n",
       " '89802e6f-85b4-44d7-9e46-8f3c0d03bc95',\n",
       " '3b6cd4f4-4e98-444f-9826-29bafc09aae5',\n",
       " 'ffa95f2d-e823-41e9-8e04-3db912c6f80b',\n",
       " 'be0382ab-0acf-4d58-9c3d-be6ab3f7f371',\n",
       " 'f082b489-b2bc-45bb-965c-0b3b2807c32f',\n",
       " '0107f509-cfd7-406d-ba8f-190d86921c98',\n",
       " '6e8dcae5-020d-4195-a822-436e1c3ab700',\n",
       " '4e4860f3-e19a-468b-9b50-c9a4981ebea5',\n",
       " '58f7b6b5-4fd2-40f2-831f-6df99deb57ca',\n",
       " 'd5f6755c-57fe-4d9b-8778-8cda5b79c4e1',\n",
       " '6ecb69fe-4ecc-4583-b0f5-ca8f38067c3c',\n",
       " 'd3af86df-bf12-493c-97b5-93927e084ff1',\n",
       " 'ea219352-6194-4770-9ca8-d84b6af728ac',\n",
       " 'a83e2bcf-c425-4ee0-8c98-a0d9a090526c',\n",
       " '19d6017b-a687-431a-b051-aee7091dd50a',\n",
       " '46d0c626-b320-4476-aa99-a2259afdce22',\n",
       " '3ebacf85-2c7e-40fe-8d35-1ac9e9851c73',\n",
       " '18c03af8-f256-4b9b-a302-46741eb3e647',\n",
       " '3fe181ee-8bc5-46f6-bfcd-6135ca0b2538',\n",
       " '828fa75c-70ee-4aea-9914-763d7edb2fd0',\n",
       " 'be54f0bb-8ee6-4de3-a562-576b12d5bbaa',\n",
       " '52483d39-f393-4c54-916f-090d6f7b940a',\n",
       " '5cc7e0be-bb26-4956-8130-55f04f0935f7',\n",
       " '164e91be-b972-4b8b-97b5-d51fa46c000c',\n",
       " '90660d56-4695-41cc-913a-a1d25d089ba8',\n",
       " 'abb6927c-e6dd-4598-a2c5-f35e0295c34e',\n",
       " '91866ad1-658a-44b7-b405-4b4d4fc4fcfa',\n",
       " 'f479d6c8-238b-4590-acf9-4f32e70a093e',\n",
       " '19f5d515-8240-4677-a255-b53f2941f6f7',\n",
       " 'a699ed9e-a121-4e0e-a1f7-67c19910e841',\n",
       " 'ef89f194-f486-4b86-83f9-c56c33bf3dd6',\n",
       " '94393061-7d2a-4aef-a27b-8f3fa859396b',\n",
       " '87363d34-61ba-4634-9f28-a026d505cc23',\n",
       " '81289bc6-6eff-46d2-8e84-6d511c3c3fca',\n",
       " '7a56effd-3ca4-499c-b9ac-c15fa80d0031',\n",
       " 'dd143442-6389-48ad-a2aa-ae7a81f51f65',\n",
       " 'f87317d3-68c0-4380-9dc9-cb1ba7512d57',\n",
       " 'fd5b9865-7015-4cb1-8366-56726efb6a70',\n",
       " 'd08b5ea5-8e05-4e9f-9c15-055cdacfe859',\n",
       " 'd6cf957c-4d8c-4203-9a6e-161a05f633e5',\n",
       " '15906644-df71-4ea9-8399-6f496f4df2ee',\n",
       " '0b1cfa57-a2b4-447d-9432-91f7f8f82da8',\n",
       " '7e1a4ba5-3b5a-4afe-83d9-52ad7f6d0bc6',\n",
       " '0ab28f16-4e2a-41a6-a1b5-5bce72473d96',\n",
       " '4acc9d53-4a5a-4894-9b7a-ec2aee982fa1',\n",
       " '0a54be49-958f-4876-81e9-33a8d64ae223',\n",
       " '74531960-f553-4f93-9707-8bc6860909a4',\n",
       " '57dcd58d-27c2-4b7c-a014-0fb3530fb0e5',\n",
       " '42f71637-852e-4d01-9654-8766a78e6a52',\n",
       " '93e98eb3-70bc-41a2-9318-268f5f599322',\n",
       " '2e40f926-95a7-4db2-947b-44647d52ff64',\n",
       " 'dab4a876-0dfe-4847-bc30-abd2c584467c',\n",
       " '3f286b49-12d6-44bd-a4f4-5e37c5eb5094',\n",
       " '7cfc0f83-3692-491e-b054-e43218713692',\n",
       " 'e74e4114-f613-45a0-96f1-6104e96bf182',\n",
       " '02c0f3d3-a997-496b-ba73-6f2a3ec85932',\n",
       " '68637bbd-bc5a-496b-982f-fb4b4c602a86',\n",
       " 'baec2ac4-a47a-4519-8d6e-baed11a5a9f5',\n",
       " '1d255854-8af6-4e57-8b7c-34b09e82a3a4',\n",
       " 'b6f2d49c-cdcf-4e7a-a9d0-4ad45c3b375e',\n",
       " 'b1e78139-2f5e-4a07-96b9-6be97abd09ee',\n",
       " '54a6a1da-900c-4c70-87b3-19e852cd4ad0',\n",
       " 'be46ae47-2d6a-4329-a039-db01ebd72c89',\n",
       " 'cf648820-f335-45a4-85b7-655643ffbcae',\n",
       " '58c4c4ba-05f1-4382-9e25-ca1a85791376',\n",
       " 'd822af18-ae3c-4d2b-bdb4-a68d08748fbc',\n",
       " '7bb4b999-8bc0-4a47-b8a4-5739f6f73e4d',\n",
       " 'd63d1cc2-0010-4201-9805-cfda052db57b',\n",
       " 'c8ac0683-28d2-4bc7-bc89-8f40849ab1e0',\n",
       " 'bbf2c185-9b32-4ab4-ac0e-0dabf82278f7',\n",
       " '0f89bf27-dce6-42ff-a49e-26626e8a4338',\n",
       " '424adf85-da1f-4454-90b3-96befb737f60',\n",
       " '956961d7-f269-4bb7-8e48-c09bc3f4cb75',\n",
       " 'db246774-55a6-4b8e-a07f-4227df239984',\n",
       " '7219d848-d23e-4356-810c-61fa5536e1c4',\n",
       " '416db6d9-25c9-4f29-9294-db8934a4ef8c',\n",
       " '5c7e2db0-f27b-48ea-9f27-90ed269fb0d4',\n",
       " '430ae6ff-03b9-4ff9-a9d8-d446a64d03d5',\n",
       " '95e3deff-cf3a-4271-93ae-81a7865f60e0',\n",
       " 'bdb8537c-3459-430d-b763-ac4f1f055914',\n",
       " '306cdf05-2b4c-4e2e-a54e-4eabd778e9ae',\n",
       " '5d175953-b5a7-4420-a12b-ce0d442c42d5',\n",
       " 'b27c8f76-2f5b-4823-971f-6681fbbfd3c9',\n",
       " '62b0dd22-6e56-4f31-83d3-7d6e5806c2bd',\n",
       " '2df25780-222b-41f5-898d-f8d84d3010be',\n",
       " '5b21c5b0-89be-4d30-a143-19e7c61abdbc',\n",
       " '42639952-7d5f-4099-bf72-db080c814638',\n",
       " 'a9f25593-c652-43f1-a8fd-8be07d74defe',\n",
       " '7a5ef5ad-020c-4a71-97c0-a849b5cdd97b',\n",
       " '12ba5197-6249-4299-a7f4-e50903e3e16c',\n",
       " '7ec5ad3b-df81-43d9-b3ee-1e8e672725ea',\n",
       " '5e64af37-2855-41bf-9a37-dbd9e0a87aec',\n",
       " 'd3eb76f7-8b82-4c69-9488-9e38d780f6b0',\n",
       " 'c5078326-2242-44f3-a590-4615a3aac780',\n",
       " 'cb4c835e-a419-46e4-81d0-5defd2036666',\n",
       " '634bcae7-5968-4463-8681-df832e39d2c0',\n",
       " '52d37fbb-9e6f-402b-a44a-7150b520b2d5',\n",
       " '58f802b1-f007-4629-8cde-606e20cc8840',\n",
       " 'a721022b-921c-42af-a0c5-7ac0c4e94ea1',\n",
       " '63c16a2b-5a07-45bd-8584-97aabdb04f18',\n",
       " '8545552f-436b-4a0e-b276-2ccf70ced82e',\n",
       " '82cc8bad-e8b2-4dc8-ad75-5bfb26673bf5',\n",
       " 'b4a62e46-fc67-4046-8b8d-01cbda4b71d6']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = InMemoryVectorStore(embedding_model)\n",
    "chunks = [chunk for f in chosenfiles for chunk in chunk_text_with_overlap(get_html(f), max_tokens=128, overlap=40)]\n",
    "print(len(chunks))\n",
    "vector_store.add_texts(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a0f20021-e91b-4627-8e42-c74d531d64a7', page_content='your username is username and that you are using the nexus cluster, have been assigned the nexusgroup submission nodes, and are assigned compute node tron00. umiacs. umd. edu : ssh - n - f - l localhost : 8888 : tron00. umiacs. umd. edu : 8889 username @ nexusgroup. umiacs. umd. edu you can then open a web browser and type in localhost : 8888 to access the notebook. notes : later versions of jupyter have token authentication enabled by default -'),\n",
       " Document(id='aa4260fc-b790-4ed5-9ea1-1b98a8972074', page_content='nexus - scratch / username / fs / nexus - containers / pytorch / pytorch _ 1. 13. 0 + cu117. sif bash you can now write / run your own pytorch python code interactively within the container or just make a python script that you can call directly from the apptainer exec command for batch processing. shared containers portable images called singularity image format or. sif files can be copied and shared. nexus maintains some shared containers in / fs / nexus - containers. these are arranged by the application ( s ) that are installed. docker'),\n",
       " Document(id='a1772c69-f61d-4ae9-9341-63190e8bc88c', page_content='. these can be found in / fs / nexus - containers / pytorch. you can just run one of the example images by doing the following ( you should have already allocated a interactive job with a gpu in nexus ). it will use the default script found at / srv / tensor. py within the image. $ hostname & & nvidia - smi - l tron38. umiacs. umd. edu gpu 0 : nvidia rtx a4000 ( uuid : gpu - 4a0a5644 - 9fc8 - 84b4 -'),\n",
       " Document(id='58b59ff4-5090-432e-8d53-74545268f091', page_content='. edu you can then open a web browser and type in localhost : 8888 to access the notebook. notes : later versions of jupyter have token authentication enabled by default - you will need to prepend the /? token = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx part of the url provided by the terminal output after starting the notebook in order to connect if this is the case. e. g. localhost : 8888 /? token = fcc6bd0f996e7aa89'),\n",
       " Document(id='54fa5a08-000b-49d7-be03-882a278f7117', page_content='##83 - eb2b1d501f1e ) gpu 1 : nvidia geforce gtx 1080 ti ( uuid : gpu - d681a21a - 8cdd - e624 - 6bf8 - 5b0234584ba2 ) nexus containers in our nexus environment we have some example containers based on our pytorch _ docker project. these can be found in / fs / nexus - containers / pytorch. you can just run one of the example images by doing the following ( you should have already allocated a interactive'),\n",
       " Document(id='a76d677f-c77a-41b7-88bc-c98df6f879be', page_content=\"accounts in umiacs are sponsored. if you don ' t already have a umiacs account, please see accounts for information on getting one. you need a full umiacs account ( not a collaborator account ) in order to access nexus. access your access to submission nodes ( alternatively called login nodes ) for nexus computational resources is determined by your account sponsor ' s department, center, or lab affiliation. you can log into the umiacs directory cr application and select the computational resource ( cr ) in the list that has the prefix nexus. the hosts section lists your available submission nodes - generally a pair of nodes of the format\"),\n",
       " Document(id='cd0c4011-409c-4656-99c6-183ceaa36d7c', page_content=\"- - partition, and / or - - qos ( respectively ) to be able to adequately submit jobs to the nexus. for resources, you may need to specify - - time for time, - - ntasks for cpus, - - mem for ram, and - - gres = gpu for gpus in your submission arguments to meet your requirements. there are defaults for all four, so if you don ' t specify something, you may be scheduled with a very minimal set of time and resources ( e. g., by default, no gpus are included if you do not specify - - gres\"),\n",
       " Document(id='d61a31e2-d752-490e-ad56-659f54db5b43', page_content='##s. umd. edu gpu 0 : nvidia rtx a4000 ( uuid : gpu - 4a0a5644 - 9fc8 - 84b4 - 5d22 - 65d45ca36506 ) $ apptainer run - - nv / fs / nexus - containers / pytorch / pytorch _ 1. 13. 0 + cu117. sif 99 984. 5538940429688 199 654. 1710815429688 299 435. 662353515625 399 291')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 overlap seemed decent from my testing but 20 is too small, but having 100 overlap with 128 length is a lot of chunks\n",
    "vector_store.similarity_search(query=\"python notebook nexus\", k=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "way bigger model testing idk if it works at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_tokenizer': Tokenizer(version=\"1.0\", truncation=TruncationParams(direction=Right, max_length=512, strategy=LongestFirst, stride=0), padding=PaddingParams(strategy=Fixed(512), direction=Right, pad_to_multiple_of=None, pad_id=0, pad_type_id=0, pad_token=\"[PAD]\"), added_tokens=[{\"id\":0, \"content\":\"[PAD]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":100, \"content\":\"[UNK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":101, \"content\":\"[CLS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":102, \"content\":\"[SEP]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":103, \"content\":\"[MASK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=BertNormalizer(clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True), pre_tokenizer=BertPreTokenizer(), post_processor=TemplateProcessing(single=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, type_id=0), SpecialToken(id=\"[SEP]\", type_id=0)], pair=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, type_id=0), SpecialToken(id=\"[SEP]\", type_id=0), Sequence(id=B, type_id=1), SpecialToken(id=\"[SEP]\", type_id=1)], special_tokens={\"[CLS]\":SpecialToken(id=\"[CLS]\", ids=[101], tokens=[\"[CLS]\"]), \"[SEP]\":SpecialToken(id=\"[SEP]\", ids=[102], tokens=[\"[SEP]\"])}), decoder=WordPiece(prefix=\"##\", cleanup=True), model=WordPiece(unk_token=\"[UNK]\", continuing_subword_prefix=\"##\", max_input_chars_per_word=100, vocab={\"[PAD]\":0, \"[unused0]\":1, \"[unused1]\":2, \"[unused2]\":3, \"[unused3]\":4, \"[unused4]\":5, \"[unused5]\":6, \"[unused6]\":7, \"[unused7]\":8, \"[unused8]\":9, \"[unused9]\":10, \"[unused10]\":11, \"[unused11]\":12, \"[unused12]\":13, \"[unused13]\":14, \"[unused14]\":15, \"[unused15]\":16, \"[unused16]\":17, \"[unused17]\":18, \"[unused18]\":19, \"[unused19]\":20, \"[unused20]\":21, \"[unused21]\":22, \"[unused22]\":23, \"[unused23]\":24, \"[unused24]\":25, \"[unused25]\":26, \"[unused26]\":27, \"[unused27]\":28, \"[unused28]\":29, \"[unused29]\":30, \"[unused30]\":31, \"[unused31]\":32, \"[unused32]\":33, \"[unused33]\":34, \"[unused34]\":35, \"[unused35]\":36, \"[unused36]\":37, \"[unused37]\":38, \"[unused38]\":39, \"[unused39]\":40, \"[unused40]\":41, \"[unused41]\":42, \"[unused42]\":43, \"[unused43]\":44, \"[unused44]\":45, \"[unused45]\":46, \"[unused46]\":47, \"[unused47]\":48, \"[unused48]\":49, \"[unused49]\":50, \"[unused50]\":51, \"[unused51]\":52, \"[unused52]\":53, \"[unused53]\":54, \"[unused54]\":55, \"[unused55]\":56, \"[unused56]\":57, \"[unused57]\":58, \"[unused58]\":59, \"[unused59]\":60, \"[unused60]\":61, \"[unused61]\":62, \"[unused62]\":63, \"[unused63]\":64, \"[unused64]\":65, \"[unused65]\":66, \"[unused66]\":67, \"[unused67]\":68, \"[unused68]\":69, \"[unused69]\":70, \"[unused70]\":71, \"[unused71]\":72, \"[unused72]\":73, \"[unused73]\":74, \"[unused74]\":75, \"[unused75]\":76, \"[unused76]\":77, \"[unused77]\":78, \"[unused78]\":79, \"[unused79]\":80, \"[unused80]\":81, \"[unused81]\":82, \"[unused82]\":83, \"[unused83]\":84, \"[unused84]\":85, \"[unused85]\":86, \"[unused86]\":87, \"[unused87]\":88, \"[unused88]\":89, \"[unused89]\":90, \"[unused90]\":91, \"[unused91]\":92, \"[unused92]\":93, \"[unused93]\":94, \"[unused94]\":95, \"[unused95]\":96, \"[unused96]\":97, \"[unused97]\":98, ...})),\n",
       " '_decode_use_source_tokenizer': False,\n",
       " 'init_inputs': (),\n",
       " 'init_kwargs': {'do_lower_case': True,\n",
       "  'unk_token': AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "  'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "  'pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "  'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "  'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "  'tokenize_chinese_chars': True,\n",
       "  'strip_accents': None,\n",
       "  'clean_up_tokenization_spaces': True,\n",
       "  'max_length': 8000,\n",
       "  'model_max_length': 32768,\n",
       "  'pad_to_multiple_of': None,\n",
       "  'pad_token_type_id': 0,\n",
       "  'padding_side': 'right',\n",
       "  'stride': 0,\n",
       "  'truncation_side': 'right',\n",
       "  'truncation_strategy': 'longest_first',\n",
       "  'name_or_path': 'dunzhang/stella_en_400M_v5'},\n",
       " 'name_or_path': 'dunzhang/stella_en_400M_v5',\n",
       " '_processor_class': None,\n",
       " 'model_max_length': 32768,\n",
       " 'padding_side': 'right',\n",
       " 'truncation_side': 'right',\n",
       " 'model_input_names': ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'clean_up_tokenization_spaces': True,\n",
       " 'split_special_tokens': False,\n",
       " 'deprecation_warnings': {},\n",
       " '_in_target_context_manager': False,\n",
       " 'chat_template': None,\n",
       " '_bos_token': None,\n",
       " '_eos_token': None,\n",
       " '_unk_token': AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '_sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '_pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '_cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '_mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '_pad_token_type_id': 0,\n",
       " '_additional_special_tokens': [],\n",
       " 'verbose': False,\n",
       " 'do_lower_case': True}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_big = AutoTokenizer.from_pretrained(\"dunzhang/stella_en_400M_v5\")\n",
    "tokenizer_big.__dict__\n",
    "# this isn't right at all here the size is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 512 sequence length, so again truncated\n",
    "embedding_model_big = HuggingFaceEmbeddings(model_name=\"dunzhang/stella_en_400M_v5\", model_kwargs={\"trust_remote_code\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"nexus - umiacs nexus from umiacs jump to navigation jump to search the nexus is the combined scheduler of resources in umiacs. the resource manager for nexus is slurm. resources are arranged into partitions where users are able to schedule computational jobs. users are arranged into a number of slurm accounts based on faculty, lab, or center investments. contents 1 getting started 1. 1 access 1. 2 jobs 1. 2. 1 interactive 1. 2. 2 batch 2 partitions 3 quality of service ( qos ) 3. 1 job qos 3. 2 partition qos 4 storage 4. 1 home directories 4. 2 scratch directories 4. 2. 1 network scratch directories 4. 2. 2 local scratch directories 4. 3 faculty allocations 4. 4 project allocations 4. 5 datasets getting started all accounts in umiacs are sponsored. if you don ' t already have a umiacs account, please see accounts for information on getting one. you need a full umiacs account ( not a collaborator account ) in order to access nexus. access your access to submission nodes ( alternatively called login nodes ) for nexus computational resources is determined by your account sponsor ' s department, center, or lab affiliation. you can log into the umiacs directory cr application and select the computational resource ( cr ) in the list that has the prefix nexus. the hosts section lists your available submission nodes - generally a pair of nodes of the format nexus < department, lab, or center abbreviation > [ 00, 01 ], e. g., nexusgroup00 and nexusgroup01. note - umiacs requires multi - factor authentication through our duo instance. this is completely discrete from both umd ' s and csd ' s duo instances. you will need to enroll one or more devices to access resources in umiacs, and will be prompted to enroll when you log into the directory application for the first time. once you have identified your submission nodes, you can ssh directly into them. from there, you are able to submit to the cluster via our slurm workload manager. you need to make sure that your submitted jobs have the correct account, partition, and qos. jobs slurm jobs are submitted by either srun or sbatch depending if you are doing an interactive job or batch job, respectively. you need to provide the where / how / who to run the job and specify the resources you need to run with. for the who / where / how, you may be required to specify - - account, - - partition, and / or - - qos ( respectively ) to be able to adequately submit jobs to the nexus. for resources, you may need to specify - - time for time, - - ntasks for cpus, - - mem for ram, and - - gres = gpu for gpus in your submission arguments to meet your requirements. there are defaults for all four, so if you don ' t specify something, you may be scheduled with a very minimal set of time and resources ( e. g., by default, no gpus are included if you do not specify - - gres = gpu ). for more information about submission flags for gpu resources, see here. you can also can run man srun on your submission node for a complete list of available submission arguments. for a list of available gpu types on nexus and their specs, please see nexus / gpus. interactive once logged into a submission node, you can run simple interactive jobs. if your session is interrupted from the submission node, the job will be killed. as such, we encourage use of a terminal multiplexer such as tmux. $ srun - - pty - - ntasks = 4 - - mem = 2gb - - gres = gpu : 1 nvidia - smi - l gpu 0 : nvidia rtx a4000 ( uuid : gpu - ae5dc1f5 - c266 - 5b9f - 58d5 - 7976e62b3ca1 ) batch batch jobs are scheduled with a script file with an optional ability to embed job scheduling parameters via variables that are defined by # sbatch lines at the top of the file. you can find some examples in our slurm / jobsubmission documentation. partitions the slurm resource manager uses partitions to act as job queues which can restrict size, time and user limits. the nexus has a number of different partitions of resources. different centers, labs, and faculty are able to invest in computational resources that are restricted to approved users through these partitions. partitions usable by all non - class account users : nexus / tron - pool of resources available to all umiacs and csd faculty and graduate students. scavenger - preemption partition that supports nodes from multiple other partitions. more resources are\",\n",
       " 'ae5dc1f5 - c266 - 5b9f - 58d5 - 7976e62b3ca1 ) batch batch jobs are scheduled with a script file with an optional ability to embed job scheduling parameters via variables that are defined by # sbatch lines at the top of the file. you can find some examples in our slurm / jobsubmission documentation. partitions the slurm resource manager uses partitions to act as job queues which can restrict size, time and user limits. the nexus has a number of different partitions of resources. different centers, labs, and faculty are able to invest in computational resources that are restricted to approved users through these partitions. partitions usable by all non - class account users : nexus / tron - pool of resources available to all umiacs and csd faculty and graduate students. scavenger - preemption partition that supports nodes from multiple other partitions. more resources are available to schedule simultaneously than in other partitions, however jobs are subject to preemption rules. you are responsible for ensuring your jobs handle this preemption correctly. the slurm scheduler will simply restart a preempted job with the same submission arguments when it is available to run again. for an overview of things you can check within scripts to determine if your job was preempted / resumed, see slurm / preemption. partitions usable by classaccounts : class - pool available for umiacs class accounts sponsored by either umiacs or csd faculty. partitions usable by specific lab / center users : nexus / cbcb - cbcb lab pool available for cbcb lab members. nexus / clip - clip lab pool available for clip lab members. nexus / cml - cml lab pool available for cml lab members. nexus / gamma - gamma lab pool available for gamma lab members. nexus / mbrc - mbrc lab pool available for mbrc lab members. nexus / mc2 - mc2 lab pool available for mc2 lab members. nexus / vulcan - vulcan lab pool available for vulcan lab members. quality of service ( qos ) slurm uses quality of service ( qos ) both to provide limits on job sizes ( termed by us as \" job qos \" ) as well as to limit resources used by all jobs running in a partition, either per user or per group ( termed by us as \" partition qos \" ). job qos job qos are used to provide limits on the size of job that you can run. you should try to allocate only the resources your job actually needs, as resources that each of your jobs schedules are counted against your fair - share priority in the future. default - default job qos. limited to 4 cpu cores, 1 gpu, and 32gb ram per job. the maximum wall time per job is 3 days. medium - limited to 8 cpu cores, 2 gpus, and 64gb ram per job. the maximum wall time per job is 2 days. high - limited to 16 cpu cores, 4 gpus, and 128gb ram per job. the maximum wall time per job is 1 day. scavenger - no resource limits per job, only a maximum wall time per job of 3 days. you are responsible for ensuring your job requests multiple nodes if it requests resources beyond what any one node is capable of. 576 total cpu cores, 72 total gpus, and 2304gb total ram are permitted simultaneously across all of your jobs running with this job qos. this job qos is both only available in the scavenger partition and the only job qos available in the scavenger partition. to use this job qos, include - - partition = scavenger and - - account = scavenger in your submission arguments. do not include any job qos argument other than - - qos = scavenger ( optional ) or submission will fail. you can display these job qos from the command line using the show _ qos command. by default, the command will only show job qos that you can access. the above four job qos are the ones that everyone can access. $ show _ qos name maxwall maxtres maxjobspu maxtrespu - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - default 3 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g high 1 - 00 : 00 : 00 cpu = 16, gres / gpu = 4,',\n",
       " 'using the show _ qos command. by default, the command will only show job qos that you can access. the above four job qos are the ones that everyone can access. $ show _ qos name maxwall maxtres maxjobspu maxtrespu - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - default 3 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g high 1 - 00 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g medium 2 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g scavenger 3 - 00 : 00 : 00 cpu = 576, gres / gpu = 72, mem = 2304g if you want to see all job qos, including those that you do not have access to, you can use the show _ qos - - all command. $ show _ qos - - all name maxwall maxtres maxjobspu maxtrespu - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - cml - cpu 7 - 00 : 00 : 00 8 cml - default 7 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g 2 cml - high 1 - 12 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g 2 cml - high _ long 14 - 00 : 00 : 00 cpu = 32, gres / gpu = 8 8 gres / gpu = 8 cml - medium 3 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g 2 cml - scavenger 3 - 00 : 00 : 00 gres / gpu = 24 cml - very _ high 1 - 12 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g 8 gres / gpu = 12 default 3 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g high 1 - 00 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g highmem 21 - 00 : 00 : 00 cpu = 32, mem = 2t huge - long 10 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g medium 2 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g scavenger 3 - 00 : 00 : 00 cpu = 576, gres / gpu = 72, mem = 2304g vulcan - cpu 2 - 00 : 00 : 00 cpu = 1024, mem = 4t 4 vulcan - default 7 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g 2 vulcan - exempt 7 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g 2 vulcan - high 1 - 12 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g 2 vulcan - janus 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 10, mem = 256g vulcan - medium 3 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g 2 vulcan - sailon 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g gres / gpu = 48 vulcan - scavenger 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g to find out what accounts and partitions you have access to, first use the show _ assoc command to show your account / job qos combinations. then, use the scontrol show partition command and note the allowaccount',\n",
       " '= 256g 2 vulcan - high 1 - 12 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g 2 vulcan - janus 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 10, mem = 256g vulcan - medium 3 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g 2 vulcan - sailon 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g gres / gpu = 48 vulcan - scavenger 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g to find out what accounts and partitions you have access to, first use the show _ assoc command to show your account / job qos combinations. then, use the scontrol show partition command and note the allowaccounts entry for each listed partition. you are able to submit to any partition that allows an account that you have. if you need to use an account other than the default account nexus, you will need to specify it via the - - account submission argument. partition qos partition qos are used to limit resources used by all jobs running in a partition, either per user ( maxtrespu ) or per group ( grptres ). to view partition qos, use the show _ partition _ qos command. $ show _ partition _ qos name maxsubmitpu maxtrespu grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - scavenger 500 cpu = 576, gres / gpu = 72, mem = 2304g tron 500 cpu = 32, gres / gpu = 4, mem = 256g if you want to see all partition qos, including those that you do not have access to, you can use the show _ partition _ qos - - all command. $ show _ partition _ qos - - all name maxsubmitpu maxtrespu grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - name maxsubmitpu maxtrespu grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - cbcb 500 cpu = 1260, mem = 50016g cbcb - heng 500 class 500 cpu = 32, gres / gpu = 4, mem = 256g clip 500 cpu = 564, mem = 5590g cml 500 cpu = 1128, mem = 11t cml - furongh 500 cml - scavenger 500 gres / gpu = 24 cml - zhou 500 gamma 500 cpu = 648, mem = 5454g mbrc 500 cpu = 240, mem = 2345g mc2 500 cpu = 312, mem = 3092g oasis 500 quics 500 cpu = 328, mem = 3484g scavenger 500 cpu = 576, gres / gpu = 72, mem = 2304g tron 500 cpu = 32, gres / gpu = 4, mem = 256g vulcan 500 cpu = 1392, mem = 12833g vulcan - ampere 500 vulcan - cpu 500 vulcan - ramani 500 vulcan - scavenger 500 note : these qos cannot be used directly when submitting jobs, with the exception of the scavenger qos ( i. e., they are not in the allowqos field for their respective partition ). partition qos limits apply to all jobs running on a given partition, regardless of what job qos is used. for example, in the default non - preemption partition ( tron ), you are restricted to 32 total cpu cores, 4 total gpus, and 256gb total ram at once across all jobs you have running in the partition. lab / group - specific partitions may',\n",
       " 'scavenger 500 cpu = 576, gres / gpu = 72, mem = 2304g tron 500 cpu = 32, gres / gpu = 4, mem = 256g vulcan 500 cpu = 1392, mem = 12833g vulcan - ampere 500 vulcan - cpu 500 vulcan - ramani 500 vulcan - scavenger 500 note : these qos cannot be used directly when submitting jobs, with the exception of the scavenger qos ( i. e., they are not in the allowqos field for their respective partition ). partition qos limits apply to all jobs running on a given partition, regardless of what job qos is used. for example, in the default non - preemption partition ( tron ), you are restricted to 32 total cpu cores, 4 total gpus, and 256gb total ram at once across all jobs you have running in the partition. lab / group - specific partitions may also have their own user limits, and / or may also have group limits on the total number of resources consumed simultaneously by all users that are using their partition, codified by the line in the output above that matches their lab / group name. note that the values listed above in the two \" tres \" columns are not fixed and may fluctuate per - partition as more resources are added to or removed from each partition. all partitions also only allow a maximum of 500 submitted ( running ( r ) or pending ( pd ) ) jobs per user in the partition simultaneously. this is to prevent excess pending jobs causing backfill issues with the slurm scheduler. if you need to submit more than 500 jobs in batch at once, you can develop and run an \" outer submission script \" that repeatedly attempts to run an \" inner submission script \" ( your original submission script ) to submit jobs in the batch periodically, until all job submissions are successful. the outer submission script should use looping logic to check if you are at the max job limit and should then retry submission after waiting for some time interval. an example outer submission script is as follows. in this example, example _ inner. sh is your inner submission script and is not an array job, and you want to run 1000 jobs. if your inner submission script is an array job, adjust the number of jobs accordingly. array jobs must be of size 500 or less. #! / bin / bash numjobs = 1000 i = 0 while [ $ i - lt $ numjobs ] do while [ [ \" $ ( sbatch example _ inner. sh 2 > & 1 ) \" = ~ \" qosmaxsubmitjobperuserlimit \" ] ] do echo \" currently at maximum job submissions allowed. \" echo \" waiting for 5 minutes before trying to submit more jobs. \" sleep 300 done i = $ ( ( $ i + 1 ) ) echo \" submitted job $ i of $ numjobs \" done it is suggested that you run the outer submission script in a tmux session to keep the terminal window executing it from being interrupted. storage all network storage available in nexus is currently nfs based, and comes in a few different flavors. compute nodes also have local storage that can be used. home directories you have 30gb of home directory storage available at / nfshomes / < username >. it has both snapshots and backups enabled. home directories are intended to store personal or configuration files only. we encourage you to not share any data in your home directory. you are encouraged to utilize our gitlab infrastructure to host your code repositories. note : to check your quota on this directory, use the command df - h ~. scratch directories scratch data has no data protection including no snapshots and the data is not backed up. there are two types of scratch directories in the nexus compute infrastructure : network scratch directories local scratch directories please note that class accounts do not have network scratch directories. network scratch directories you are allocated 200gb of scratch space via nfs from / fs / nexus - scratch / < username > where < username > is your umiacs username. it is not backed up or protected in any way. this directory is automounted ; you will need to cd into the directory or request / specify a fully qualified file path to access it. you can view your quota usage by running df - h / fs / nexus - scratch / < username >. you may request a permanent increase of up to 400gb total space without any faculty approval by contacting staff. if you need space beyond 400gb, you will need faculty approval and / or a project allocation for this. if you choose to increase your scratch space beyond 400gb, the increased space is also subject to the 270 tb days limit mentioned in the project allocation section before we check back in for renewal. for example, if you request 1. 4tb total space, you',\n",
       " 'scratch directories you are allocated 200gb of scratch space via nfs from / fs / nexus - scratch / < username > where < username > is your umiacs username. it is not backed up or protected in any way. this directory is automounted ; you will need to cd into the directory or request / specify a fully qualified file path to access it. you can view your quota usage by running df - h / fs / nexus - scratch / < username >. you may request a permanent increase of up to 400gb total space without any faculty approval by contacting staff. if you need space beyond 400gb, you will need faculty approval and / or a project allocation for this. if you choose to increase your scratch space beyond 400gb, the increased space is also subject to the 270 tb days limit mentioned in the project allocation section before we check back in for renewal. for example, if you request 1. 4tb total space, you may have this for 270 days ( 1tb beyond the 400gb permanent increase ). the amount increased beyond 400gb will also count against your faculty member \\' s 20tb total storage limit mentioned below. this file system is available on all submission, data management, and computational nodes within the cluster. local scratch directories each computational node that you can schedule compute jobs on also has one or more local scratch directories. these are always named / scratch0, / scratch1, etc. and are not backed up or protected in any way. these directories are almost always more performant than any other storage available to the job as they are mounted from disks directly attached to the compute node. however, you must stage your data within the confines of your job and extract the relevant resultant data elsewhere before the end of your job. these local scratch directories have a tmpwatch job which will delete unaccessed data after 90 days, scheduled via maintenance jobs to run once a month during our monthly maintenance windows. please make sure you secure any resultant data you wish to keep from these directories at the end of your job. faculty allocations each faculty member can be allocated 1tb of permanent lab space upon request. we can also support grouping these individual allocations together into larger center, lab, or research group allocations if desired by the faculty. please contact staff to inquire. lab space storage is fully protected. it has snapshots enabled and is backed up nightly. project allocations project allocations are available per user for 270 tb days ; you can have a 1tb allocation for up to 270 days, a 3tb allocation for 90 days, etc.. a single faculty member can not have more than 20tb of project allocations across all of their sponsored accounts active simultaneously. network scratch allocation space increases beyond the 400gb permanent maximum also have the increase count against this limit ( i. e., a 1tb network scratch allocation would have 600gb counted towards this limit ). project storage is fully protected. it has snapshots enabled and is backed up nightly. the maximum allocation length you can request is 540 days ( 500gb space ) and the maximum storage space you can request is 9tb ( 30 day length ). to request an allocation, please contact staff with the faculty member ( s ) that the project is under involved in the conversation. please include the following details : project name ( short ) description size ( 1tb, 2tb, etc. ) length in days ( 270 days, 135 days, etc. ) other user ( s ) that need to access the allocation, if any these allocations are available via / fs / nexus - projects / < project name >. renewal is not guaranteed to be available due to limits on the amount of total storage. near the end of the allocation period, staff will contact you and ask if you are still in need of the storage allocation. if renewal is available, you can renew for up to another 270 tb days with reapproval from the original faculty approver. if you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period. staff will then remove the allocation. if you do not respond to staff \\' s request by the end of the allocation period, staff will make the allocation temporarily inaccessible. if you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible. if one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation. datasets we have read - only dataset storage available at / fs / nexus - datasets. if there are datasets that you would like to see curated and available, please see this page. the list of nexus datasets we currently host can be viewed here. retrieved from \" https : / / wiki. umiac',\n",
       " '. if you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period. staff will then remove the allocation. if you do not respond to staff \\' s request by the end of the allocation period, staff will make the allocation temporarily inaccessible. if you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible. if one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation. datasets we have read - only dataset storage available at / fs / nexus - datasets. if there are datasets that you would like to see curated and available, please see this page. the list of nexus datasets we currently host can be viewed here. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus & oldid = 12024 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 13 september 2024, at 13 : 21. privacy policy about umiacs disclaimers',\n",
       " 'nexus / accounts - umiacs nexus / accounts from umiacs jump to navigation jump to search accounts all accounts in the nexus ( and umiacs accounts in general ) are required to be sponsored. new accounts can be requested via our requests application. if you already have a full umiacs account ( not a collaborator account ) then you already have access to nexus. faculty umiacs and csd faculty members can use the requests application to request new accounts for themselves. when filling out the form as a faculty member, please use the username of the current director of computing facilities at umiacs as the principal investigator. this is currently derek. once you submit the initial form, you will be required to verify the email address you have listed as your external contact. once verified, the director of computing facilities will approve sponsorship of the account and your account will be installed within 24 business hours ( 9am - 5pm m - f ). after receiving an account, faculty can let their students know to request an account listing them as the principal investigator. faculty will receive an email to confirm sponsorship through this same application when a student requests an account. once approved, these sponsored accounts will be installed within 24 business hours ( 9am - 5pm m - f ). students / collaborators when directed by a faculty member, students or collaborators can use the requests application to request new accounts for themselves. when filling out the form as a student or collaborator, please use the umiacs account username of the umiacs or csd faculty member as the principal investigator. all accounts must be sponsored in umiacs. should your faculty sponsor leave umiacs / csd or individually revoke sponsorship of your account, you will be given 30 days to find a new account sponsor. should this 30 days lapse, your account will be archived and removed. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / accounts & oldid = 10653 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 16 september 2022, at 17 : 49. privacy policy about umiacs disclaimers',\n",
       " \"apptainer - umiacs apptainer from umiacs ( redirected from nexus / apptainer ) jump to navigation jump to search apptainer is a container platform that doesn ' t elevate the privileges of a user running the container. this is important as umiacs runs many multi - tenant hosts ( such as nexus ) and doesn ' t provide administrative control to users on them. while docker is popular, the most typical setups require a daemon that has administrative level privileges that makes it not tenable. apptainer was previously branded as singularity. you should still be able to run commands on the system with singularity, however you should start migrating to using the apptainer command. contents 1 overview 2 nexus containers 2. 1 bind mounts 2. 2 shared containers 3 docker workflow example overview you can find out what the current version is that we provide by running the apptainer - - version command. if this instead says apptainer : command not found and you are using a umiacs - supported host, please contact staff and we will ensure that the software is available on the host you are looking for it on. # apptainer - - version apptainer version 1. 2. 5 - 1. el8 apptainer can run a variety of images including its own format and docker images. to create images from definition files, you need to have administrative rights. you will need to either use podman to accomplish this on umiacs - supported hosts, or alternatively do this on a host that you have full administrative access to ( laptop or personal desktop ) rather than a umiacs - supported host. if you are going to pull large images, you may run out of space in your home directory. we suggest you run the following commands to setup alternate cache and tmp directories. we are using / scratch0 but you can substitute any large enough local scratch directory, network scratch directory, or project directory you would like. export workdir = / scratch0 / $ user export apptainer _ cachedir = $ { workdir } /. cache export apptainer _ tmpdir = $ { workdir } /. tmp mkdir - p $ apptainer _ cachedir mkdir - p $ apptainer _ tmpdir we do suggest you pull images down into an intermediate file ( sif file ) as you then do not have to worry about re - caching the image. $ apptainer pull cuda12. 2. 2. sif docker : / / nvidia / cuda : 12. 2. 2 - base - ubi8 info : converting oci blobs to sif format info : starting build... getting image source signatures copying blob d5d706ce7b29 done copying blob b4dc78aeafca done copying blob 24a22c1b7260 done copying blob 8dea37be3176 done copying blob 25fa05cd42bd done copying blob a57130ec8de1 done copying blob 880a66924cf5 done copying config db554d658b done writing manifest to image destination storing signatures 2022 / 10 / 14 10 : 31 : 17 info unpack layer : sha256 : 25fa05cd42bd8fabb25d2a6f3f8c9f7ab34637903d00fd2ed1c1d0fa980427dd 2022 / 10 / 14 10 : 31 : 19 info unpack layer : sha256 : 24a22c1b72605a4dbcec13b743ef60a6cbb43185fe46fd8a35941f9af7c11153 2022 / 10 / 14 10 : 31 : 19 info unpack layer : sha256 : 8dea37be3176a88fae41c265562d5fb438d9281c356dcb4edeaa51451dbdfdb2 2022 / 10 / 14 10 : 31 : 20 info unpack layer : sha256 : b4dc78aeafca6321025300e9d3050c5ba3fb2ac743ae547c6e1efa3f9284ce0b 2022 / 10 / 14 10 : 31 : 20 info unpack layer : sha256 : a57130ec8de1e44163e965620d5aed2abe6cddf48b48272964bfd8bca101df38 2022 / 10 / 14 10 : 31 : 20 info unpack layer : sha25\",\n",
       " '10 : 31 : 19 info unpack layer : sha256 : 8dea37be3176a88fae41c265562d5fb438d9281c356dcb4edeaa51451dbdfdb2 2022 / 10 / 14 10 : 31 : 20 info unpack layer : sha256 : b4dc78aeafca6321025300e9d3050c5ba3fb2ac743ae547c6e1efa3f9284ce0b 2022 / 10 / 14 10 : 31 : 20 info unpack layer : sha256 : a57130ec8de1e44163e965620d5aed2abe6cddf48b48272964bfd8bca101df38 2022 / 10 / 14 10 : 31 : 20 info unpack layer : sha256 : d5d706ce7b293ffb369d3bf0e3f58f959977903b82eb26433fe58645f79b778b 2022 / 10 / 14 10 : 31 : 49 info unpack layer : sha256 : 880a66924cf5e11df601a4f531f3741c6867a3e05238bc9b7cebb2a68d479204 info : creating sif file... $ apptainer inspect cuda12. 2. 2. sif... maintainer : nvidia corporation < sw - cuda - installer @ nvidia. com > name : ubi8 org. label - schema. build - arch : amd64 org. label - schema. build - date : wednesday _ 24 _ january _ 2024 _ 13 : 53 : 0 _ est org. label - schema. schema - version : 1. 0 org. label - schema. usage. apptainer. version : 1. 2. 5 - 1. el8 org. label - schema. usage. singularity. deffile. bootstrap : docker org. label - schema. usage. singularity. deffile. from : nvidia / cuda : 12. 2. 2 - base - ubi8... now you can run the local image with the run command or start a shell with the shell command. please note that if you are in an environment with gpus and you want to access them inside the container you need to specify the - - nv flag. nvidia has a very specific driver and libraries that are required to run cuda programs, so this is to ensure that all appropriate devices are created inside the container and that these libraries are made available in the container. $ apptainer run - - nv cuda12. 2. 2. sif nvidia - smi - l gpu 0 : nvidia geforce gtx 1080 ti ( uuid : gpu - 8e040d17 - 402e - cc86 - 4e83 - eb2b1d501f1e ) gpu 1 : nvidia geforce gtx 1080 ti ( uuid : gpu - d681a21a - 8cdd - e624 - 6bf8 - 5b0234584ba2 ) nexus containers in our nexus environment we have some example containers based on our pytorch _ docker project. these can be found in / fs / nexus - containers / pytorch. you can just run one of the example images by doing the following ( you should have already allocated a interactive job with a gpu in nexus ). it will use the default script found at / srv / tensor. py within the image. $ hostname & & nvidia - smi - l tron38. umiacs. umd. edu gpu 0 : nvidia rtx a4000 ( uuid : gpu - 4a0a5644 - 9fc8 - 84b4 - 5d22 - 65d45ca36506 ) $ apptainer run - - nv / fs / nexus - containers / pytorch / pytorch _ 1. 13. 0 + cu117. sif 99 984. 5538940429688 199 654. 1710815429688 299 435. 662353515625 399 291. 1429138183594 499 195. 5575714111328 599 132. 3363037109375 699 90. 5206069946289 799 62. 86',\n",
       " '##y within the image. $ hostname & & nvidia - smi - l tron38. umiacs. umd. edu gpu 0 : nvidia rtx a4000 ( uuid : gpu - 4a0a5644 - 9fc8 - 84b4 - 5d22 - 65d45ca36506 ) $ apptainer run - - nv / fs / nexus - containers / pytorch / pytorch _ 1. 13. 0 + cu117. sif 99 984. 5538940429688 199 654. 1710815429688 299 435. 662353515625 399 291. 1429138183594 499 195. 5575714111328 599 132. 3363037109375 699 90. 5206069946289 799 62. 86213684082031 899 44. 56754684448242 999 32. 466392517089844 1099 24. 461835861206055 1199 19. 166893005371094 1299 15. 6642427444458 1399 13. 347112655639648 1499 11. 814264297485352 1599 10. 800163269042969 1699 10. 129261016845703 1799 9. 685370445251465 1899 9. 391674041748047 1999 9. 19735336303711 result : y = 0. 0022362577728927135 + 0. 837898313999176 x + - 0. 0003857926349155605 x ^ 2 + - 0. 09065020829439163 x ^ 3 bind mounts to get data into the container you need to pass some bind mounts. apptainer containers will not automatically mount data from the outside operating system other than your home directory. users need to manually bind mounts for other file paths. - - bind / fs / nexus - scratch / < username > / < projectname > : / mnt in this example, we will exec an interactive session with gpus and binding our nexus scratch directory which allows us to specify the command we want to run inside the container. apptainer exec - - nv - - bind / fs / nexus - scratch / username : / fs / nexus - scratch / username / fs / nexus - containers / pytorch / pytorch _ 1. 13. 0 + cu117. sif bash you can now write / run your own pytorch python code interactively within the container or just make a python script that you can call directly from the apptainer exec command for batch processing. shared containers portable images called singularity image format or. sif files can be copied and shared. nexus maintains some shared containers in / fs / nexus - containers. these are arranged by the application ( s ) that are installed. docker workflow example we have a pytorch _ docker example workflow using our gitlab as a docker registry. you can clone the repository and further customize this to your needs. the workflow is : run docker on a laptop or personal desktop on to create the image, or use podman on a umiacs - supported system. tag the image and and push it to your repository ( this can be any docker registry ) pull the image down onto one of our workstations / clusters and run it with your data. $ apptainer pull pytorch _ docker. sif docker : / / registry. umiacs. umd. edu / derek / pytorch _ docker info : converting oci blobs to sif format info : starting build... getting image source signatures copying blob 85386706b020 done... 2022 / 10 / 14 10 : 58 : 36 info unpack layer : sha256 : b6f46848806c8750a68edc4463bf146ed6c3c4af18f5d3f23281dcdfb1c65055 2022 / 10 / 14 10 : 58 : 43 info unpack layer : sha256 : 44845dc671f759820baac0376198141ca683f554bb16a177a3cfe262c9e368ff info : creating sif file... $ apptainer exec - - nv',\n",
       " '/ derek / pytorch _ docker info : converting oci blobs to sif format info : starting build... getting image source signatures copying blob 85386706b020 done... 2022 / 10 / 14 10 : 58 : 36 info unpack layer : sha256 : b6f46848806c8750a68edc4463bf146ed6c3c4af18f5d3f23281dcdfb1c65055 2022 / 10 / 14 10 : 58 : 43 info unpack layer : sha256 : 44845dc671f759820baac0376198141ca683f554bb16a177a3cfe262c9e368ff info : creating sif file... $ apptainer exec - - nv pytorch _ docker. sif python3 - c \\' from _ _ future _ _ import print _ function ; import torch ; print ( torch. cuda. current _ device ( ) ) ; x = torch. rand ( 5, 3 ) ; print ( x ) \\' 0 tensor ( [ [ 0. 3273, 0. 7174, 0. 3587 ], [ 0. 2250, 0. 3896, 0. 4136 ], [ 0. 3626, 0. 0383, 0. 6274 ], [ 0. 6241, 0. 8079, 0. 2950 ], [ 0. 0804, 0. 9705, 0. 0030 ] ] ) retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = apptainer & oldid = 11997 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 13 august 2024, at 17 : 51. privacy policy about umiacs disclaimers',\n",
       " \"nexus / cbcb - umiacs nexus / cbcb from umiacs jump to navigation jump to search the compute nodes from cbcb ' s previous standalone cluster have folded into nexus as of mid 2023. the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 submission nodes 2 nodes 3 partitions 4 qos 5 jobs 6 storage 7 migration 7. 1 operating system / software submission nodes you can ssh to nexuscbcb. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexuscbcb00. umiacs. umd. edu nexuscbcb01. umiacs. umd. edu nodes all nodes in cbcb - owned partitions ( see below section ) owned by cbcb faculty are named in the format cbcb # #. the sets of nodes are : 22 nodes that were purchased in october 2022 with center - wide funding. they are cbcb [ 00 - 21 ]. 4 nodes from the previous standalone cbcb cluster that moved in as of summer 2023. they are cbcb [ 22 - 25 ]. a few additional nodes purchased by dr. heng huang since then. they are all remaining ' cbcb ' named nodes. nodenames quantity cpu cores per node ( cpus ) memory per node ( type ) local storage per node ( type / location ) gpus per node ( type ) cbcb [ 00 - 21 ] 22 32 ( dual amd epyc 7313 ) ~ 2tb ( ddr4 3200mhz ) ~ 350gb ( sata ssd / scratch0 ), ~ 2tb ( nvme ssd / scratch1 ) 0 cbcb22 1 28 ( dual intel xeon e5 - 2680 v4 ) ~ 768gb ( ddr4 2400mhz ) ~ 650gb ( sata ssd / scratch0 ) 0 cbcb [ 23 - 24 ] 2 24 ( dual intel xeon e5 - 2650 v4 ) ~ 256gb ( ddr4 2400mhz ) ~ 800gb ( sata ssd / scratch0 ) 0 cbcb25 1 24 ( dual intel xeon e5 - 2650 v4 ) ~ 256gb ( ddr4 2400mhz ) ~ 1. 4tb ( sata ssd / scratch0 ) 2 ( 1x nvidia geforce gtx 1080 ti, 1x nvidia geforce rtx 2080 ti ) cbcb26 1 128 ( dual amd epyc 7763 ) ~ 512gb ( ddr4 3200mhz ) ~ 3. 4tb ( nvme ssd / scratch0 ), ~ 14tb ( nvme ssd / scratch1 ) 8 ( nvidia rtx a5000 ) cbcb27 1 64 ( dual amd epyc 7513 ) ~ 256gb ( ddr4 3200mhz ) ~ 3. 4tb ( sata ssd / scratch0 ), ~ 3. 5tb ( nvme ssd / scratch1 ) 8 ( nvidia rtx a6000 ) cbcb [ 28 - 29 ] 2 32 ( dual amd epyc 9124 ) ~ 768gb ( ddr5 4800mhz ) ~ 350gb ( sata ssd / scratch0 ), ~ 7tb ( nvme ssd / scratch1 ) 8 ( nvidia rtx 6000 ada generation ) total 30 1060 ( various ) ~ 49tb ( various ) ~ 94tb ( various ) 34 ( various ) here is the listing of nodes as shown by the slurm alias show _ nodes ( again, all nodes are named in the format cbcb # # ) : [ root @ nexusctl00 ~ ] # show _ nodes | grep cbcb nodelist cpus memory avail _ features gres state cbcb00 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb01 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb02 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb03 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb04 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb05 32 206\",\n",
       " \"( various ) ~ 94tb ( various ) 34 ( various ) here is the listing of nodes as shown by the slurm alias show _ nodes ( again, all nodes are named in the format cbcb # # ) : [ root @ nexusctl00 ~ ] # show _ nodes | grep cbcb nodelist cpus memory avail _ features gres state cbcb00 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb01 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb02 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb03 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb04 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb05 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb06 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb07 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb08 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb09 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb10 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb11 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb12 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb13 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb14 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb15 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb16 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb17 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb18 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb19 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb20 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb21 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb22 28 771245 rhel8, xeon, e5 - 2680 ( null ) idle cbcb23 24 255150 rhel8, xeon, e5 - 2650 ( null ) idle cbcb24 24 255150 rhel8, xeon, e5 - 2650 ( null ) idle cbcb25 24 255278 rhel8, xeon, e5 - 2650, pascal, turi gpu : rtx2080ti : 1, gpu : gtx1080ti : 1 idle cbcb26 128 513243 rhel8, zen, epyc - 7763, ampere gpu : rtxa5000 : 8 idle cbcb27 64 255167 rhel8, zen, epyc - 7513, ampere gpu : rtxa6000 : 8 idle cbcb28 32 771166 rhel8, zen, epyc - 9124, ada gpu : rtx6000ada : 8 idle cbcb29 32 771166 rhel8, zen, epyc - 9124, ada gpu : rtx6000ada : 8 idle partitions there are two partitions available to general cbcb slurm users. you must specify one of these two partitions when submitting your job. cbcb - this is the default partition. job allocations on all nodes except those also in the cbcb - heng partition are guaranteed. cbcb - interactive - this is a partition that only allows interactive jobs ; you cannot submit jobs via sbatch to this partition. job allocations are guaranteed. there is one additional partition available solely to dr. heng huang ' s sponsored accounts. cbcb - heng - this partition is for exclusive priority access to dr. huang ' s purchased gpu nodes. job allocations are guaranteed. qos cbcb users have access to all of the standard job qoses in the cbcb and cbcb - heng partitions using the cbcb account. the additional job qoses for the cbcb and cbcb - heng\",\n",
       " \"##u : rtx6000ada : 8 idle partitions there are two partitions available to general cbcb slurm users. you must specify one of these two partitions when submitting your job. cbcb - this is the default partition. job allocations on all nodes except those also in the cbcb - heng partition are guaranteed. cbcb - interactive - this is a partition that only allows interactive jobs ; you cannot submit jobs via sbatch to this partition. job allocations are guaranteed. there is one additional partition available solely to dr. heng huang ' s sponsored accounts. cbcb - heng - this partition is for exclusive priority access to dr. huang ' s purchased gpu nodes. job allocations are guaranteed. qos cbcb users have access to all of the standard job qoses in the cbcb and cbcb - heng partitions using the cbcb account. the additional job qoses for the cbcb and cbcb - heng partitions specifically are : highmem : allows for significantly increased memory to be allocated. huge - long : allows for longer jobs using higher overall resources. please note that the partition has a grptres limit of 100 % of the available cores / ram on the partition - specific nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. the only allowed job qos for the cbcb - interactive partition is : interactive : allows for 4 cpu / 128g mem jobs up to 12 hours in length - can only be used via srun or salloc. jobs you will need to specify - - partition = cbcb and - - account = cbcb to be able to submit jobs to the cbcb partition. [ username @ nexuscbcb00 : ~ ] $ srun - - pty - - ntasks = 16 - - mem = 2000g - - qos = highmem - - partition = cbcb - - account = cbcb - - time 1 - 00 : 00 : 00 bash srun : job 218874 queued and waiting for resources srun : job 218874 has been allocated resources [ username @ cbcb00 : ~ ] $ scontrol show job 218874 jobid = 218874 jobname = bash userid = username ( 1000 ) groupid = username ( 21000 ) mcs _ label = n / a priority = 897 nice = 0 account = cbcb qos = highmem jobstate = running reason = none dependency = ( null ) requeue = 1 restarts = 0 batchflag = 0 reboot = 0 exitcode = 0 : 0 runtime = 00 : 00 : 06 timelimit = 1 - 00 : 00 : 00 timemin = n / a submittime = 2022 - 11 - 18t11 : 13 : 56 eligibletime = 2022 - 11 - 18t11 : 13 : 56 accruetime = 2022 - 11 - 18t11 : 13 : 56 starttime = 2022 - 11 - 18t11 : 13 : 56 endtime = 2022 - 11 - 19t11 : 13 : 56 deadline = n / a preempteligibletime = 2022 - 11 - 18t11 : 13 : 56 preempttime = none suspendtime = none secspresuspend = 0 lastschedeval = 2022 - 11 - 18t11 : 13 : 56 scheduler = main partition = cbcb allocnode : sid = nexuscbcb00 : 25443 reqnodelist = ( null ) excnodelist = ( null ) nodelist = cbcb00 batchhost = cbcb00 numnodes = 1 numcpus = 16 numtasks = 16 cpus / task = 1 reqb : s : c : t = 0 : 0 : * : * tres = cpu = 16, mem = 2000g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 2000g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage cbcb still has its current storage allocation in place. all data filesystems that were available in the standalone cbcb cluster are also available in nexus. please note about the change in your home directory in the migration section below. cbcb users can also request nexus project allocations. migration operating system / software cbcb ' s standalone cluster submission and compute nodes were running rhel\",\n",
       " '* : * tres = cpu = 16, mem = 2000g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 2000g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage cbcb still has its current storage allocation in place. all data filesystems that were available in the standalone cbcb cluster are also available in nexus. please note about the change in your home directory in the migration section below. cbcb users can also request nexus project allocations. migration operating system / software cbcb \\' s standalone cluster submission and compute nodes were running rhel7. nexus is exclusively running rhel8, so any software you may have compiled may need to be re - compiled to work correctly in this new environment. the cbcb module tree for rhel8 may not yet be fully populated with rhel8 software. if you do not see the modules you need, please reach out to the cbcb software maintainers. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / cbcb & oldid = 12056 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 26 september 2024, at 13 : 26. privacy policy about umiacs disclaimers',\n",
       " 'nexus / clip - umiacs nexus / clip from umiacs jump to navigation jump to search the previous standalone cluster for clip \\' s compute nodes have folded into nexus as of late 2022. the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 submission nodes 2 resources 3 qos 4 jobs 5 storage submission nodes you can ssh to nexusclip. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexusclip00. umiacs. umd. edu nexusclip01. umiacs. umd. edu resources the clip partition has nodes brought over from the previous standalone clip slurm scheduler as well as some more recent purchases. the compute nodes are named clip # #. qos clip users have access to all of the standard job qoses in the clip partition using the clip account. the additional job qoses for the clip partition specifically are : huge - long : allows for longer jobs using higher overall resources. please note that the partition has a grptres limit of 100 % of the available cores / ram on the partition - specific nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. jobs you will need to specify - - partition = clip and - - account = clip to be able to submit jobs to the clip partition. [ username @ nexusclip00 : ~ ] $ srun - - pty - - ntasks = 4 - - mem = 8g - - qos = default - - partition = clip - - account = clip - - time 1 - 00 : 00 : 00 bash srun : job 218874 queued and waiting for resources srun : job 218874 has been allocated resources [ username @ clip00 : ~ ] $ scontrol show job 218874 jobid = 218874 jobname = bash userid = username ( 1000 ) groupid = username ( 21000 ) mcs _ label = n / a priority = 897 nice = 0 account = clip qos = default jobstate = running reason = none dependency = ( null ) requeue = 1 restarts = 0 batchflag = 0 reboot = 0 exitcode = 0 : 0 runtime = 00 : 00 : 06 timelimit = 1 - 00 : 00 : 00 timemin = n / a submittime = 2022 - 11 - 18t11 : 13 : 56 eligibletime = 2022 - 11 - 18t11 : 13 : 56 accruetime = 2022 - 11 - 18t11 : 13 : 56 starttime = 2022 - 11 - 18t11 : 13 : 56 endtime = 2022 - 11 - 19t11 : 13 : 56 deadline = n / a preempteligibletime = 2022 - 11 - 18t11 : 13 : 56 preempttime = none suspendtime = none secspresuspend = 0 lastschedeval = 2022 - 11 - 18t11 : 13 : 56 scheduler = main partition = clip allocnode : sid = nexusclip00 : 25443 reqnodelist = ( null ) excnodelist = ( null ) nodelist = clip00 batchhost = clip00 numnodes = 1 numcpus = 4 numtasks = 4 cpus / task = 1 reqb : s : c : t = 0 : 0 : * : * tres = cpu = 4, mem = 8g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 8g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage all data filesystems that were available in the standalone clip cluster are also available in nexus. clip users can also request nexus project allocations. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / clip & oldid = 12034 \" navigation menu personal tools log in',\n",
       " ': * : * tres = cpu = 4, mem = 8g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 8g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage all data filesystems that were available in the standalone clip cluster are also available in nexus. clip users can also request nexus project allocations. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / clip & oldid = 12034 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 17 september 2024, at 16 : 35. privacy policy about umiacs disclaimers',\n",
       " \"nexus / cml - umiacs nexus / cml from umiacs jump to navigation jump to search the compute nodes from cml ' s previous standalone cluster have folded into nexus as of the scheduled maintenance window for august 2023 ( thursday 08 / 17 / 2023, 5 - 8pm ). the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 usage 2 partitions 3 accounts 4 qos 5 storage 5. 1 home directories 5. 2 project directories 5. 3 scratch directories 5. 3. 1 network scratch directory 5. 3. 2 local scratch directories 5. 4 datasets 5. 5 models usage you can ssh to nexuscml. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexuscml00. umiacs. umd. edu nexuscml01. umiacs. umd. edu all partitions, qoses, and account names from the standalone cml cluster have been moved over to nexus. however, please note that cml - is prepended to all of the values that were present in the standalone cml cluster to distinguish them from existing values in nexus. the lone exception is the base account that was named cml in the standalone cluster ( it is also named just cml in nexus ). here are some before / after examples of job submission with various parameters : standalone cml cluster submission command nexus cluster submission command srun - - partition = dpart - - qos = medium - - account = tomg - - gres = gpu : rtx2080ti : 2 - - pty bash srun - - partition = cml - dpart - - qos = cml - medium - - account = cml - tomg - - gres = gpu : rtx2080ti : 2 - - pty bash srun - - partition = cpu - - qos = cpu - - pty bash srun - - partition = cml - cpu - - qos = cml - cpu - - account = cml - - pty bash srun - - partition = scavenger - - qos = scavenger - - account = scavenger - - gres = gpu : 4 - - pty bash srun - - partition = cml - scavenger - - qos = cml - scavenger - - account = cml - scavenger - - gres = gpu : 4 - - pty bash cml users ( exclusively ) can schedule non - interruptible jobs on cml nodes with any non - scavenger job parameters. please note that the cml - dpart partition has a grptres limit of 100 % of the available cores / ram on all cml # # nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. it also has a max submission limit of 500 jobs per user simultaneously so as to not overload the cluster. this is codified by the partition qos named cml. please note that the cml compute nodes are also in the institute - wide scavenger partition in nexus. cml users still have scavenging priority over these nodes via the cml - scavenger partition ( i. e., all cml - partition jobs ( other than cml - scavenger ) can preempt both cml - scavenger and scavenger partition jobs, and cml - scavenger partition jobs can preempt scavenger partition jobs ). partitions there are three partitions available to general cml slurm users. you must specify a partition when submitting your job. cml - dpart - this is the default partition. job allocations are guaranteed. cml - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs in other cml - partitions are ready to be scheduled. cml - cpu - this partition is for cpu focused jobs. job allocations are guaranteed. there are two additional partitions available solely to specific faculty members and their sponsored accounts. cml - furongh - this partition is for exclusive priority access to dr. furong huang ' s purchased a6000 node. job allocations are guaranteed. cml - zhou - this partition is for exclusive priority access to dr. tianyi zhou\",\n",
       " \"partition jobs, and cml - scavenger partition jobs can preempt scavenger partition jobs ). partitions there are three partitions available to general cml slurm users. you must specify a partition when submitting your job. cml - dpart - this is the default partition. job allocations are guaranteed. cml - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs in other cml - partitions are ready to be scheduled. cml - cpu - this partition is for cpu focused jobs. job allocations are guaranteed. there are two additional partitions available solely to specific faculty members and their sponsored accounts. cml - furongh - this partition is for exclusive priority access to dr. furong huang ' s purchased a6000 node. job allocations are guaranteed. cml - zhou - this partition is for exclusive priority access to dr. tianyi zhou ' s purchased nodes. job allocations are guaranteed. accounts the center has a base slurm account cml which has a modest number of guaranteed billing resources available to all cluster users at any given time. other faculty that have invested in the cluster have an additional account provided to their sponsored accounts on the cluster, which provides a number of guaranteed billing resources corresponding to the amount that they invested. if you do not specify an account when submitting your job, you will receive the cml account, which only has access to the cml - cpu, cml - default, and cml - medium qoses ( see below section ). if you need access to a different qos, or if the cml account is at its billing limit ( see below in this section ), please use your faculty sponsor ' s account if they have one available. however, keep in mind that if you use your faculty sponsor has their own named partition ( see previous section ), using the faculty - specific account in the cml - dpart partition may block access to resources in the faculty - specific partition, since the billing limit for the account is charged regardless of what partition is being used. the current faculty accounts are : cml - abhinav cml - cameron cml - furongh cml - hajiagha cml - john cml - ramani cml - sfeizi cml - tokekar cml - tomg cml - zhou $ sacctmgr show account format = account % 20, description % 30, organization % 10 account descr org - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -......... cml cml cml cml - abhinav cml - abhinav shrivastava cml cml - cameron cml - maria cameron cml cml - furongh cml - furong huang cml cml - hajiagha cml - mohammad hajiaghayi cml cml - john cml - john dickerson cml cml - ramani cml - ramani duraiswami cml cml - scavenger cml - scavenger cml cml - sfeizi cml - soheil feizi cml cml - tokekar cml - pratap tokekar cml cml - tomg cml - tom goldstein cml cml - zhou cml - tianyi zhou cml......... faculty can manage this list of users via our directory application in the security groups section. the security group that controls access has the prefix cml _ and then the faculty username. it will also list slurm : / / nexusctl. umiacs. umd. edu as the associated uri. you can check your account associations by running the show _ assoc command to see the accounts you are associated with. please contact staff and include your faculty member in the conversation if you do not see the appropriate association. $ show _ assoc user account maxjobs grptres qos - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -......... tomg cml cml - cpu, cml - default, cml - medium tomg cml - scavenger cml - scavenger tom\",\n",
       " \"associated uri. you can check your account associations by running the show _ assoc command to see the accounts you are associated with. please contact staff and include your faculty member in the conversation if you do not see the appropriate association. $ show _ assoc user account maxjobs grptres qos - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -......... tomg cml cml - cpu, cml - default, cml - medium tomg cml - scavenger cml - scavenger tomg cml - tomg cml - default, cml - high, cml - medium......... you can also see the total number of track - able resources ( tres ) allowed for each account by running the following command. please make sure you give the appropriate account that you are looking for. the billing number displayed here is the sum of resource weightings for all nodes appropriated to that account. $ sacctmgr show assoc account = cml format = user, account, qos, grptres user account qos grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - cml billing = 6481...... qos cml currently has 5 qos for the cml - dpart partition ( though cml - high _ long and cml - very _ high may not be available to all faculty accounts ), 1 qos for the cml - scavenger partition, and 1 qos for the cml - cpu partition. if you do not specify a qos when submitting your job using the - - qos parameter, you will receive the cml - default qos assuming you are using a cml account. if your faculty member ' s slurm account does not have one or both of the cml - high _ long or cml - very _ high qos available to it, we can add it to their account provided they approve. please contact staff if this is desired. the important part here is that in different qos you can have a shorter / longer maximum wall time, a different total number of jobs running at once, and a different maximum number of track - able resources ( tres ) for the job. in the cml - scavenger qos, one more constraint that you are restricted by is the total number of tres per user ( over multiple jobs ). $ show _ qos - - all | grep cml name maxwall maxtres maxjobspu maxtrespu - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -... cml - cpu 7 - 00 : 00 : 00 8 cml - default 7 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g 2 cml - high 1 - 12 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g 2 cml - high _ long 14 - 00 : 00 : 00 cpu = 32, gres / gpu = 8 8 gres / gpu = 8 cml - medium 3 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g 2 cml - scavenger 3 - 00 : 00 : 00 gres / gpu = 24 cml - very _ high 1 - 12 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g 8 gres / gpu = 12... $ show _ partition _ qos - - all | grep cml name maxsubmitpu maxtrespu grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\",\n",
       " \"= 128g 2 cml - high _ long 14 - 00 : 00 : 00 cpu = 32, gres / gpu = 8 8 gres / gpu = 8 cml - medium 3 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g 2 cml - scavenger 3 - 00 : 00 : 00 gres / gpu = 24 cml - very _ high 1 - 12 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g 8 gres / gpu = 12... $ show _ partition _ qos - - all | grep cml name maxsubmitpu maxtrespu grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -... cml 500 cpu = 1128, mem = 11t cml - cpu 500 cml - furongh 500 cml - scavenger 500 gres / gpu = 24 cml - wriva 500 cml - zhou 500... storage there are 3 types of user storage available to users in the cml : home directories project directories scratch directories there are also 2 types of read - only storage available for common use among users in the cml : dataset directories model directories cml users can also request nexus project allocations. home directories you have 30gb of home directory storage available at / nfshomes / < username >. it has both snapshots and backups enabled. home directories are intended to store personal or configuration files only. we encourage you to not share any data in your home directory. you are encouraged to utilize our gitlab infrastructure to host your code repositories. note : to check your quota on this directory, use the command df - h ~. project directories you can request project based allocations for up to 6tb for up to 120 days with one or more approvals : allocations up to and including 3tb require approval from a cml faculty member allocations above 3tb ( up to 6tb ) require approval from both a cml faculty member and the director of cml to request an allocation, please contact staff with the faculty member ( s ) that the project is under involved in the conversation. please include the following details : project name ( short ) description size ( 1tb, 2tb, etc. ) length in days ( 30 days, 90 days, etc. ) other user ( s ) that need to access the allocation, if any these allocations will be available from / fs / cml - projects under a name that you provide when you request the allocation. near the end of the allocation period, staff will contact you and ask if you would like to renew the allocation for up to another 120 days ( requires re - approval from a cml faculty member and the director of cml ). if you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period. staff will then remove the allocation. if you do not respond to staff ' s request by the end of the allocation period, staff will make the allocation temporarily inaccessible. if you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible. if one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation. this data is backed up nightly. scratch directories scratch data has no data protection including no snapshots and the data is not backed up. there are two types of scratch directories in the cml compute infrastructure : network scratch directory local scratch directories network scratch directory you have 200gb of scratch storage available at / cmlscratch / < username >. it is not backed up or protected in any way. this directory is automounted so you will need to cd into the directory or request / specify a fully qualified file path to access this. you may request a permanent increase of up to 800gb total space without any faculty approval by contacting staff. if you need space beyond 800gb, you will need faculty approval and / or a project directory. space increases beyond 800gb also have a maximum request period of 120 days ( as with project directories ), after which they will need to be renewed with re - approval from a cml faculty member and the director of cml. as with project directories, allocations over 3tb total space require approval from the\",\n",
       " '##hots and the data is not backed up. there are two types of scratch directories in the cml compute infrastructure : network scratch directory local scratch directories network scratch directory you have 200gb of scratch storage available at / cmlscratch / < username >. it is not backed up or protected in any way. this directory is automounted so you will need to cd into the directory or request / specify a fully qualified file path to access this. you may request a permanent increase of up to 800gb total space without any faculty approval by contacting staff. if you need space beyond 800gb, you will need faculty approval and / or a project directory. space increases beyond 800gb also have a maximum request period of 120 days ( as with project directories ), after which they will need to be renewed with re - approval from a cml faculty member and the director of cml. as with project directories, allocations over 3tb total space require approval from the director of cml in addition to your faculty member. this file system is available on all submission, data management, and computational nodes within the cluster. local scratch directories each computational node that you can schedule compute jobs on has one or more local scratch directories. these are always named / scratch0, / scratch1, etc. these are almost always more performant than any other storage available to the job. however, you must stage data to these directories within the confines of your jobs and stage the data out before the end of your jobs. these local scratch directories have a tmpwatch job which will delete unaccessed data after 90 days, scheduled via maintenance jobs to run once a month during our monthly maintenance windows. again, please make sure you secure any data you write to these directories at the end of your job. datasets we have read - only dataset storage available at / fs / cml - datasets. if there are datasets that you would like to see curated and available, please see this page. the list of cml datasets we currently host can be viewed here. models we have read - only model storage available at / fs / cml - models. if there are models that you would like to see downloaded and made available, please see this page. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / cml & oldid = 12046 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 18 september 2024, at 19 : 28. privacy policy about umiacs disclaimers',\n",
       " 'nexus / gamma - umiacs nexus / gamma from umiacs jump to navigation jump to search the gamma lab has a partition of gpu nodes available in the nexus. only gamma lab members are able to run non - interruptible jobs on these nodes. contents 1 access 2 quality of service 3 hardware 4 example 5 storage 5. 1 home directories 5. 2 project directories 5. 3 scratch directories 5. 3. 1 network scratch directory 5. 3. 2 local scratch directories 5. 4 datasets access you can always find out what hosts you have access to submit via the nexus # access page. the gamma lab in particular has a special submission host that has additional local storage available. nexusgamma00. umiacs. umd. edu please do not run anything on the login node. always allocate yourself machines on the compute nodes ( see instructions below ) to run any job. quality of service gamma users have access to all of the standard job qoses in the gamma partition using the gamma account. the additional job qoses for the gamma partition specifically are : huge - long : allows for longer jobs using higher overall resources. please note that the partition has a grptres limit of 100 % of the available cores / ram on the partition - specific nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. hardware nodenames type quantity cpu cores per node memory per node gpus per node gammagpu [ 00 - 04, 06 - 09 ] a5000 gpu node 9 32 256gb 8 gammagpu05 a4000 gpu node 1 32 256gb 8 total 10 320 2560gb 80 example from nexusgamma00. umiacs. umd. edu you can run the following example to submit an interactive job. please note that you need to specify the - - partition and - - account. please refer to our slurm documentation about about how to further customize your submissions including making a batch submission. the following command will allocate 8 gpus for 2 days in an interactive session. change parameters accordingly to your needs. we discourage use of srun and promote use of sbatch for fair use of gpus. $ srun - - pty - - gres = gpu : 8 - - account = gamma - - partition = gamma - - qos = huge - long bash $ hostname gammagpu01. umiacs. umd. edu $ nvidia - smi - l gpu 0 : nvidia rtx a5000 ( uuid : gpu - cdfb2e0c - d69f - 354b - 02f4 - 15161dc7fa66 ) gpu 1 : nvidia rtx a5000 ( uuid : gpu - be53e7a1 - b8fd - 7089 - 3cac - 7a2fbf4ec7dd ) gpu 2 : nvidia rtx a5000 ( uuid : gpu - 774efbb1 - d7ec - a0bb - e992 - da9d1fa6b193 ) gpu 3 : nvidia rtx a5000 ( uuid : gpu - d1692181 - c7de - e273 - 5f95 - 53ad381614c3 ) gpu 4 : nvidia rtx a5000 ( uuid : gpu - ba51fd6c - 37bf - 1b95 - 5f68 - 987c18a6292a ) gpu 5 : nvidia rtx a5000 ( uuid : gpu - c1224a2a - 4a3b - ff16 - 0308 - 4f36205b9859 ) gpu 6 : nvidia rtx a5000 ( uuid : gpu - 8d20d6cd - abf5 - 2630 - ab88 - 6bba438c55fe ) gpu 7 : nvidia rtx a5000 ( uuid : gpu - 93170910 - 5d94 - 6da5 - 8a24 - f561d7da1e2d ) you can also use sbatch to submit your job. here are two examples on how to do that. $ sbatch - - pty - - gres = gpu : 8 - - account = gamma - - partition = gamma - - qos = huge - long - - time = 1 - 23 : 00 : 00 script. sh or $ sbatch script. sh / / script. sh / / #! / bin / bash # sbatch - - gres = gpu : 8 #',\n",
       " \"gpu 6 : nvidia rtx a5000 ( uuid : gpu - 8d20d6cd - abf5 - 2630 - ab88 - 6bba438c55fe ) gpu 7 : nvidia rtx a5000 ( uuid : gpu - 93170910 - 5d94 - 6da5 - 8a24 - f561d7da1e2d ) you can also use sbatch to submit your job. here are two examples on how to do that. $ sbatch - - pty - - gres = gpu : 8 - - account = gamma - - partition = gamma - - qos = huge - long - - time = 1 - 23 : 00 : 00 script. sh or $ sbatch script. sh / / script. sh / / #! / bin / bash # sbatch - - gres = gpu : 8 # sbatch - - account = gamma # sbatch - - partition = gamma # sbatch - - qos = huge - long # sbatch - - time = 1 - 23 : 00 : 00 python your _ file. py storage there are 3 types of user storage available to users in gamma : home directories project directories scratch directories there is also read - only storage available for dataset directories. gamma users can also request nexus project allocations. home directories you have 30gb of home directory storage available at / nfshomes / < username >. it has both snapshots and backups enabled. home directories are intended to store personal or configuration files only. we encourage you to not share any data in your home directory. you are encouraged to utilize our gitlab infrastructure to host your code repositories. note : to check your quota on this directory, use the command df - h ~. project directories you can request project based allocations for up to 8tb and up to 180 days with approval from a gamma faculty member. to request an allocation, please contact staff with the faculty member ( s ) that approved the project in the conversation. please include the following details : project name ( short ) description size ( 1tb, 2tb, etc. ) length in days ( 30 days, 90 days, etc. ) other user ( s ) that need to access the allocation, if any these allocations will be available from / fs / gamma - projects under a name that you provide when you request the allocation. near the end of the allocation period, staff will contact you and ask if you would like to renew the allocation ( requires re - approval from a gamma faculty member ). if you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period. staff will then remove the allocation. if you do not respond to staff ' s request by the end of the allocation period, staff will make the allocation temporarily inaccessible. if you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible. if one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation. this data is backed up nightly. scratch directories scratch data has no data protection, there are no snapshots and the data is not backed up. there are two types of scratch directories : network scratch directory local scratch directories network scratch directory you are allocated 100gb of scratch space via nfs from / gammascratch / $ username. it is not backed up or protected in any way. this directory is automounted so you may not see your directory if you run ls / gammascratch but it will be mounted when you cd into your / gammascratch directory. you may request a permanent increase of up to 200gb total space without any faculty approval by contacting staff. if you need space beyond 200gb, you will need faculty approval. this file system is available on all submission, data management, and computational nodes within the cluster. local scratch directories these file systems are not available over nfs and there are no backups or snapshots available for these file systems. each computational node that you can schedule compute jobs on has one or more local scratch directories. these are always named / scratch0, / scratch1, etc. these directories are local to each node, ie. the / scratch0 on two different nodes are completely separate. these directories are almost always more performant than any other storage available to the job. however, you must stage data to these directories within the confines of your jobs and stage the data out before the end of your jobs. these local scratch directories have a tmpwatch job which will delete unaccessed data after 90 days, scheduled via maintenance jobs\",\n",
       " 'without any faculty approval by contacting staff. if you need space beyond 200gb, you will need faculty approval. this file system is available on all submission, data management, and computational nodes within the cluster. local scratch directories these file systems are not available over nfs and there are no backups or snapshots available for these file systems. each computational node that you can schedule compute jobs on has one or more local scratch directories. these are always named / scratch0, / scratch1, etc. these directories are local to each node, ie. the / scratch0 on two different nodes are completely separate. these directories are almost always more performant than any other storage available to the job. however, you must stage data to these directories within the confines of your jobs and stage the data out before the end of your jobs. these local scratch directories have a tmpwatch job which will delete unaccessed data after 90 days, scheduled via maintenance jobs to run once a month during our monthly maintenance windows. again, please make sure you secure any data you write to these directories at the end of your job. gamma has invested in a 20tb nvme scratch file system on nexusgamma00. umiacs. umd. edu that is available as / scratch1. to utilize this space, you will need to copy data from / to this over ssh from a compute node. to make this easier, you may want to setup ssh keys that will allow you to copy data without prompting for passwords. the / scratch1 directory on nexusgamma00. umiacs. umd. edu doesn \\' t have a tmpwatch. the files in this directory need to be manually removed once they are no longer needed. datasets we have read - only dataset storage available at / fs / gamma - datasets. if there are datasets that you would like to see curated and available, please see this page. the list of gamma datasets we currently host can be viewed here. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / gamma & oldid = 11906 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 25 june 2024, at 18 : 25. privacy policy about umiacs disclaimers',\n",
       " 'nexus / gpus - umiacs nexus / gpus from umiacs jump to navigation jump to search there are several different types of nvidia gpus in the nexus cluster that are available to be scheduled. they are listed below in order of newest to oldest architecture, and then alphabetically. we do not list the exact quantities of gpu here since they change frequently due to additions to or removals from the cluster or during compute node troubleshooting. to see which compute nodes have which gpus and in what quantities, use the show _ nodes command on a submission or compute node. the quantities are listed under the gres column. name gres string ( slurm ) architecture cuda cores gpu memory memory bandwidth fp32 performance ( tflops ) tf32 performance ( dense / sparse tops ) h100 nvlink [ 0 ] h100 - nvl hopper 33792 188gb hbm3 7. 87tb / s 134 not officially published / 1671 h100 sxm h100 - sxm hopper 16896 80gb hbm3 3. 35tb / s 67 not officially published / 989 rtx 6000 ada generation rtx6000ada ada lovelace 18176 48gb gddr6 960gb / s 91. 1 182. 1 / 364. 2 a100 pcie 80gb a100 ampere 6912 80gb hbm2e 1. 94tb / s 19. 5 156 / 312 a100 sxm 80gb a100 ampere 6912 80gb hbm2e 2. 04tb / s 19. 5 156 / 312 geforce rtx 3070 rtx3070 ampere 5888 8gb gddr6 448gb / s 20. 3 20. 3 / 40. 6 geforce rtx 3090 rtx3090 ampere 10496 24gb gddr6x 936gb / s 35. 6 35. 6 / 71 rtx a4000 rtxa4000 ampere 6144 16gb gddr6 448gb / s 19. 2 not officially published rtx a5000 rtxa5000 ampere 8192 24gb gddr6 768gb / s 27. 8 not officially published rtx a6000 rtxa6000 ampere 10752 48gb gddr6 768gb / s 38. 7 77. 4 / 154. 8 geforce rtx 2080 ti rtx2080ti turing 4352 11gb gddr5x 616gb / s 13. 4 n / a geforce gtx 1080 ti gtx1080ti pascal 3584 11gb gddr5x 484gb / s 11. 3 n / a quadro p6000 p6000 pascal 3840 24gb gddr5x 432gb / s 12. 6 n / a tesla p100 p100 pascal 3584 16gb cowos hbm2 732gb / s 9. 3 n / a titan x ( pascal ) titanxpascal pascal 3584 12gb gddr5x 480gb / s 11. 0 n / a titan xp titanxp pascal 3840 12gb gddr5x 548gb / s 12. 1 n / a geforce gtx titan x gtxtitanx maxwell 3072 12gb gddr5 336gb / s 6. 7 n / a [ 0 ] - this gpu is actually a pair of two physical cards connected over nvlink bridges. nvidia \\' s provided specifications for this gpu type are for one physical card ; to get these specs, we have hence doubled their advertised values. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / gpus & oldid = 12042 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 17 september 2024, at 17 : 28. privacy policy about umiacs disclaimers',\n",
       " 'more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 17 september 2024, at 17 : 28. privacy policy about umiacs disclaimers',\n",
       " \"nexus / mbrc - umiacs nexus / mbrc from umiacs jump to navigation jump to search the compute nodes from mbrc ' s previous standalone cluster have folded into nexus as of mid 2023. the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 submission nodes 2 resources 3 qos 4 jobs 5 storage 5. 1 project directories submission nodes you can ssh to nexusmbrc. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexusmbrc00. umiacs. umd. edu nexusmbrc01. umiacs. umd. edu resources the mbrc partition has nodes brought over from the previous standalone mbrc slurm scheduler. the compute nodes are named mbrc # #. qos mbrc users have access to all of the standard job qoses in the mbrc partition using the mbrc account. the additional job qoses for the mbrc partition specifically are : huge - long : allows for longer jobs using higher overall resources. please note that the partition has a grptres limit of 100 % of the available cores / ram on the partition - specific nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. jobs you will need to specify - - partition = mbrc and - - account = mbrc to be able to submit jobs to the mbrc partition. [ username @ nexusmbrc00 : ~ ] $ srun - - pty - - ntasks = 4 - - mem = 8g - - qos = default - - partition = mbrc - - account = mbrc - - time 1 - 00 : 00 : 00 bash srun : job 218874 queued and waiting for resources srun : job 218874 has been allocated resources [ username @ mbrc00 : ~ ] $ scontrol show job 218874 jobid = 218874 jobname = bash userid = username ( 1000 ) groupid = username ( 21000 ) mcs _ label = n / a priority = 897 nice = 0 account = mbrc qos = default jobstate = running reason = none dependency = ( null ) requeue = 1 restarts = 0 batchflag = 0 reboot = 0 exitcode = 0 : 0 runtime = 00 : 00 : 06 timelimit = 1 - 00 : 00 : 00 timemin = n / a submittime = 2022 - 11 - 18t11 : 13 : 56 eligibletime = 2022 - 11 - 18t11 : 13 : 56 accruetime = 2022 - 11 - 18t11 : 13 : 56 starttime = 2022 - 11 - 18t11 : 13 : 56 endtime = 2022 - 11 - 19t11 : 13 : 56 deadline = n / a preempteligibletime = 2022 - 11 - 18t11 : 13 : 56 preempttime = none suspendtime = none secspresuspend = 0 lastschedeval = 2022 - 11 - 18t11 : 13 : 56 scheduler = main partition = mbrc allocnode : sid = nexusmbrc00 : 25443 reqnodelist = ( null ) excnodelist = ( null ) nodelist = mbrc00 batchhost = mbrc00 numnodes = 1 numcpus = 4 numtasks = 4 cpus / task = 1 reqb : s : c : t = 0 : 0 : * : * tres = cpu = 4, mem = 8g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 8g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage in addition to storage types available to all nexus users, mbrc users can also request mbrc project directories. project directories for this cluster we have decided to allocate network storage on a project by project basis. jonathan heagerty will be the point of contact as it\",\n",
       " 'cpus / task = 1 reqb : s : c : t = 0 : 0 : * : * tres = cpu = 4, mem = 8g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 8g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage in addition to storage types available to all nexus users, mbrc users can also request mbrc project directories. project directories for this cluster we have decided to allocate network storage on a project by project basis. jonathan heagerty will be the point of contact as it pertains to allocating the requested / required storage for each project. as a whole, the mbrc cluster has limited network storage and for this there will be limits to how much and how long network storage can be appropriated. if the requested storage size is significantly large relative to the total allotted amount, the request will be relayed from jonathan heagerty to the mbrc cluster faculty for approval. two other situations that would need approval from the mbrc cluster faculty would be : to request an increase to a projects current storage allotment or to request a time extension for a projects storage. when making a request for storage please provide the following information when contacting staff : - name of user requesting storage : example : jheager2 - name of project : example : foveated rendering - collaborators working on the project : example : sida li - storage size : example : 1tb - length of time for storage : example : 6 - 8 months retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / mbrc & oldid = 12040 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 17 september 2024, at 16 : 40. privacy policy about umiacs disclaimers',\n",
       " 'nexus / mc2 - umiacs nexus / mc2 from umiacs jump to navigation jump to search the previous standalone cluster for mc2 \\' s compute nodes have folded into nexus as of mid 2022. the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 submission nodes 2 resources 3 qos 4 jobs 5 storage submission nodes you can ssh to nexusmc2. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexusmc200. umiacs. umd. edu nexusmc201. umiacs. umd. edu resources the mc2 partition has nodes brought over from the previous standalone mc2 slurm scheduler. the compute nodes are named twist # #. qos mc2 users have access to all of the standard job qoses in the mc2 partition using the mc2 account. the additional jobs qoses for the mc2 partition specifically are : highmem : allows for significantly increased memory to be allocated. huge - long : allows for longer jobs using higher overall resources. please note that the partition has a grptres limit of 100 % of the available cores / ram on the partition - specific nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. jobs you will need to specify - - partition = mc2 and - - account = mc2 to be able to submit jobs to the mc2 partition. [ username @ nexusmc200 ~ ] $ srun - - pty - - ntasks = 4 - - mem = 8g - - qos = default - - partition = mc2 - - account = mc2 - - time 1 - 00 : 00 : 00 bash srun : job 218874 queued and waiting for resources srun : job 218874 has been allocated resources [ username @ twist00 ~ ] $ scontrol show job 218874 jobid = 218874 jobname = bash userid = username ( 1000 ) groupid = username ( 21000 ) mcs _ label = n / a priority = 897 nice = 0 account = mc2 qos = default jobstate = running reason = none dependency = ( null ) requeue = 1 restarts = 0 batchflag = 0 reboot = 0 exitcode = 0 : 0 runtime = 00 : 00 : 06 timelimit = 1 - 00 : 00 : 00 timemin = n / a submittime = 2022 - 11 - 18t11 : 13 : 56 eligibletime = 2022 - 11 - 18t11 : 13 : 56 accruetime = 2022 - 11 - 18t11 : 13 : 56 starttime = 2022 - 11 - 18t11 : 13 : 56 endtime = 2022 - 11 - 19t11 : 13 : 56 deadline = n / a preempteligibletime = 2022 - 11 - 18t11 : 13 : 56 preempttime = none suspendtime = none secspresuspend = 0 lastschedeval = 2022 - 11 - 18t11 : 13 : 56 scheduler = main partition = mc2 allocnode : sid = nexusmc200 : 25443 reqnodelist = ( null ) excnodelist = ( null ) nodelist = twist00 batchhost = twist00 numnodes = 1 numcpus = 4 numtasks = 4 cpus / task = 1 reqb : s : c : t = 0 : 0 : * : * tres = cpu = 4, mem = 8g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 8g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage all data filesystems that were available in the standalone mc2 cluster are also available in nexus. mc2 users can also request nexus project allocations. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php',\n",
       " '/ task = 1 reqb : s : c : t = 0 : 0 : * : * tres = cpu = 4, mem = 8g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 8g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage all data filesystems that were available in the standalone mc2 cluster are also available in nexus. mc2 users can also request nexus project allocations. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / mc2 & oldid = 12055 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 26 september 2024, at 13 : 26. privacy policy about umiacs disclaimers',\n",
       " 'nexus / tron - umiacs nexus / tron from umiacs jump to navigation jump to search the tron partition is a subset of resources available in the nexus. it was purchased using college - level funding for umiacs and csd faculty. hardware the full configuration includes 70 nodes with specs as detailed below. nodenames type quantity cpu cores per node memory per node gpus per node tron [ 00 - 05 ] a6000 gpu node 6 32 256gb 8 tron [ 06 - 44 ] a4000 gpu node 39 16 128gb 4 tron [ 46 - 61 ] a5000 gpu node 16 48 256gb 8 tron [ 62 - 69 ] rtx 2080 ti gpu node 8 32 384gb 8 tron [ 00 - 44, 46 - 69 ] total 69 1840 13282gb 396 retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / tron & oldid = 11884 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 12 june 2024, at 15 : 56. privacy policy about umiacs disclaimers',\n",
       " \"nexus / vulcan - umiacs nexus / vulcan from umiacs jump to navigation jump to search the compute nodes from vulcan ' s previous standalone cluster have folded into nexus as of the scheduled maintenance window for august 2023 ( thursday 08 / 17 / 2023, 5 - 8pm ). the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 usage 2 nodes 3 partitions 4 accounts 5 qos 6 storage 6. 1 home directories 6. 2 scratch directories 6. 2. 1 network scratch directory 6. 2. 2 local scratch directories 6. 3 datasets 6. 4 project storage 6. 5 object storage 7 migration 7. 1 home directories usage you can ssh to nexusvulcan. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexusvulcan00. umiacs. umd. edu nexusvulcan01. umiacs. umd. edu all partitions, qoses, and account names from the standalone vulcan cluster have been moved over to nexus. however, please note that vulcan - is prepended to all of the values that were present in the standalone vulcan cluster to distinguish them from existing values in nexus. the lone exception is the base account that was named vulcan in the standalone cluster ( it is also named just vulcan in nexus ). here are some before / after examples of job submission with various parameters : standalone vulcan cluster submission command nexus cluster submission command srun - - partition = dpart - - qos = medium - - account = abhinav - - gres = gpu : gtx1080ti : 2 - - pty bash srun - - partition = vulcan - dpart - - qos = vulcan - medium - - account = vulcan - abhinav - - gres = gpu : gtx1080ti : 2 - - pty bash srun - - partition = cpu - - qos = cpu - - pty bash srun - - partition = vulcan - cpu - - qos = vulcan - cpu - - account = vulcan - - pty bash srun - - partition = scavenger - - qos = scavenger - - account = vulcan - - gres = gpu : 4 - - pty bash srun - - partition = vulcan - scavenger - - qos = vulcan - scavenger - - account = vulcan - - gres = gpu : 4 - - pty bash vulcan users ( exclusively ) can schedule non - interruptible jobs on vulcan nodes with any non - scavenger job parameters. please note that the vulcan - dpart partition has a grptres limit of 100 % of the available cores / ram on all vulcan # # in aggregate nodes plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. it also has a max submission limit of 500 jobs per user simultaneously so as to not overload the cluster. this is codified by the partition qos named vulcan. please note that the vulcan compute nodes are also in the institute - wide scavenger partition in nexus. vulcan users still have scavenging priority over these nodes via the vulcan - scavenger partition ( i. e., all vulcan - partition jobs ( other than vulcan - scavenger ) can preempt both vulcan - scavenger and scavenger partition jobs, and vulcan - scavenger partition jobs can preempt scavenger partition jobs ). nodes there are currently 46 gpu nodes available running a mixture of nvidia rtx a6000, nvidia rtx a5000, nvidia rtx a4000, nvidia quadro p6000, nvidia geforce gtx 1080 ti, nvidia geforce rtx 2080 ti, and nvidia tesla p100 cards. there are also 4 cpu - only nodes available. all nodes are scheduled with the slurm resource manager. partitions there are three partitions available to general vulcan slurm users. you must specify a partition when submitting your job. vulcan - dpart - this is the default partition. job allocations are guaranteed. only nodes with gpus from architectures before nvidia ' s ampere architecture are included in this partition. vulcan - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs\",\n",
       " \"). nodes there are currently 46 gpu nodes available running a mixture of nvidia rtx a6000, nvidia rtx a5000, nvidia rtx a4000, nvidia quadro p6000, nvidia geforce gtx 1080 ti, nvidia geforce rtx 2080 ti, and nvidia tesla p100 cards. there are also 4 cpu - only nodes available. all nodes are scheduled with the slurm resource manager. partitions there are three partitions available to general vulcan slurm users. you must specify a partition when submitting your job. vulcan - dpart - this is the default partition. job allocations are guaranteed. only nodes with gpus from architectures before nvidia ' s ampere architecture are included in this partition. vulcan - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs in other vulcan - partitions are ready to be scheduled. vulcan - cpu - this partition is for cpu focused jobs. job allocations are guaranteed. there are a few additional partitions available to subsets of vulcan users based on specific requirements. vulcan - ampere - this partition contains nodes with gpus from nvidia ' s ampere architecture. job allocations are guaranteed. as of thursday 02 / 29 / 2024 at 12pm, there is a 4 hour time limit on interactive jobs in this partition. if you need to run longer jobs, you will need to modify your workflow into a job that can be submitted as a batch script. as of thursday 03 / 21 / 2024 at 5pm, there is a limit of 4 cpus and 48g memory maximum per gpu requested by a job. if you need to run jobs with more cpus / memory, you will either need to request more gpus in the job or use a different partition. submission is restricted to the slurm accounts of the faculty who invested in these nodes : abhinav shrivastava ( vulcan - abhinav ) jia - bin huang ( vulcan - jbhuang ) christopher metzler ( vulcan - metzler ) matthias zwicker ( vulcan - zwicker ) accounts vulcan has a base slurm account vulcan which has a modest number of guaranteed billing resources available to all cluster users at any given time. other faculty that have invested in vulcan compute infrastructure have an additional account provided to their sponsored accounts on the cluster. if you do not specify an account when submitting your job, you will receive the vulcan account. if your faculty sponsor has their own account, it is recommended to use that account for job submission. the current faculty accounts are : vulcan - abhinav vulcan - djacobs vulcan - jbhuang vulcan - lsd vulcan - metzler vulcan - rama vulcan - ramani vulcan - yaser vulcan - zwicker $ sacctmgr show account format = account % 20, description % 30, organization % 10 account descr org - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -......... vulcan vulcan vulcan vulcan - abhinav vulcan - abhinav shrivastava vulcan vulcan - djacobs vulcan - david jacobs vulcan vulcan - jbhuang vulcan - jia - bin huang vulcan vulcan - lsd vulcan - larry davis vulcan vulcan - metzler vulcan - chris metzler vulcan vulcan - rama vulcan - rama chellappa vulcan vulcan - ramani vulcan - ramani duraiswami vulcan vulcan - yaser vulcan - yaser yacoob vulcan vulcan - zwicker vulcan - matthias zwicker vulcan......... faculty can manage this list of users via our directory application in the security groups section. the security group that controls access has the prefix vulcan _ and then the faculty username. it will also list slurm : / / nexusctl. umiacs. umd. edu as the associated uri. you can check your account associations by running the show _ assoc command to see the accounts you are associated with. please contact staff and include your faculty member in the conversation if you do not see the appropriate association. $ show _ assoc user account maxjobs grptres qos - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\",\n",
       " 'the security group that controls access has the prefix vulcan _ and then the faculty username. it will also list slurm : / / nexusctl. umiacs. umd. edu as the associated uri. you can check your account associations by running the show _ assoc command to see the accounts you are associated with. please contact staff and include your faculty member in the conversation if you do not see the appropriate association. $ show _ assoc user account maxjobs grptres qos - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -............ abhinav vulcan 48 vulcan - cpu, vulcan - default, vulcan - medium, vulcan - scavenger abhinav vulcan - abhinav 48 vulcan - cpu, vulcan - default, vulcan - high, vulcan - medium, vulcan - scavenger............ you can also see the total number of track - able resources ( tres ) allowed for each account by running the following command. please make sure you give the appropriate account that you are looking for. as shown below, there is a concurrent limit of 64 total gpus for all users not in a contributing faculty group. $ sacctmgr show assoc account = vulcan format = user, account, qos, grptres user account qos grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - vulcan gres / gpu = 64...... qos you need to decide the qos to submit with which will set a certain number of restrictions to your job. if you do not specify a qos when submitting your job using the - - qos parameter, you will receive the vulcan - default qos assuming you are using a vulcan account. the following sacctmgr command will list the current qos. either the vulcan - default, vulcan - medium, or vulcan - high qos is required for the vulcan - dpart partition. please note that only faculty accounts ( see above ) have access to the vulcan - high qos. the following example will show you the current limits that the qos have. the output is truncated to show only relevant vulcan qos. $ show _ qos name maxwall maxtres maxjobspu maxtrespu - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -... vulcan - cpu 2 - 00 : 00 : 00 cpu = 1024, mem = 4t 4 vulcan - default 7 - 00 : 00 : 00 cpu = 4, gres / gpu = 1, mem = 32g 2 vulcan - exempt 7 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g 2 vulcan - high 1 - 12 : 00 : 00 cpu = 16, gres / gpu = 4, mem = 128g 2 vulcan - janus 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 10, mem = 256g vulcan - medium 3 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g 2 vulcan - sailon 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g gres / gpu = 48 vulcan - scavenger 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g... $ show _ partition _ qos name maxsubmitpu maxtrespu grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -',\n",
       " \"cpu = 16, gres / gpu = 4, mem = 128g 2 vulcan - janus 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 10, mem = 256g vulcan - medium 3 - 00 : 00 : 00 cpu = 8, gres / gpu = 2, mem = 64g 2 vulcan - sailon 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g gres / gpu = 48 vulcan - scavenger 3 - 00 : 00 : 00 cpu = 32, gres / gpu = 8, mem = 256g... $ show _ partition _ qos name maxsubmitpu maxtrespu grptres - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -... vulcan 500 cpu = 1760, mem = 15824g vulcan - scavenger 500... storage vulcan has the following storage available. please also review umiacs local data storage policies including any volume that is labeled as scratch. vulcan users can also request nexus project allocations. home directories you have 30gb of home directory storage available at / nfshomes / < username >. it has both snapshots and backups enabled. home directories are intended to store personal or configuration files only. we encourage you to not share any data in your home directory. you are encouraged to utilize our gitlab infrastructure to host your code repositories. note : to check your quota on this directory, use the command df - h ~. scratch directories scratch data has no data protection including no snapshots and the data is not backed up. there are two types of scratch directories in the vulcan compute infrastructure : network scratch directory local scratch directories network scratch directory you have 300gb of scratch storage available at / vulcanscratch / < username >. it is not backed up or protected in any way. this directory is automounted so you will need to cd into the directory or request / specify a fully qualified file path to access this. you may request a temporary increase of up to 500gb total space for a maximum of 120 days without any faculty approval by contacting staff. once the temporary increase period is over, you will be contacted and given a one - week window of opportunity to clean and secure your data before staff will forcibly remove data to get your space back under 300gb. if you need space beyond 500gb or for longer than 120 days, you will need faculty approval and / or a project directory. this file system is available on all submission, data management, and computational nodes within the cluster. local scratch directories each computational node that you can schedule compute jobs on has one or more local scratch directories. these are always named / scratch0, / scratch1, etc. these are almost always more performant than any other storage available to the job. however, you must stage their data within the confine of their job and stage the data out before the end of their job. these local scratch directories have a tmpwatch job which will delete unaccessed data after 90 days, scheduled via maintenance jobs to run once a month at 1am. different nodes will run the maintenance jobs on different days of the month to ensure the cluster is still highly available at all times. please make sure you secure any data you write to these directories at the end of your job. datasets we have read - only dataset storage available at / fs / vulcan - datasets. if there are datasets that you would like to see curated and available, please see this page. the list of vulcan datasets we currently host can be viewed here. project storage users within the vulcan compute infrastructure can request project based allocations for up to 10tb for up to 180 days by contacting staff with approval from the vulcan faculty manager ( dr. shrivastava ). these allocations will be available from / fs / vulcan - projects and / fs / cfar - projects under a name that you provide when you request the allocation. near the end of the allocation period, staff will contact you and ask if you would like to renew the allocation for up to another 180 days ( requires re - approval from dr. shrivastava ). if you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period. staff will then remove the allocation. if you do not respond to staff ' s request by the end of the allocation period, staff will make the allocation temporarily inaccessible\",\n",
       " 'the list of vulcan datasets we currently host can be viewed here. project storage users within the vulcan compute infrastructure can request project based allocations for up to 10tb for up to 180 days by contacting staff with approval from the vulcan faculty manager ( dr. shrivastava ). these allocations will be available from / fs / vulcan - projects and / fs / cfar - projects under a name that you provide when you request the allocation. near the end of the allocation period, staff will contact you and ask if you would like to renew the allocation for up to another 180 days ( requires re - approval from dr. shrivastava ). if you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period. staff will then remove the allocation. if you do not respond to staff \\' s request by the end of the allocation period, staff will make the allocation temporarily inaccessible. if you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible. if one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation. project storage is fully protected. it has snapshots enabled and is backed up nightly. object storage all vulcan users can request project allocations in the umiacs object store. please contact staff with a short project name and the amount of storage you will need to get started. to access this storage, you \\' ll need to use a s3 client or our umobj command line utilities. an example on how to use the umobj command line utilities can be found here. a full set of documentation for the utilities can be found on the umobj gitlab page. migration home directories the nexus uses nfshomes home directories - if your umiacs account was created before february 22nd, 2023, you were using / cfarhomes / < username > as your home directory on the standalone vulcan cluster. while / cfarhomes is available on nexus, your shell initialization scripts from it will not automatically load. please copy over anything you need to your / nfshomes / < username > directory at your earliest convenience, as / cfarhomes will be retired in a two phase process : fri 11 / 17 / 2023, 5pm : cfarhomes directories are made read - only thu 12 / 21 / 2023, 5 - 8pm ( monthly maintenance window ) : cfarhomes directories are taken offline retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / vulcan & oldid = 12037 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 17 september 2024, at 16 : 37. privacy policy about umiacs disclaimers',\n",
       " 'nexus / gpus - umiacs nexus / gpus from umiacs ( redirected from nexus / vulcan / gpus ) jump to navigation jump to search there are several different types of nvidia gpus in the nexus cluster that are available to be scheduled. they are listed below in order of newest to oldest architecture, and then alphabetically. we do not list the exact quantities of gpu here since they change frequently due to additions to or removals from the cluster or during compute node troubleshooting. to see which compute nodes have which gpus and in what quantities, use the show _ nodes command on a submission or compute node. the quantities are listed under the gres column. name gres string ( slurm ) architecture cuda cores gpu memory memory bandwidth fp32 performance ( tflops ) tf32 performance ( dense / sparse tops ) h100 nvlink [ 0 ] h100 - nvl hopper 33792 188gb hbm3 7. 87tb / s 134 not officially published / 1671 h100 sxm h100 - sxm hopper 16896 80gb hbm3 3. 35tb / s 67 not officially published / 989 rtx 6000 ada generation rtx6000ada ada lovelace 18176 48gb gddr6 960gb / s 91. 1 182. 1 / 364. 2 a100 pcie 80gb a100 ampere 6912 80gb hbm2e 1. 94tb / s 19. 5 156 / 312 a100 sxm 80gb a100 ampere 6912 80gb hbm2e 2. 04tb / s 19. 5 156 / 312 geforce rtx 3070 rtx3070 ampere 5888 8gb gddr6 448gb / s 20. 3 20. 3 / 40. 6 geforce rtx 3090 rtx3090 ampere 10496 24gb gddr6x 936gb / s 35. 6 35. 6 / 71 rtx a4000 rtxa4000 ampere 6144 16gb gddr6 448gb / s 19. 2 not officially published rtx a5000 rtxa5000 ampere 8192 24gb gddr6 768gb / s 27. 8 not officially published rtx a6000 rtxa6000 ampere 10752 48gb gddr6 768gb / s 38. 7 77. 4 / 154. 8 geforce rtx 2080 ti rtx2080ti turing 4352 11gb gddr5x 616gb / s 13. 4 n / a geforce gtx 1080 ti gtx1080ti pascal 3584 11gb gddr5x 484gb / s 11. 3 n / a quadro p6000 p6000 pascal 3840 24gb gddr5x 432gb / s 12. 6 n / a tesla p100 p100 pascal 3584 16gb cowos hbm2 732gb / s 9. 3 n / a titan x ( pascal ) titanxpascal pascal 3584 12gb gddr5x 480gb / s 11. 0 n / a titan xp titanxp pascal 3840 12gb gddr5x 548gb / s 12. 1 n / a geforce gtx titan x gtxtitanx maxwell 3072 12gb gddr5 336gb / s 6. 7 n / a [ 0 ] - this gpu is actually a pair of two physical cards connected over nvlink bridges. nvidia \\' s provided specifications for this gpu type are for one physical card ; to get these specs, we have hence doubled their advertised values. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / gpus & oldid = 12042 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 17 september 2024, at 17 : 28. privacy policy about umiacs disclaimers',\n",
       " 'namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 17 september 2024, at 17 : 28. privacy policy about umiacs disclaimers',\n",
       " \"slurm - umiacs slurm from umiacs jump to navigation jump to search contents 1 simple linux utility for resource management ( slurm ) 1. 1 documentation 1. 2 commands 1. 2. 1 srun 1. 2. 2 salloc 1. 2. 3 sbatch 1. 2. 4 squeue 1. 2. 5 scancel 1. 2. 6 sacct 1. 2. 7 sstat 1. 3 modules 1. 4 running jupyter notebook on a compute node 1. 4. 1 setting up your python virtual environment 1. 4. 2 running jupyter notebook 2 quick guide to translate pbs / torque to slurm simple linux utility for resource management ( slurm ) slurm is an open - source workload manager designed for linux clusters of all sizes. it provides three key functions. first, it allocates exclusive or non - exclusive access to resources ( computer nodes ) to users for some duration of time so they can perform work. second, it provides a framework for starting, executing, and monitoring work ( typically a parallel job ) on a set of allocated nodes. finally, it arbitrates contention for resources by managing a queue of pending work. documentation submitting jobs checking job status checking cluster status understanding job priority job preemption overview official documentation faq related documentation : using vs code commands below are some of the common commands used in slurm. further information on how to use these commands is found in the documentation linked above. to see all flags available for a command, please check the command ' s manual by using man < command > on the command line. srun srun runs a parallel job on a cluster managed by slurm. if necessary, it will first create a resource allocation in which to run the parallel job. salloc salloc allocates a slurm job allocation, which is a set of resources ( nodes ), possibly with some set of constraints ( e. g. number of processors per node ). when salloc successfully obtains the requested allocation, it then runs the command specified by the user. finally, when the user specified command is complete, salloc relinquishes the job allocation. if no command is specified, salloc runs the user ' s default shell. sbatch sbatch submits a batch script to slurm. the batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. the batch script may contain options preceded with # sbatch before any executable commands in the script. squeue squeue views job and job step information for jobs managed by slurm. scancel scancel signals or cancels jobs, job arrays, or job steps. an arbitrary number of jobs or job steps may be signaled using job specification filters or a space separated list of specific job and / or job step ids. sacct sacct displays job accounting data stored in the job accounting log file or slurm database in a variety of forms for your analysis. the sacct command displays information on jobs, job steps, status, and exitcodes by default. you can tailor the output with the use of the - - format = option to specify the fields to be shown. sstat sstat displays job status information for your analysis. the sstat command displays information pertaining to cpu, task, node, resident set size ( rss ) and virtual memory ( vm ). you can tailor the output with the use of the - - fields = option to specify the fields to be shown. modules if you are trying to use gnu modules in a slurm job, please read the section of our modules documentation on non - interactive shell sessions. this also needs to be done if the os version of the compute node you are scheduled on is different from the os version of the submission node you are submitting the job from. running jupyter notebook on a compute node the steps to run a jupyter notebook from a compute node are listed below. setting up your python virtual environment create a python virtual environment on the compute node you are assigned and activate it. next, install jupyter using pip by following the steps here. you may also use other environment management systems such as conda if desired. running jupyter notebook after you ' ve set up the python virtual environment, submit a job, activate the environment within the job, and run the following command on the compute node you are assigned : jupyter notebook - - no - browser - - port = 8889 - - ip = 0. 0. 0. 0 this will start running the notebook on port 8889. note : you must keep this shell window open to be able to connect. if the submission node for the cluster you are using is not accessible via the public internet, you must also be on a machine connected to the\",\n",
       " \". running jupyter notebook on a compute node the steps to run a jupyter notebook from a compute node are listed below. setting up your python virtual environment create a python virtual environment on the compute node you are assigned and activate it. next, install jupyter using pip by following the steps here. you may also use other environment management systems such as conda if desired. running jupyter notebook after you ' ve set up the python virtual environment, submit a job, activate the environment within the job, and run the following command on the compute node you are assigned : jupyter notebook - - no - browser - - port = 8889 - - ip = 0. 0. 0. 0 this will start running the notebook on port 8889. note : you must keep this shell window open to be able to connect. if the submission node for the cluster you are using is not accessible via the public internet, you must also be on a machine connected to the umiacs network or connected to our vpn in order to access the jupyter notebook once you start the ssh tunnel, so ensure this is the case before starting the tunnel. then, on your local machine, run ssh - n - f - l localhost : 8888 : < nodename > : 8889 < username > @ < submissionnode >. umiacs. umd. edu this will tunnel port 8889 from the compute node to port 8888 on your local machine, using < submissionnode > as an intermediate node. make sure to replace < username > with your username, < submissionnode > with the name of the submission node you want to use, and < nodename > with the name of the compute node you are assigned. note that this command will not display any output if the connection is successful due to the included ssh flags. you must also keep this shell window open to be able to connect. for example, assuming your username is username and that you are using the nexus cluster, have been assigned the nexusgroup submission nodes, and are assigned compute node tron00. umiacs. umd. edu : ssh - n - f - l localhost : 8888 : tron00. umiacs. umd. edu : 8889 username @ nexusgroup. umiacs. umd. edu you can then open a web browser and type in localhost : 8888 to access the notebook. notes : later versions of jupyter have token authentication enabled by default - you will need to prepend the /? token = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx part of the url provided by the terminal output after starting the notebook in order to connect if this is the case. e. g. localhost : 8888 /? token = fcc6bd0f996e7aa89376c33cb34f7b80890502aacc97d98e if the port on the compute node mentioned in the example above ( 8889 ) is not working, it may be that someone else has already started a process ( jupyter notebook or otherwise ) using that specific port number on that specific compute node. the port number can be replaced with any other ephemeral port number you ' d like, just make sure to change it in both the command you run on the compute node and the ssh command from your local machine. quick guide to translate pbs / torque to slurm pbs / torque was the previous workload manager and job submission framework used at umiacs prior to slurm ' s adoption. below is a quick guide of how to translate some common pbs / torque commands to slurm ones. user commands pbs / torque slurm job submission qsub [ filename ] sbatch [ filename ] job deletion qdel [ job _ id ] scancel [ job _ id ] job status ( by job ) qstat [ job _ id ] squeue - - job [ job _ id ] full job status ( by job ) qstat - f [ job _ id ] scontrol show job [ job _ id ] job status ( by user ) qstat - u [ username ] squeue - - user = [ username ] environment variables pbs / torque slurm job id $ pbs _ jobid $ slurm _ jobid submit directory $ pbs _ o _ workdir $ slurm _ submit _ dir node list $ pbs _ nodefile $ slurm _ job _ nodelist job specification pbs / torque slurm script directive # pbs # sbatch job name - n [ name ] - - job - name = [ name ] or - j [ name ] node count - l nodes = [ count\",\n",
       " '] job deletion qdel [ job _ id ] scancel [ job _ id ] job status ( by job ) qstat [ job _ id ] squeue - - job [ job _ id ] full job status ( by job ) qstat - f [ job _ id ] scontrol show job [ job _ id ] job status ( by user ) qstat - u [ username ] squeue - - user = [ username ] environment variables pbs / torque slurm job id $ pbs _ jobid $ slurm _ jobid submit directory $ pbs _ o _ workdir $ slurm _ submit _ dir node list $ pbs _ nodefile $ slurm _ job _ nodelist job specification pbs / torque slurm script directive # pbs # sbatch job name - n [ name ] - - job - name = [ name ] or - j [ name ] node count - l nodes = [ count ] - - nodes = [ min [ - max ] ] or - n [ min [ - max ] ] cpu count - l ppn = [ count ] - - ntasks - per - node = [ count ] cpus per task - - cpus - per - task = [ count ] memory size - l mem = [ mb ] - - mem = [ mb ] or - - mem - per - cpu = [ mb ] wall clock limit - l walltime = [ hh : mm : ss ] - - time = [ min ] or - - time = [ days - hh : mm : ss ] node properties - l nodes = 4 : ppn = 8 : [ property ] - - constraint = [ list ] standard output file - o [ file _ name ] - - output = [ file _ name ] or - o [ file _ name ] standard error file - e [ file _ name ] - - error = [ file _ name ] or - e [ file _ name ] combine stdout / stderr - j oe ( both to stdout ) ( default if you don \\' t specify - - error ) job arrays - t [ array _ spec ] - - array = [ array _ spec ] or - a [ array _ spec ] delay job start - a [ time ] - - begin = [ time ] retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm & oldid = 11958 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 25 july 2024, at 14 : 22. privacy policy about umiacs disclaimers',\n",
       " 'slurm / arrayjobs - umiacs slurm / arrayjobs from umiacs jump to navigation jump to search here is an example to get you started using array jobs in slurm. array computation example job save this code to a file called test. py. import time print ( \\' start at \\' + time. strftime ( \\' % h : % m : % s \\' ) ) print ( \\' sleep for 10 seconds... \\' ) time. sleep ( 10 ) print ( \\' stop at \\' + time. strftime ( \\' % h : % m : % s \\' ) ) submission script save this to a file called array. sh and you should be able to submit the job as sbatch array. sh. #! / bin / bash # # # # # # # # # # # # # # # # # # # # # # job - array example # # # # # # # # # # # # # # # # # # # # # # # sbatch - - job - name = example # sbatch - - array = 1 - 16 # run 16 jobs at the same time # sbatch - - time = 0 - 00 : 05 : 00 # run for 5 minutes ( d - hh : mm : ss ) # sbatch - - mem - per - cpu = 500mb # use 500mb per core # all bash commands must be after all sbatch directives # define and create a unique scratch directory scratch _ directory = / scratch0 / $ { user } / job - array - example / $ { slurm _ jobid } mkdir - p $ { scratch _ directory } cd $ { scratch _ directory } cp $ { slurm _ submit _ dir } / test. py $ { scratch _ directory } # each job will see a different $ { slurm _ array _ task _ id } echo \" now processing task id : : \" $ { slurm _ array _ task _ id } python test. py > output _ $ { slurm _ array _ task _ id }. txt # after the job is done we copy our output back to $ slurm _ submit _ dir cp output _ $ { slurm _ array _ task _ id }. txt $ { slurm _ submit _ dir } # we step out of the scratch directory and remove it cd $ { slurm _ submit _ dir } rm - rf $ { scratch _ directory } # happy end exit 0 retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / arrayjobs & oldid = 11731 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 28 march 2024, at 20 : 30. privacy policy about umiacs disclaimers',\n",
       " 'slurm / clusterstatus - umiacs slurm / clusterstatus from umiacs jump to navigation jump to search contents 1 cluster status 1. 1 sinfo 1. 2 scontrol 1. 3 sacctmgr cluster status slurm offers a variety of tools to check the general status of nodes / partitions in a cluster. sinfo the sinfo command will show you the status of partitions in the cluster. passing the - n flag will show each node individually. $ sinfo partition avail timelimit nodes state nodelist gamma up infinite 3 idle gammagpu [ 01 - 03 ] scavenger up infinite 2 drain tron [ 50 - 51 ] scavenger up infinite 21 mix tron [ 00 - 01, 03 - 15, 46 - 49, 52 - 53 ] scavenger up infinite 31 idle tron [ 02, 16 - 45 ] tron * up 3 - 00 : 00 : 00 2 drain tron [ 50 - 51 ] tron * up 3 - 00 : 00 : 00 21 mix tron [ 00 - 01, 03 - 15, 46 - 49, 52 - 53 ] tron * up 3 - 00 : 00 : 00 31 idle tron [ 02, 16 - 45 ] $ sinfo - n nodelist nodes partition state gammagpu01 1 gamma idle gammagpu02 1 gamma idle gammagpu03 1 gamma idle tron00 1 scavenger mix tron00 1 tron * mix tron01 1 scavenger mix tron01 1 tron * mix tron02 1 scavenger idle tron02 1 tron * idle tron03 1 scavenger mix tron03 1 tron * mix tron04 1 scavenger mix tron04 1 tron * mix... tron52 1 scavenger mix tron52 1 tron * mix tron53 1 scavenger mix tron53 1 tron * mix scontrol the scontrol command can be used to view the status / configuration of the nodes in the cluster. if passed specific node name ( s ) only information about those node ( s ) will be displayed, otherwise all nodes will be listed. to specify multiple nodes, separate each node name by a comma ( no spaces ). $ scontrol show nodes tron05, tron13 nodename = tron05 arch = x86 _ 64 corespersocket = 16 cpualloc = 28 cputot = 32 cpuload = 47. 32 availablefeatures = rhel8, amd, epyc - 7302 activefeatures = rhel8, amd, epyc - 7302 gres = gpu : rtxa6000 : 8 nodeaddr = tron05 nodehostname = tron05 version = 21. 08. 5 os = linux 4. 18. 0 - 348. 20. 1. el8 _ 5. x86 _ 64 # 1 smp tue mar 8 12 : 56 : 54 est 2022 realmemory = 257538 allocmem = 157696 freemem = 197620 sockets = 2 boards = 1 state = mixed threadspercore = 1 tmpdisk = 0 weight = 100 owner = n / a mcs _ label = n / a partitions = scavenger, tron boottime = 2022 - 04 - 21t17 : 40 : 51 slurmdstarttime = 2022 - 04 - 21t18 : 00 : 56 lastbusytime = 2022 - 04 - 22t11 : 21 : 16 cfgtres = cpu = 32, mem = 257538m, billing = 346, gres / gpu = 8, gres / gpu : rtxa6000 = 8 alloctres = cpu = 28, mem = 154g, gres / gpu = 7, gres / gpu : rtxa6000 = 7 capwatts = n / a currentwatts = 0 avewatts = 0 extsensorsjoules = n / s extsensorswatts = 0 extsensorstemp = n / s nodename = tron13 arch = x86 _ 64 corespersocket = 16 cpualloc = 1 cputot = 16 cpuload = 8. 41 availablefeatures = rhel8, amd, epyc - 7302p activefeatures = rhel8, amd, epyc - 7302p gres = gpu : rtxa4000 : 4 nodeaddr = tron13 nodehostname = tron13 version = 21. 08. 5 os = linux 4. 18. 0 - 348. 20. 1. el8 _ 5. x86 _ 64 # 1 smp tue mar 8 12 : 56 :',\n",
       " 'gres / gpu : rtxa6000 = 7 capwatts = n / a currentwatts = 0 avewatts = 0 extsensorsjoules = n / s extsensorswatts = 0 extsensorstemp = n / s nodename = tron13 arch = x86 _ 64 corespersocket = 16 cpualloc = 1 cputot = 16 cpuload = 8. 41 availablefeatures = rhel8, amd, epyc - 7302p activefeatures = rhel8, amd, epyc - 7302p gres = gpu : rtxa4000 : 4 nodeaddr = tron13 nodehostname = tron13 version = 21. 08. 5 os = linux 4. 18. 0 - 348. 20. 1. el8 _ 5. x86 _ 64 # 1 smp tue mar 8 12 : 56 : 54 est 2022 realmemory = 128525 allocmem = 65536 freemem = 33463 sockets = 1 boards = 1 state = mixed threadspercore = 1 tmpdisk = 0 weight = 10 owner = n / a mcs _ label = n / a partitions = scavenger, tron boottime = 2022 - 04 - 21t17 : 40 : 46 slurmdstarttime = 2022 - 04 - 21t17 : 54 : 51 lastbusytime = 2022 - 04 - 22t13 : 04 : 57 cfgtres = cpu = 16, mem = 128525m, billing = 173, gres / gpu = 4, gres / gpu : rtxa4000 = 4 alloctres = cpu = 1, mem = 64g, gres / gpu = 4, gres / gpu : rtxa4000 = 4 capwatts = n / a currentwatts = 0 avewatts = 0 extsensorsjoules = n / s extsensorswatts = 0 extsensorstemp = n / s sacctmgr the sacctmgr command shows cluster accounting information. one of the helpful commands is to list the available qoses. $ sacctmgr list qos format = name, priority, maxwall, maxjobspu name priority maxwall maxjobspu - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - normal 0 dpart 0 2 - 00 : 00 : 00 8 gpu 0 08 : 00 : 00 2 retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / clusterstatus & oldid = 11363 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 2 october 2023, at 17 : 36. privacy policy about umiacs disclaimers',\n",
       " \"slurm / jobstatus - umiacs slurm / jobstatus from umiacs jump to navigation jump to search contents 1 job status 1. 1 squeue 1. 2 sstat 1. 3 sacct 2 job codes job status slurm offers a variety of tools to check the status of your jobs before, during, and after execution. when you first submit your job, slurm should give you a job id which represents the resources allocated to your job. individual calls to srun will spawn job steps which can also be queried individually. squeue the squeue command shows job status in the queue. helpful flags : - u username to show only your jobs ( replace username with your umiacs username ) - - start to estimate start time for a job that has not yet started and the reason why it is waiting - s to show the status of individual job steps for a job ( e. g. batch jobs ) examples : [ username @ nexusclip00 ~ ] $ squeue - u username jobid partition name user st time nodes nodelist ( reason ) 162 tron hellowor username r 0 : 03 2 tron [ 00 - 01 ] [ username @ nexusclip00 ~ ] $ squeue - - start - u username jobid partition name user st start _ time nodes schednodes nodelist ( reason ) 163 tron hellowo2 username pd 2020 - 05 - 11t18 : 36 : 49 1 tron02 ( priority ) [ username @ nexusclip00 ~ ] $ squeue - s - u username stepid name partition user time nodelist 162. 0 sleep tron username 0 : 05 tron00 162. 1 sleep tron username 0 : 05 tron01 sstat the sstat command shows metrics from currently running job steps. if you don ' t specify a job step, the lowest job step is displayed. sstat - - format jobid, ntasks, nodelist, maxrss, maxvmsize, averss, avevmsize < $ jobid >. < $ jobstep > [ username @ nexusclip00 ~ ] $ sstat - - format jobid, ntasks, nodelist, maxrss, maxvmsize, averss, avevmsize 171 jobid ntasks nodelist maxrss maxvmsize averss avevmsize - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 171. 0 1 tron00 0 186060k 0 107900k [ username @ nexusclip00 ~ ] $ sstat - - format jobid, ntasks, nodelist, maxrss, maxvmsize, averss, avevmsize 171. 1 jobid ntasks nodelist maxrss maxvmsize averss avevmsize - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 171. 1 1 tron01 0 186060k 0 107900k note that if you do not have any jobsteps, sstat will return an error. [ username @ nexusclip00 ~ ] $ sstat - - format jobid, ntasks, nodelist, maxrss, maxvmsize, averss, avevmsize 172 jobid ntasks nodelist maxrss maxvmsize averss avevmsize - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - sstat : error : no steps running for job 237 if you do not run any srun commands, you will not create any job steps and metrics will not be available for your job. your batch scripts should follow this format : #! / bin / bash # sbatch... # sbatch... # set environment up module load... # launch job steps srun <\",\n",
       " '##s, maxvmsize, averss, avevmsize 172 jobid ntasks nodelist maxrss maxvmsize averss avevmsize - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - sstat : error : no steps running for job 237 if you do not run any srun commands, you will not create any job steps and metrics will not be available for your job. your batch scripts should follow this format : #! / bin / bash # sbatch... # sbatch... # set environment up module load... # launch job steps srun < command to run > # that would be step 1 srun < command to run > # that would be step 2 sacct the sacct command shows metrics from past jobs. [ username @ nexusclip00 ~ ] $ sacct jobid jobname partition account alloccpus state exitcode - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 162 helloworld tron nexus 2 completed 0 : 0 162. batch batch nexus 1 completed 0 : 0 162. 0 sleep nexus 1 completed 0 : 0 162. 1 sleep nexus 1 completed 0 : 0 163 helloworld tron nexus 2 completed 0 : 0 163. batch batch nexus 1 completed 0 : 0 163. 0 sleep nexus 1 completed 0 : 0 to check one specific job, you can run something like the following ( if you omit. < $ jobstep >, all jobsteps will be shown ) : sacct - - format jobid, jobname, ntasks, nodelist, maxrss, maxvmsize, averss, avevmsize, elapsed - j < $ jobid >. < $ jobstep > [ username @ nexusclip00 ~ ] $ sacct - - format jobid, jobname, ntasks, nodelist, maxrss, maxvmsize, averss, avevmsize, elapsed - j 171 jobid jobname ntasks nodelist maxrss maxvmsize averss avevmsize elapsed - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 171 helloworld tron [ 00 - 01 ] 00 : 00 : 30 171. batch batch 1 tron00 0 119784k 0 113120k 00 : 00 : 30 171. 0 sleep 1 tron00 0 186060k 0 107900k 00 : 00 : 30 171. 1 sleep 1 tron01 0 186060k 0 107900k 00 : 00 : 30 job codes if you list the current running jobs and your job is in pd ( pending ), slurm will provide you some information on what the reason for this in the nodelist parameter. you can use scontrol show job < jobid > to get all the parameters for your job to help identify why your job is not running. [ username @ nexusclip00 ~ ] $ squeue - u username jobid partition name user st time nodes nodelist ( reason ) 1 tron bash username pd 0 : 00 1 ( assocgrpgres ) 2 tron bash username pd 0 : 00 1 ( resources ) 3 tron bash username pd 0 : 00 1 ( priority ) 4 tron bash username pd 0 : 00 1 ( qosmaxgresperuser ) 5 tron bash username pd 0 : 00 1 ( reqnodenotavail, reserved for maintenance ) some common ones are as follows : resources - the cluster does not currently have the resources to fit your job in your selected partition. priority - the cluster has reserved resources for higher priority jobs in your selected partition. qosmax * peruser or qosmax * peruserlimit - the quality of service ( qos ) your job is requesting to use has some limit per user ( cpu, mem, gres,',\n",
       " '##ip00 ~ ] $ squeue - u username jobid partition name user st time nodes nodelist ( reason ) 1 tron bash username pd 0 : 00 1 ( assocgrpgres ) 2 tron bash username pd 0 : 00 1 ( resources ) 3 tron bash username pd 0 : 00 1 ( priority ) 4 tron bash username pd 0 : 00 1 ( qosmaxgresperuser ) 5 tron bash username pd 0 : 00 1 ( reqnodenotavail, reserved for maintenance ) some common ones are as follows : resources - the cluster does not currently have the resources to fit your job in your selected partition. priority - the cluster has reserved resources for higher priority jobs in your selected partition. qosmax * peruser or qosmax * peruserlimit - the quality of service ( qos ) your job is requesting to use has some limit per user ( cpu, mem, gres, etc. ). use show _ qos and show _ partition _ qos to identify the limit ( s ) and then use scontrol show job < jobid > for each of your jobs running in that qos to see the resources they are currently consuming. assocgrpbilling - the slurm account you are using has a limit on the overall billing amount available in total for the account. use sacctmgr show assoc account = < accountname > where user = to identify the limit, replacing < accountname > with the account you are submitting your job with. you can see all jobs running under the account and their billing values by running squeue - a < accountname > - o \" jobid :. 18, partition :. 9, name :. 8, username :. 8, statecompact :. 2, timeused :. 10, numnodes :. 6, reasonlist : 45, tres - alloc : 80 \". the billing value will be part of the tres - alloc string for each job. reqnodenotavail - none of the nodes that could run your job ( based on requested partition / resources ) currently have the resources to fit your job. alternatively, if you also see reserved for maintenance, there is a reservation in place ( often for a maintenance window ). you can see the current reservations by running scontrol show reservation. often the culprit is that you have requested a timelimit that will conflict with the reservation. you can either lower your timelimit such that the job will complete before the reservation begins, or leave your job to wait until the reservation completes. slurm \\' s full list of reasons / explanations can be found here. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / jobstatus & oldid = 12005 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 22 august 2024, at 19 : 12. privacy policy about umiacs disclaimers',\n",
       " 'slurm / jobsubmission - umiacs slurm / jobsubmission from umiacs jump to navigation jump to search contents 1 job submission 1. 1 srun 1. 1. 1 common srun arguments 1. 1. 2 interactive shell sessions 1. 2 salloc 1. 3 sbatch 1. 3. 1 advanced batch scripts 1. 3. 2 more examples 1. 3. 3 scancel 2 identifying resources and features 2. 1 show _ nodes 2. 1. 1 examples 2. 2 show _ available _ nodes 2. 2. 1 footnotes 2. 2. 2 examples 3 requesting gpus 4 mpi example job submission slurm offers a variety of ways to run jobs. it is important to understand the different options available and how to request the resources required for a job in order for it to run successfully. all job submission should be done from submit nodes ; any computational code should be run in a job allocation on compute nodes. the following commands outline how to allocate resources on the compute nodes and submit processes to be run on the allocated nodes. the cluster that everyone with a umiacs account has access to is nexus. please visit the nexus page for instructions on how to connect to your assigned submit nodes. computationally intensive processes run on submission nodes will be terminated. please submit jobs to be scheduled on compute nodes for this purpose. for details on how slurm decides how to schedule jobs when multiple jobs are waiting in a scheduler \\' s queue, please see slurm / priority. srun the srun command is used to run a process on the compute nodes in the cluster. if you pass it a normal shell command ( or command that executes a script ), it will submit a job to run that shell command / script on a compute node and then return. srun accepts many command line options to specify the resources required by the command passed to it. some common command line arguments are listed below and full documentation of all available options is available in the man page for srun, which can be accessed by running man srun. $ srun - - qos = default - - mem = 100mb - - time = 1 : 00 : 00 bash - c \\' echo \" hello world from \" ` hostname ` \\' hello world from tron33. umiacs. umd. edu it is important to understand that srun is an interactive command. by default input to srun is broadcast to all compute nodes running your process and output from the compute nodes is redirected to srun. this behavior can be changed ; however, srun will always wait for the command passed to finish before exiting, so if you start a long running process and end your terminal session, your process will stop running on the compute nodes and your job will end. to run a non - interactive submission that will remain running after you logout, you will need to wrap your srun commands in a batch script and submit it with sbatch. common srun arguments - - job - name = < jobname > requests your job be named < jobname > - - mem = 1g requests 1gb of memory for your job, if no unit is given mb is assumed - - ntasks = 2 requests 2 \" tasks \" which map to cores on a cpu for your job ; if passed to srun, runs the given command concurrently on each core - - nodes = 2 requests 2 nodes be allocated to your job ; if passed to srun, runs the given command concurrently on each node - - nodelist = < nodename > requests to run your job on the < nodename > node - - time = dd - hh : mm : ss requests your job run for dd days, hh hours, mm minutes, and ss seconds - - error = < errname > redirects stderr for your job to the < errname > file - - partition = < partitionname > requests your job run in the < partitionname > partition - - qos = < qosname > default requests your job run with the < qosname > qos, to see the available qos options on a cluster, run show _ qos - - account = < accountname > requests your job runs under the < accountname > slurm account, different accounts have different available partitions / qos - - output = < outname > redirects stdout for your job to the < outname > file - - requeue requests your job be automatically requeued if it is preempted - - exclusive requests your job be the only one running on the node ( s ) it is assigned to. this requires that your job be allocated all of the resources on the node ( s ). the scheduler does not automatically give your job all of the node \\' s / nodes \\' resources, however, so if you need more than the default, you still need to request these with - - ntasks and',\n",
       " '##os = < qosname > default requests your job run with the < qosname > qos, to see the available qos options on a cluster, run show _ qos - - account = < accountname > requests your job runs under the < accountname > slurm account, different accounts have different available partitions / qos - - output = < outname > redirects stdout for your job to the < outname > file - - requeue requests your job be automatically requeued if it is preempted - - exclusive requests your job be the only one running on the node ( s ) it is assigned to. this requires that your job be allocated all of the resources on the node ( s ). the scheduler does not automatically give your job all of the node \\' s / nodes \\' resources, however, so if you need more than the default, you still need to request these with - - ntasks and - - mem interactive shell sessions an interactive shell session on a compute node can be useful for debugging or developing code that isn \\' t ready to be run as a batch job. to get an interactive shell on a node, use srun with the - - pty argument to invoke a shell : $ srun - - pty - - qos = default - - mem = 1g - - time = 01 : 00 : 00 bash $ hostname tron33. umiacs. umd. edu please do not leave interactive shells running for long periods of time when you are not working. this blocks resources from being used by everyone else. salloc the salloc command can also be used to request resources be allocated without needing a batch script. running salloc with a list of resources will allocate the resources you requested, create a job, and drop you into a subshell with the environment variables necessary to run commands in the newly created job allocation. when your time is up or you exit the subshell, your job allocation will be relinquished. $ salloc - - qos = default - n 1 - - mem = 2g - - time = 01 : 00 : 00 salloc : granted job allocation 159 $ srun / usr / bin / hostname tron33. umiacs. umd. edu $ exit exit salloc : relinquishing job allocation 159 please note that any commands not invoked with srun will be run locally on the submit node. please be careful when using salloc. sbatch the sbatch command allows you to write a batch script to be submitted and run non - interactively on the compute nodes. to run a simple hello world command on the compute nodes you could write a file, helloworld. sh with the following contents : #! / bin / bash srun bash - c \\' echo hello world from ` hostname ` \\' then you need to submit the script with sbatch and request resources : $ sbatch - - qos = default - - mem = 1g - - time = 1 : 00 : 00 helloworld. sh submitted batch job 121 slurm will return a job number that you can use to check the status of your job with squeue : $ squeue jobid partition name user st time nodes nodelist ( reason ) 121 tron hellowor username r 0 : 01 1 tron32 advanced batch scripts you can also write a batch script with all of your resources / options defined in the script itself. this is useful for jobs that need to be run tens / hundreds / thousands of times. you can then handle any necessary environment setup and run commands on the resources you requested by invoking commands with srun. the srun commands can also be more complex and be told to only use portions of your entire job allocation, each of these distinct srun commands makes up one \" job step \". the batch script will be run on the first node allocated as part of your job allocation and each job step will be run on whatever resources you tell them to. in the following example, we have a batch job that will request 2 nodes in the cluster. we then load a specific version of python into our environment and submit two job steps, each one using one node. since srun is blocks until the command finishes, we use the \\' & \\' operator to background the process so that both job steps can run at once ; however, this means that we then need to use the wait command to block processing until all background processes have finished. #! / bin / bash # lines that begin with # sbatch specify commands to be used by slurm for scheduling # sbatch - - job - name = helloworld # sets the job name # sbatch - - output = helloworld. out. % j # indicates a file to redirect stdout to ; % j is the jobid. if set, must be',\n",
       " 'as part of your job allocation and each job step will be run on whatever resources you tell them to. in the following example, we have a batch job that will request 2 nodes in the cluster. we then load a specific version of python into our environment and submit two job steps, each one using one node. since srun is blocks until the command finishes, we use the \\' & \\' operator to background the process so that both job steps can run at once ; however, this means that we then need to use the wait command to block processing until all background processes have finished. #! / bin / bash # lines that begin with # sbatch specify commands to be used by slurm for scheduling # sbatch - - job - name = helloworld # sets the job name # sbatch - - output = helloworld. out. % j # indicates a file to redirect stdout to ; % j is the jobid. if set, must be set to a file instead of a directory or else submission will fail. # sbatch - - error = helloworld. out. % j # indicates a file to redirect stderr to ; % j is the jobid. if set, must be set to a file instead of a directory or else submission will fail. # sbatch - - time = 00 : 05 : 00 # how long you would like your job to run ; format = dd - hh : mm : ss # sbatch - - qos = default # set qos, this will determine what resources can be requested # sbatch - - nodes = 2 # number of nodes to allocate for your job # sbatch - - ntasks = 4 # request 4 cpu cores be reserved for your node total # sbatch - - ntasks - per - node = 2 # request 2 cpu cores be reserved per node # sbatch - - mem = 1g # memory required by job ; if unit is not specified mb will be assumed. for multi - node jobs, this argument allocates this much memory * per node * srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # use srun to invoke commands within your job ; using an \\' & \\' srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # will background the process allowing them to run concurrently wait # wait for any background processes to complete # once the end of the batch script is reached your job allocation will be revoked another useful thing to know is that you can pass additional arguments into your sbatch scripts on the command line and reference them as $ { 1 } for the first argument and so on. more examples slurm / arrayjobs scancel the scancel command can be used to cancel job allocations or job steps that are no longer needed. it can be passed individual job ids or an option to delete all of your jobs or jobs that meet certain criteria. scancel 255 cancel job 255 scancel 255. 3 cancel job step 3 of job 255 scancel - - user username - - partition = tron cancel all jobs for username in the tron partition identifying resources and features the sinfo command can show you additional features of nodes in the cluster but you need to ask it to show some non - default options using a command like sinfo - o \" % 40n % 8c % 8m % 35f % 35g \". $ sinfo - o \" % 40n % 8c % 8m % 35f % 35g \" nodelist cpus memory avail _ features gres legacy00 48 125940 rhel8, zen, epyc - 7402 ( null ) legacy [ 01 - 11, 13 - 19, 22 - 28, 30 ] 12 + 61804 + rhel8, xeon, e5 - 2620 ( null ) cbcb [ 23 - 24 ], twist [ 02 - 05 ] 24 255150 rhel8, xeon, e5 - 2650 ( null ) cbcb26 128 513243 rhel8, zen, epyc - 7763, ampere gpu : rtxa5000 : 8 cbcb27 64 255167 rhel8, zen, epyc - 7513, ampere gpu : rtxa6000 : 8 cbcb [ 00 - 21 ] 32 2061175 rhel8, zen, epyc - 7313 ( null ) cbcb22, cmlcpu [ 00, 06 - 07 ], legacy20 24 + 384270 + rhel8, xeon, e5 - 2680 ( null ) cbcb25 24 255278 rhel8, xeon, e5 - 2650, pascal, turing gpu : rtx2080ti : 1, gp',\n",
       " '2620 ( null ) cbcb [ 23 - 24 ], twist [ 02 - 05 ] 24 255150 rhel8, xeon, e5 - 2650 ( null ) cbcb26 128 513243 rhel8, zen, epyc - 7763, ampere gpu : rtxa5000 : 8 cbcb27 64 255167 rhel8, zen, epyc - 7513, ampere gpu : rtxa6000 : 8 cbcb [ 00 - 21 ] 32 2061175 rhel8, zen, epyc - 7313 ( null ) cbcb22, cmlcpu [ 00, 06 - 07 ], legacy20 24 + 384270 + rhel8, xeon, e5 - 2680 ( null ) cbcb25 24 255278 rhel8, xeon, e5 - 2650, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 1 legacy21 8 61746 rhel8, xeon, e5 - 2623 ( null ) tron [ 06 - 09, 12 - 15, 21 ] 16 126214 + rhel8, zen, epyc - 7302p, ampere gpu : rtxa4000 : 4 tron [ 10 - 11, 16 - 20, 34 ] 16 126217 rhel8, zen, epyc - 7313p, ampere gpu : rtxa4000 : 4 tron [ 22 - 33, 35 - 45 ] 16 126214 + rhel8, zen, epyc - 7302, ampere gpu : rtxa4000 : 4 clip11 16 126217 rhel8, zen, epyc - 7313, ampere gpu : rtxa4000 : 4 clip00 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 3 clip02 20 126255 rhel8, xeon, e5 - 2630, pascal gpu : gtx1080ti : 3 clip03 20 126243 rhel8, xeon, e5 - 2630, pascal, turing gpu : rtx2080ti : 1, gpu : gtx1080ti : 2 clip04 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtx3090 : 4 clip [ 05 - 06 ] 24 126216 rhel8, zen, epyc - 7352, ampere gpu : rtxa6000 : 2 clip07 8 255263 rhel8, xeon, e5 - 2623, pascal gpu : gtx1080ti : 3 clip09 32 383043 rhel8, xeon, 6130, pascal, turing gpu : rtx2080ti : 5, gpu : gtx1080ti : 3 clip13, cml30, vulcan [ 29 - 32 ] 32 255218 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 8 clip08, vulcan [ 08 - 22, 25 ] 32 255258 + rhel8, xeon, e5 - 2683, pascal gpu : gtx1080ti : 8 clip12, gammagpu [ 10 - 17 ] 16 126203 + rhel8, zen, epyc - 7313, ampere gpu : rtxa6000 : 4 clip01 32 255276 rhel8, xeon, e5 - 2683, pascal gpu : titanxpascal : 1, gpu : titanxp : 2 clip10 44 1029404 rhel8, xeon, e5 - 2699 ( null ) cml [ 00, 02 - 11, 13 - 14 ], tron [ 62 - 63, 65 - 66, 68 - 32 351530 + rhel8, xeon, 4216, turing gpu : rtx2080ti : 8 cml01 32 383030 rhel8, xeon, 4216, turing gpu : rtx2080ti : 6 cml12 32 383038 rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtxa4000 : 1 cml [ 15 - 16 ] 32 383038 rhel8, xeon, 4216, turing gpu : rtx2080ti : 7 cml [ 17 - 28 ], gammagpu05 32 255225 + rhel8, zen, epyc - 7282, ampere gpu : rtxa4000 : 8 cml31 32 384094 rhel8, zen, epyc -',\n",
       " '##on [ 62 - 63, 65 - 66, 68 - 32 351530 + rhel8, xeon, 4216, turing gpu : rtx2080ti : 8 cml01 32 383030 rhel8, xeon, 4216, turing gpu : rtx2080ti : 6 cml12 32 383038 rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtxa4000 : 1 cml [ 15 - 16 ] 32 383038 rhel8, xeon, 4216, turing gpu : rtx2080ti : 7 cml [ 17 - 28 ], gammagpu05 32 255225 + rhel8, zen, epyc - 7282, ampere gpu : rtxa4000 : 8 cml31 32 384094 rhel8, zen, epyc - 9124, ampere gpu : a100 : 1 cml32 64 512999 rhel8, zen, epyc - 7543, ampere gpu : a100 : 4 cmlcpu [ 01 - 04 ] 20 384271 rhel8, xeon, e5 - 2660 ( null ) gammagpu00 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa5000 : 8 mbrc [ 00 - 01 ] 20 189498 rhel8, xeon, 4114, turing gpu : rtx2080ti : 8 twist [ 00 - 01 ] 8 61727 rhel8, xeon, e5 - 1660 ( null ) legacygpu08 20 513327 rhel8, xeon, e5 - 2640, maxwell gpu : m40 : 2 brigid [ 16 - 17 ] 48 512897 rhel8, zen, epyc - 7443 ( null ) brigid [ 18 - 19 ] 20 61739 rhel8, xeon, e5 - 2640 ( null ) legacygpu06 20 255249 rhel8, xeon, e5 - 2699, maxwell gpu : gtxtitanx : 4 tron [ 00 - 05 ] 32 255233 rhel8, zen, epyc - 7302, ampere gpu : rtxa6000 : 8 tron [ 46 - 61 ] 48 255232 rhel8, zen, epyc - 7352, ampere gpu : rtxa5000 : 8 tron [ 64, 67 ] 32 383028 + rhel8, xeon, 4216, turing, ampere gpu : rtx2080ti : 7, gpu : rtx3070 : 1 vulcan00 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7, gpu : p100 : 1 vulcan [ 01 - 04, 06 - 07 ] 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 8 vulcan05 32 255259 rhel8, xeon, e5 - 2683, pascal gpu : p6000 : 7 janus [ 02 - 04 ] 40 383025 rhel8, xeon, 6248, turing gpu : rtx2080ti : 10 legacygpu00 20 255249 rhel8, xeon, e5 - 2650, pascal gpu : titanxp : 4 legacygpu [ 01 - 02, 07 ] 20 255249 + rhel8, xeon, e5 - 2650, maxwell gpu : gtxtitanx : 4 legacygpu [ 03 - 04 ] 16 255268 rhel8, xeon, e5 - 2630, maxwell gpu : gtxtitanx : 2 legacygpu05 44 513193 rhel8, xeon, e5 - 2699, pascal gpu : gtx1080ti : 4 vulcan23 32 383030 rhel8, xeon, 4612, turing gpu : rtx2080ti : 8 vulcan26 24 770126 rhel8, xeon, 6146, pascal gpu : titanxp : 10 vulcan [ 27 - 28 ] 56 770093 rhel8, xeon, 8280, turing gpu : rtx2080ti : 10 vulcan24 16 126216 rhel8, zen, 7282, ampere gpu : rtxa6000 : 4 gammagpu [ 01 - 04, 06 - 09 ], vulcan [ 33 - 37 ] 32 255215 + rhel8, zen, epyc - 7313, ampere gpu : rtxa5000 : 8 vulcan [ 38 - 44 ]',\n",
       " \"2 legacygpu05 44 513193 rhel8, xeon, e5 - 2699, pascal gpu : gtx1080ti : 4 vulcan23 32 383030 rhel8, xeon, 4612, turing gpu : rtx2080ti : 8 vulcan26 24 770126 rhel8, xeon, 6146, pascal gpu : titanxp : 10 vulcan [ 27 - 28 ] 56 770093 rhel8, xeon, 8280, turing gpu : rtx2080ti : 10 vulcan24 16 126216 rhel8, zen, 7282, ampere gpu : rtxa6000 : 4 gammagpu [ 01 - 04, 06 - 09 ], vulcan [ 33 - 37 ] 32 255215 + rhel8, zen, epyc - 7313, ampere gpu : rtxa5000 : 8 vulcan [ 38 - 44 ] 32 255215 rhel8, zen, epyc - 7313, ampere gpu : rtxa4000 : 8 note that all of the nodes shown by this may not necessarily be in a partition you are able to submit to. you can identify further specific information about a node using scontrol with various flags. there are also two command aliases developed by umiacs staff to show various node information in aggregate. they are show _ nodes and show _ available _ nodes. show _ nodes the show _ nodes command alias shows each node ' s name, number of cpus, memory, { os, cpu architecture, cpu type, gpu architecture ( if the node has gpus ) } ( as avail _ features ), gres ( gpus ), and state. it essentially wraps the sinfo command with some pre - determined output format options and shows each node on its own line, in alphabetical order. to only view nodes in a specific partition, append - p < partition name > to the command alias. examples $ show _ nodes nodelist cpus memory avail _ features gres state brigid16 48 512897 rhel8, x86 _ 64, zen, epyc - 7443 ( null ) idle brigid17 48 512897 rhel8, x86 _ 64, zen, epyc - 7443 ( null ) idle.................. vulcan45 32 513250 rhel8, x86 _ 64, zen, epyc - 7313, ampere gpu : rtxa6000 : 8 idle ( specific partition ) $ show _ nodes - p tron nodelist cpus memory avail _ features gres state tron00 32 255233 rhel8, x86 _ 64, zen, epyc - 7302, ampere gpu : rtxa6000 : 8 idle tron01 32 255233 rhel8, x86 _ 64, zen, epyc - 7302, ampere gpu : rtxa6000 : 8 idle.................. tron69 32 383030 rhel8, x86 _ 64, xeon, 4216, turing gpu : rtx2080ti : 8 idle show _ available _ nodes the show _ available _ nodes command alias takes zero or more arguments that correspond to slurm constructs, resources, or features that you are looking to request a job with and tells you what nodes could theoretically [ 0, 1 ] run a job with these arguments immediately. it assumes your job is a single - node job. these arguments are : - - partition : only include nodes in the specified partition ( s ). - - account : only include nodes from partitions that can use the specified account ( s ). - - qos : only include nodes from partitions that can use the specified qos ( es ). - - cpus : only include nodes with at least this many cpus free. - - mem : only include nodes with at least this much memory free. the default unit is mb if unspecified, but any of { k, m, g, t } can be suffixed to the number provided ( will then be interpreted as kb, mb, gb, or tb, respectively ). gres - related arguments : - - gres, - - and - gres : only include nodes whose list of gres contains all of the specified gres type / quantity pairings. - - or - gres : only include nodes whose list of gres contains any of the specified gres type / quantity pairings. functionally identical to - - and - gres if only one gres type / quantity pairing is specified. gpu - related arguments : - - gpus, - - and - gpus : only include nodes whose list of gpus ( a subset of gres )\",\n",
       " \"- cpus : only include nodes with at least this many cpus free. - - mem : only include nodes with at least this much memory free. the default unit is mb if unspecified, but any of { k, m, g, t } can be suffixed to the number provided ( will then be interpreted as kb, mb, gb, or tb, respectively ). gres - related arguments : - - gres, - - and - gres : only include nodes whose list of gres contains all of the specified gres type / quantity pairings. - - or - gres : only include nodes whose list of gres contains any of the specified gres type / quantity pairings. functionally identical to - - and - gres if only one gres type / quantity pairing is specified. gpu - related arguments : - - gpus, - - and - gpus : only include nodes whose list of gpus ( a subset of gres ) contains all of the specified gpu type / quantity pairings. - - or - gpus : only include nodes whose list of gpus ( a subset of gres ) contains any of the specified gpu type / quantity pairings. functionally identical to - - and - gpus if only one gpu type / quantity pairing is specified. feature - related arguments : - - feature, - - and - feature : only include nodes whose list of features contains all of the specified feature ( s ). - - or - feature : only include nodes whose list of features contains any of the specified feature ( s ). functionally identical to - - and - feature if only one feature is specified. these arguments are also viewable by running show _ available _ nodes - h. if your passed argument set does not contain any resource - based arguments ( cpus / ram / gres or gpus ), a node is defined as available if it has at least 1 cpu and 1mb of ram available. if there are no nodes available that meet your passed argument set, you will receive the message there are no nodes that have currently free resources that meet this argument set. footnotes [ 0 ] - as of now, this command alias does not factor in resources occupied by jobs that could be preempted ( based on the partition ( s ) passed to it, if present ). this is soon to come. [ 1 ] - this command alias also does not factor in jobs with higher priority values requesting more resources, in the same partition ( s ), blocking execution of a job submitted with the arguments checked by the command alias. this is due to the complexity of calculating a job ' s priority value before it is actually submitted. examples show all available nodes : $ show _ available _ nodes brigid17 cpus = 16, mem = 414593m brigid18 cpus = 8, mem = 24875m... show nodes available in the tron partition : $ show _ available _ nodes - - partition tron tron00 cpus = 14, mem = 50433m, gres = gpu : rtxa6000 : 1 tron01 cpus = 10, mem = 17665m, gres = gpu : rtxa6000 : 2... show nodes with one or more rtx a5000 or rtx a6000 gpus available to the vulcan account : $ show _ available _ nodes - - account vulcan - - or - gpus rtxa5000 : 1, rtxa6000 : 1 vulcan32 cpus = 16, mem = 193778m, gres = gpu : rtxa6000 : 4 vulcan33 cpus = 15, mem = 181499m, gres = gpu : rtxa5000 : 3... show nodes with 4 or more cpus, 48g or more memory, and one or more rtx a6000 gpus available in the scavenger partition : $ show _ available _ nodes - - partition = scavenger - - cpus = 4 - - mem = 48g - - or - gpus = rtxa6000 : 1 cbcb27 cpus = 59, mem = 218303m, gres = gpu : rtxa6000 : 6 clip06 cpus = 20, mem = 93448m, gres = gpu : rtxa6000 : 1... show nodes with turing or ampere architecture gpus available in the scavenger partition : $ show _ available _ nodes - - partition = scavenger - - or - feature = ampere, turing cbcb25 cpus = 24, mem = 255278m, gres = gpu : rtx2080ti : 1, gpu : gtx1080ti : 1 cbcb26 cpus = 127, mem = 447707m, gres = gpu : rtxa5000\",\n",
       " '- - partition = scavenger - - cpus = 4 - - mem = 48g - - or - gpus = rtxa6000 : 1 cbcb27 cpus = 59, mem = 218303m, gres = gpu : rtxa6000 : 6 clip06 cpus = 20, mem = 93448m, gres = gpu : rtxa6000 : 1... show nodes with turing or ampere architecture gpus available in the scavenger partition : $ show _ available _ nodes - - partition = scavenger - - or - feature = ampere, turing cbcb25 cpus = 24, mem = 255278m, gres = gpu : rtx2080ti : 1, gpu : gtx1080ti : 1 cbcb26 cpus = 127, mem = 447707m, gres = gpu : rtxa5000 : 7... show nodes with zen architecture cpus and ampere architecture gpus available in the scavenger partition : $ show _ available _ nodes - - partition = scavenger - - and - feature = zen, ampere cbcb26 cpus = 127, mem = 447707m, gres = gpu : rtxa5000 : 7 cbcb27 cpus = 59, mem = 218303m, gres = gpu : rtxa6000 : 6... ( bogus example ) attempt to show nodes available in the bogus partition : $ show _ available _ nodes - - partition = bogus there are no nodes that have currently free resources that meet this argument set. requesting gpus if you need to do processing on a gpu, you will need to request that your job have access to gpus just as you need to request processors or cpu cores. in slurm, gpus are considered \" generic resources \" also known as gres. to request some number of gpus be reserved / available for your job, you can use the flag - - gres = gpu : # ( with the actual number of gpus you want ). if there are multiple types of gpus available in the cluster and you need a specific type, you can provide the type option to the gres flag e. g. - - gres = gpu : rtxa5000 : #. if you do not request a specific type of gpu, you are likely to be scheduled on an older, lower spec \\' d gpu. note that some qoses may have limits on the number of gpus you can request per job, so you may need to specify a different qos to request more gpus. $ srun - - pty - - qos = medium - - gres = gpu : 2 nvidia - smi... wed mar 6 16 : 59 : 39 2024 + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + | nvidia - smi 535. 129. 03 driver version : 535. 129. 03 cuda version : 12. 2 | | - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + | gpu name persistence - m | bus - id disp. a | volatile uncorr. ecc | | fan temp perf pwr : usage / cap | memory - usage | gpu - util compute m. | | | | mig m. | | = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = + = = = = = = = = = = = = = = = = = = = = = = + = = = = = = = = = = = = = = = = = = = = = = | | 0 nvidia geforce rtx 2080 ti off | 00000000 : 3d : 00. 0 off | n / a | | 32 % 23c p8 1w / 250w | 0mib / 11264mib | 0 % default | | | | n / a | + -',\n",
       " 'disp. a | volatile uncorr. ecc | | fan temp perf pwr : usage / cap | memory - usage | gpu - util compute m. | | | | mig m. | | = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = + = = = = = = = = = = = = = = = = = = = = = = + = = = = = = = = = = = = = = = = = = = = = = | | 0 nvidia geforce rtx 2080 ti off | 00000000 : 3d : 00. 0 off | n / a | | 32 % 23c p8 1w / 250w | 0mib / 11264mib | 0 % default | | | | n / a | + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + | 1 nvidia geforce rtx 2080 ti off | 00000000 : 40 : 00. 0 off | n / a | | 32 % 25c p8 1w / 250w | 0mib / 11264mib | 0 % default | | | | n / a | + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + | processes : | | gpu gi ci pid type process name gpu memory | | id id usage | | = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = | | no running processes found | + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + please note that your job will only be able to see / access the gpus you requested. if you only need 1 gpu, please only request 1 gpu. the others on the node ( if any ) will be left available for other users. $ srun - - pty - - gres = gpu : rtxa5000 : 1 nvidia - smi thu aug 25 15 : 22 : 15 2022 + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + | nvidia - smi 470. 129. 06 driver version : 470. 129. 06 cuda version : 11. 4 | | - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + | gpu name persistence - m | bus - id disp.',\n",
       " '+ - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + | nvidia - smi 470. 129. 06 driver version : 470. 129. 06 cuda version : 11. 4 | | - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + | gpu name persistence - m | bus - id disp. a | volatile uncorr. ecc | | fan temp perf pwr : usage / cap | memory - usage | gpu - util compute m. | | | | mig m. | | = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = + = = = = = = = = = = = = = = = = = = = = = = + = = = = = = = = = = = = = = = = = = = = = = | | 0 nvidia rtx a5000 off | 00000000 : 01 : 00. 0 off | off | | 30 % 23c p8 20w / 230w | 0mib / 24256mib | 0 % default | | | | n / a | + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - - - - - - - - + + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + | processes : gpu memory | | gpu pid type process name usage | | = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = | | no running processes found | + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + as with all other flags, the - - gres flag may also be passed to sbatch and salloc rather than directly to srun. mpi example to run mpi jobs, you will need to include the - - mpi = pmix flag in your submission arguments. #! / usr / bin / bash # sbatch - - job - name = mpi _ test # job name # sbatch - - nodes = 4 # number of nodes # sbatch - - ntasks = 8 # number of mpi ranks # sbatch - - ntasks - per - node = 2 # number of mpi ranks per node # sbatch - - ntasks - per - socket = 1 # number of tasks per processor socket on the node # sbatch - - time = 00 : 30 : 00 # time limit hrs : min : sec srun - - mpi = pmix / nfshomes / username / testing / mpi / a. out retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / jobsubmission & oldid = 11991 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this',\n",
       " 'ntasks = 8 # number of mpi ranks # sbatch - - ntasks - per - node = 2 # number of mpi ranks per node # sbatch - - ntasks - per - socket = 1 # number of tasks per processor socket on the node # sbatch - - time = 00 : 30 : 00 # time limit hrs : min : sec srun - - mpi = pmix / nfshomes / username / testing / mpi / a. out retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / jobsubmission & oldid = 11991 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 13 august 2024, at 16 : 21. privacy policy about umiacs disclaimers',\n",
       " 'slurm / preemption - umiacs slurm / preemption from umiacs jump to navigation jump to search if you are submitting to a partition which is eligible for preemption ( e. g. scavenger ), you are responsible for making sure that your job can be interrupted / restarted gracefully. here we are documenting some slurm behaviors that you can use to determine if your job has been cancelled or preempted. you should be able to take different code paths during startup / shutdown of your jobs based on this information. contents 1 flowchart for job cancellation / preemption 1. 1 cancellation 1. 2 preemption / requeue 2 key takeaways 2. 1 on job cancellation / preemption 2. 2 on resuming 3 testing flowchart for job cancellation / preemption cancellation slurm controller sends an internal cancel signal to slurm node ( s ) where the job is currently assigned. slurm node ( s ) send sigcont and sigterm around the same time, and the following fields of note are set in the output of scontrol - - json show job $ slurm _ jobid : job _ state = [ \\' cancelled \\', \\' completing \\' ] if the processes don \\' t stop within a certain amount of time, eventually sigkill will be sent. preemption / requeue slurm controller sends an internal preemption / requeue signal to slurm node ( s ) where the job is currently assigned. slurm node ( s ) send sigcont and sigterm around the same time, and the following fields of note are set in the output of scontrol - - json show job $ slurm _ jobid : job _ state = [ \\' pending \\', \\' completing \\' ] restart _ cnt + = 1 if the processes don \\' t stop within a certain amount of time, eventually sigkill will be sent. once the job is stopped, it enters the pending state for two minutes, and then is eligible to be run again. when the job runs again, an additional environment variable will be defined, slurm _ restart _ count, which reports the number of times the job has been preempted / requeued. key takeaways on job cancellation / preemption you can handle the sigterm signal and run scontrol - - json show job $ slurm _ jobid within your script. based on the value of job _ state, your script can take a different codepath depending on if it was cancelled or was preempted. the output of scontrol - - json show job $ slurm _ jobid is equivalent to the slurm rest api \\' s endpoint \" get / slurm / v0. 0. 40 / job / { job _ id } \". for more information on how to parse the output of this scontrol command, please refer to slurm \\' s rest api documentation. [ 1 ] on resuming if your job needs to behave differently based on whether or not it was previously preempted, you can check if the environment variable slurm _ restart _ count is defined. testing you can use the following commands to manually cancel and requeue your jobs ( requeueing and preemption are handled similarly ). with these tools, you can test your scripts to ensure that they gracefully handle these scenarios. cancellation : scancel < job id > preemption : scontrol requeue < job id > retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / preemption & oldid = 11792 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 10 may 2024, at 15 : 54. privacy policy about umiacs disclaimers',\n",
       " 'facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 10 may 2024, at 15 : 54. privacy policy about umiacs disclaimers',\n",
       " 'slurm / priority - umiacs slurm / priority from umiacs jump to navigation jump to search slurm at umiacs is configured to prioritize jobs based on a number of factors, termed multifactor priority in slurm. each job submitted to the scheduler is assigned a priority value, which can be viewed in the output of scontrol show job < jobid >. example : $ scontrol show job 1 jobid = 1 jobname = bash userid = username ( 13337 ) groupid = username ( 13337 ) mcs _ label = n / a priority = 10841 nice = 0 account = nexus qos = default... contents 1 pending jobs 2 priority factors 2. 1 age 2. 2 association 2. 3 partition 2. 4 fair - share 2. 4. 1 gpu partitions 2. 4. 2 cpu - only partitions 2. 5 nice value pending jobs if the partition that you submit your job to cannot instantly begin your job due to no compute node ( s ) having the resources free to run it, your job will remain in the pending state with the listed reason ( resources ). if there is another job already pending with this reason, you submit a job to the same partition, and your job gets assigned a lower priority value than that pending job, your job will instead remain in the pending state with reason ( priority ). if there are multiple jobs pending and your job is not the highest priority job pending, the scheduler will only begin execution of your job if starting your job would not push the begin times for any higher priority jobs in the same partition further back. lowering some combination of the resources you are requesting and / or the time limit may allow submitted jobs to run more quickly or instantly during times where a partition is under resource pressure. the command squeue - j < jobid > - - start can be used to provide a time estimate for when your job will start, where < jobid > is the job id you receive from either srun or sbatch. you can use the command alias show _ available _ nodes with a variety of different submission arguments to get a better idea of what jobs may be able to begin sooner. priority factors the priority factors in use at umiacs include : age of job i. e. time spent waiting to run in the queue association ( slurm account ) being used partition job was submitted to fair - share of resources \" nice \" value that job was submitted with age the longer a job is eligible to run but cannot due to resources being unavailable, the higher the job \\' s priority becomes as it continue to wait in the queue. the priority modifier for this factor reaches its limit after 7 days. association some lab / center - specific slurm accounts may have priority values directly attached to them. jobs run under these accounts gain this many extra points of priority. partition the partition named scavenger on each of our clusters always has a lower priority factor for its jobs than all other partitions on that cluster. as mentioned in other umiacs cluster - specific documentation, jobs submitted to this partition are also preemptable. these two design choices give the partition its name ; jobs submitted to the scavenger partition \" scavenge \" for available resources on the cluster rather than consume dedicated resources, and are interrupted by jobs asking to consume dedicated resources. on nexus, labs / centers may also have their own scavenger partitions ( < labname > - scavenger ) if the faculty for the lab / center have decided upon some sort of limit on jobs ( number of simultaneous jobs, number of actively consumed billing resources, etc. ) in their non - scavenger partitions. these lab / center scavenger partitions allow for more jobs to be run by members of that lab / center on that lab \\' s / center \\' s nodes only, but are preemptable by that lab \\' s / center \\' s non - scavenger partition jobs. in decreasing order of priority ( highest first ), our priority tiers for partitions are : account - specific non - preemptable partitions lab / center - specific non - \" scavenger \" named partitions lab / center - specific \" scavenger \" named partitions institute - wide scavenger partition a job in a lower priority tier will never have a higher priority value than any job in any of the higher priority tiers. fair - share the more resources your jobs have already consumed within an account, the lower priority factor your future jobs will have when compared to other users \\' jobs in the same account who have used fewer resources ( so as to \" fair - share \" with other users ). additionally, if there are multiple accounts that can submit to a partition, and the sum of resources of all users \\' jobs within account a is greater than the sum of resources of all users \\' jobs within account b, the lower priority factor all future jobs from users in account a will have',\n",
       " 'order of priority ( highest first ), our priority tiers for partitions are : account - specific non - preemptable partitions lab / center - specific non - \" scavenger \" named partitions lab / center - specific \" scavenger \" named partitions institute - wide scavenger partition a job in a lower priority tier will never have a higher priority value than any job in any of the higher priority tiers. fair - share the more resources your jobs have already consumed within an account, the lower priority factor your future jobs will have when compared to other users \\' jobs in the same account who have used fewer resources ( so as to \" fair - share \" with other users ). additionally, if there are multiple accounts that can submit to a partition, and the sum of resources of all users \\' jobs within account a is greater than the sum of resources of all users \\' jobs within account b, the lower priority factor all future jobs from users in account a will have when compared to all future jobs from users in account b. ( in other words, fair - share is hierarchical. ) you can view the various fair - share statistics with the command sshare - l. it will show your specific fairshare values ( always between 0. 0 and 1. 0 ) within accounts that you have access to. you can also view other accounts \\' level fairshare ( levelfs ). account user rawshares normshares rawusage normusage effectvusage fairshare levelfs grptresmins tresrunmins - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - root 0. 000000 66034847484 1. 000000 cpu = 7746109, mem = 69754856514, e + cbcb 1 0. 032258 14115111102 0. 213757 0. 213757 0. 150910 cpu = 4969, mem = 20355003, energy = + class 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + clip 1 0. 032258 1568122041 0. 023733 0. 023733 1. 359207 cpu = 70083, mem = 1464478788, ener + cml 1 0. 032258 17338485 0. 000263 0. 000263 122. 854754 cpu = 29958, mem = 245415936, energ + cml - abhinav 1 0. 032258 784250 0. 000012 0. 000012 2. 7161e + 03 cpu = 0, mem = 0, energy = 0, node = 0, b + cml - cameron 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + cml - furongh 1 0. 032258 2098793815 0. 031784 0. 031784 1. 014924 cpu = 940758, mem = 8995575569, ene + cml - hajiagha 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + cml - john 1 0. 032258 258872094 0. 003920 0. 003920 8. 228447 cpu = 476993, mem = 5494963200, ene + cml - ramani 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + cml - scavenger 1 0. 032258 6734023027 0. 101979 0. 101979 0. 316321 cpu = 1496736,',\n",
       " '= 940758, mem = 8995575569, ene + cml - hajiagha 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + cml - john 1 0. 032258 258872094 0. 003920 0. 003920 8. 228447 cpu = 476993, mem = 5494963200, ene + cml - ramani 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + cml - scavenger 1 0. 032258 6734023027 0. 101979 0. 101979 0. 316321 cpu = 1496736, mem = 13036434773, e + cml - sfeizi 1 0. 032258 185510632 0. 002809 0. 002809 11. 482444 cpu = 70732, mem = 579442005, energ + cml - tokekar 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + cml - tomg 1 0. 032258 99040108 0. 001500 0. 001500 21. 507603 cpu = 0, mem = 0, energy = 0, node = 0, b + cml - zhou 1 0. 031250 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + gamma 1 0. 032258 8880343229 0. 134482 0. 134482 0. 239869 cpu = 2532358, mem = 23460226867, e + mbrc 1 0. 032258 27060567 0. 000410 0. 000410 78. 716582 cpu = 0, mem = 0, energy = 0, node = 0, b + mc2 1 0. 032258 9175 0. 000000 0. 000000 2. 3215e + 05 cpu = 0, mem = 0, energy = 0, node = 0, b + nexus 1 0. 032258 3346084300 0. 050672 0. 050672 0. 636599 cpu = 121941, mem = 1468973003, ene + nexus username 1 0. 000779 69666 0. 000001 0. 000021 0. 457407 37. 435501 cpu = 0, mem = 0, energy = 0, node = 0, b + scavenger 1 0. 032258 21762190063 0. 329562 0. 329562 0. 097882 cpu = 1085904, mem = 4775150199, en + scavenger username 1 0. 000779 171 0. 000000 0. 000000 0. 033975 9. 8885e + 04 cpu = 0, mem = 0, energy = 0, node = 0, b + vulcan 1 0. 032258 1458631376 0. 022089 0. 022089 1. 460352 cpu = 25968, mem = 106368204, energ + vulcan - abhinav 1 0. 032258 4441051354 0. 067254 0. 067254 0. 479648 cpu = 850445, mem = 9471827285, ene + vulcan - djacobs 1 0. 032258 381503730 0. 005777 0. 005777 5. 583472 cpu = 7656, mem = 250882730, energy + vulcan - janus 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + vulcan - jbhuang 1 0. 032258 15619477 0. 000237 0. 000237 136. 375587 cpu = 0, mem = 0, energy = 0, node = 0, b + vulcan - lsd 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0',\n",
       " \". 479648 cpu = 850445, mem = 9471827285, ene + vulcan - djacobs 1 0. 032258 381503730 0. 005777 0. 005777 5. 583472 cpu = 7656, mem = 250882730, energy + vulcan - janus 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + vulcan - jbhuang 1 0. 032258 15619477 0. 000237 0. 000237 136. 375587 cpu = 0, mem = 0, energy = 0, node = 0, b + vulcan - lsd 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + vulcan - metzler 1 0. 032258 435471075 0. 006595 0. 006595 4. 891520 cpu = 16235, mem = 133000942, energ + vulcan - rama 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + vulcan - ramani 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + vulcan - yaser 1 0. 032258 209285667 0. 003166 0. 003166 10. 189036 cpu = 15366, mem = 251762005, energ + vulcan - zwicker 1 0. 032258 0 0. 000000 0. 000000 inf cpu = 0, mem = 0, energy = 0, node = 0, b + the actual resource billing weights for the three main resources ( memory per gb, cpu cores, and number of gpus if applicable ) are per - partition and can be viewed in the tresbillingweights line in the output of scontrol show partition. the billing value for a job is the sum of all resource weightings for resources the job has requested. this value is then multiplied by the amount of time a job has run in seconds to get the amount it contributes to the rawusage for the association within the account it is running under. the algorithm we use for resource weightings differs depending on if there are any gpus in a partition or not, and is as follows : gpu partitions each resource ( memory / cpu / gpu ) is given a weighting value such that their relative billings to each other within the partition are equal ( 33. 33 % each ). memory is typically always the most abundant resource by unit ( weighting value of 1. 0 per gb ) and the cpu / gpu values are adjusted accordingly. different gpu types may also be weighted differently within the gpu relative billing. a baseline gpu type is first chosen. all gpus of that type and other types that have lower fp32 performance ( in tflops ) are given a weighting factor of 1. 0. gpu types with higher fp32 performance than the baseline gpu are given a weighting factor calculated by dividing their fp32 performance by the baseline gpu ' s fp32 performance. the weighting values for each gpu type are then determined by normalizing the sum of all of gpu cards ' billing values multiplied by their weighting factors against the relative billing percentage for gpus ( 33. 33 % ). the current baseline gpu is the nvidia rtx a4000. cpu - only partitions each resource ( memory / cpu ) is first given a weighting value such that their relative billings to each other within the partition are equal ( 50 % each ). memory is typically always the most abundant resource by unit ( weighting value of 1. 0 per gb ) and the cpu value is adjusted accordingly. the final cpu weight value is then divided by 10, which ends up translating to roughly 90. 9 % of the billing weight being for memory and 9. 1 % being for cpu. the division of the cpu value is done so as to not affect accounts ' fair - share priority factors as much when running cpu - only jobs given the popularity of gpu computing. nice value this is a submission argument that you as the user can include when submitting your jobs to deprioritize them. larger values will deprioritize jobs more e. g., srun - - pty - - nice = 2 bash will have lower priority than srun - - pty - - nice = 1 bash which will have lower priority than srun - - pt\",\n",
       " 'relative billings to each other within the partition are equal ( 50 % each ). memory is typically always the most abundant resource by unit ( weighting value of 1. 0 per gb ) and the cpu value is adjusted accordingly. the final cpu weight value is then divided by 10, which ends up translating to roughly 90. 9 % of the billing weight being for memory and 9. 1 % being for cpu. the division of the cpu value is done so as to not affect accounts \\' fair - share priority factors as much when running cpu - only jobs given the popularity of gpu computing. nice value this is a submission argument that you as the user can include when submitting your jobs to deprioritize them. larger values will deprioritize jobs more e. g., srun - - pty - - nice = 2 bash will have lower priority than srun - - pty - - nice = 1 bash which will have lower priority than srun - - pty bash assuming all three jobs were submitted at the same time. you cannot use negative values for this argument. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / priority & oldid = 11955 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 19 july 2024, at 17 : 53. privacy policy about umiacs disclaimers']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = [chunk for f in chosenfiles for chunk in chunk_text_with_overlap(get_html(f), max_tokens=1024, overlap=200, tokenizer=tokenizer_big)]\n",
    "# above ~1000 for this specific question makes the retrievals bad\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_core.vectorstores.in_memory.InMemoryVectorStore at 0x1d4ce21a260>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = InMemoryVectorStore(embedding_model_big)\n",
    "vector_store.add_texts(chunks)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='242f79d4-30d6-403c-9fc5-e93049b7c789', page_content=\". running jupyter notebook on a compute node the steps to run a jupyter notebook from a compute node are listed below. setting up your python virtual environment create a python virtual environment on the compute node you are assigned and activate it. next, install jupyter using pip by following the steps here. you may also use other environment management systems such as conda if desired. running jupyter notebook after you ' ve set up the python virtual environment, submit a job, activate the environment within the job, and run the following command on the compute node you are assigned : jupyter notebook - - no - browser - - port = 8889 - - ip = 0. 0. 0. 0 this will start running the notebook on port 8889. note : you must keep this shell window open to be able to connect. if the submission node for the cluster you are using is not accessible via the public internet, you must also be on a machine connected to the umiacs network or connected to our vpn in order to access the jupyter notebook once you start the ssh tunnel, so ensure this is the case before starting the tunnel. then, on your local machine, run ssh - n - f - l localhost : 8888 : < nodename > : 8889 < username > @ < submissionnode >. umiacs. umd. edu this will tunnel port 8889 from the compute node to port 8888 on your local machine, using < submissionnode > as an intermediate node. make sure to replace < username > with your username, < submissionnode > with the name of the submission node you want to use, and < nodename > with the name of the compute node you are assigned. note that this command will not display any output if the connection is successful due to the included ssh flags. you must also keep this shell window open to be able to connect. for example, assuming your username is username and that you are using the nexus cluster, have been assigned the nexusgroup submission nodes, and are assigned compute node tron00. umiacs. umd. edu : ssh - n - f - l localhost : 8888 : tron00. umiacs. umd. edu : 8889 username @ nexusgroup. umiacs. umd. edu you can then open a web browser and type in localhost : 8888 to access the notebook. notes : later versions of jupyter have token authentication enabled by default - you will need to prepend the /? token = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx part of the url provided by the terminal output after starting the notebook in order to connect if this is the case. e. g. localhost : 8888 /? token = fcc6bd0f996e7aa89376c33cb34f7b80890502aacc97d98e if the port on the compute node mentioned in the example above ( 8889 ) is not working, it may be that someone else has already started a process ( jupyter notebook or otherwise ) using that specific port number on that specific compute node. the port number can be replaced with any other ephemeral port number you ' d like, just make sure to change it in both the command you run on the compute node and the ssh command from your local machine. quick guide to translate pbs / torque to slurm pbs / torque was the previous workload manager and job submission framework used at umiacs prior to slurm ' s adoption. below is a quick guide of how to translate some common pbs / torque commands to slurm ones. user commands pbs / torque slurm job submission qsub [ filename ] sbatch [ filename ] job deletion qdel [ job _ id ] scancel [ job _ id ] job status ( by job ) qstat [ job _ id ] squeue - - job [ job _ id ] full job status ( by job ) qstat - f [ job _ id ] scontrol show job [ job _ id ] job status ( by user ) qstat - u [ username ] squeue - - user = [ username ] environment variables pbs / torque slurm job id $ pbs _ jobid $ slurm _ jobid submit directory $ pbs _ o _ workdir $ slurm _ submit _ dir node list $ pbs _ nodefile $ slurm _ job _ nodelist job specification pbs / torque slurm script directive # pbs # sbatch job name - n [ name ] - - job - name = [ name ] or - j [ name ] node count - l nodes = [ count\"),\n",
       " Document(id='8b63c936-cc19-4158-8f7c-810214805748', page_content=\"gpu 6 : nvidia rtx a5000 ( uuid : gpu - 8d20d6cd - abf5 - 2630 - ab88 - 6bba438c55fe ) gpu 7 : nvidia rtx a5000 ( uuid : gpu - 93170910 - 5d94 - 6da5 - 8a24 - f561d7da1e2d ) you can also use sbatch to submit your job. here are two examples on how to do that. $ sbatch - - pty - - gres = gpu : 8 - - account = gamma - - partition = gamma - - qos = huge - long - - time = 1 - 23 : 00 : 00 script. sh or $ sbatch script. sh / / script. sh / / #! / bin / bash # sbatch - - gres = gpu : 8 # sbatch - - account = gamma # sbatch - - partition = gamma # sbatch - - qos = huge - long # sbatch - - time = 1 - 23 : 00 : 00 python your _ file. py storage there are 3 types of user storage available to users in gamma : home directories project directories scratch directories there is also read - only storage available for dataset directories. gamma users can also request nexus project allocations. home directories you have 30gb of home directory storage available at / nfshomes / < username >. it has both snapshots and backups enabled. home directories are intended to store personal or configuration files only. we encourage you to not share any data in your home directory. you are encouraged to utilize our gitlab infrastructure to host your code repositories. note : to check your quota on this directory, use the command df - h ~. project directories you can request project based allocations for up to 8tb and up to 180 days with approval from a gamma faculty member. to request an allocation, please contact staff with the faculty member ( s ) that approved the project in the conversation. please include the following details : project name ( short ) description size ( 1tb, 2tb, etc. ) length in days ( 30 days, 90 days, etc. ) other user ( s ) that need to access the allocation, if any these allocations will be available from / fs / gamma - projects under a name that you provide when you request the allocation. near the end of the allocation period, staff will contact you and ask if you would like to renew the allocation ( requires re - approval from a gamma faculty member ). if you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period. staff will then remove the allocation. if you do not respond to staff ' s request by the end of the allocation period, staff will make the allocation temporarily inaccessible. if you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible. if one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation. this data is backed up nightly. scratch directories scratch data has no data protection, there are no snapshots and the data is not backed up. there are two types of scratch directories : network scratch directory local scratch directories network scratch directory you are allocated 100gb of scratch space via nfs from / gammascratch / $ username. it is not backed up or protected in any way. this directory is automounted so you may not see your directory if you run ls / gammascratch but it will be mounted when you cd into your / gammascratch directory. you may request a permanent increase of up to 200gb total space without any faculty approval by contacting staff. if you need space beyond 200gb, you will need faculty approval. this file system is available on all submission, data management, and computational nodes within the cluster. local scratch directories these file systems are not available over nfs and there are no backups or snapshots available for these file systems. each computational node that you can schedule compute jobs on has one or more local scratch directories. these are always named / scratch0, / scratch1, etc. these directories are local to each node, ie. the / scratch0 on two different nodes are completely separate. these directories are almost always more performant than any other storage available to the job. however, you must stage data to these directories within the confines of your jobs and stage the data out before the end of your jobs. these local scratch directories have a tmpwatch job which will delete unaccessed data after 90 days, scheduled via maintenance jobs\"),\n",
       " Document(id='783dcd7e-da73-4fb3-b3a5-415a5a83f91d', page_content='/ derek / pytorch _ docker info : converting oci blobs to sif format info : starting build... getting image source signatures copying blob 85386706b020 done... 2022 / 10 / 14 10 : 58 : 36 info unpack layer : sha256 : b6f46848806c8750a68edc4463bf146ed6c3c4af18f5d3f23281dcdfb1c65055 2022 / 10 / 14 10 : 58 : 43 info unpack layer : sha256 : 44845dc671f759820baac0376198141ca683f554bb16a177a3cfe262c9e368ff info : creating sif file... $ apptainer exec - - nv pytorch _ docker. sif python3 - c \\' from _ _ future _ _ import print _ function ; import torch ; print ( torch. cuda. current _ device ( ) ) ; x = torch. rand ( 5, 3 ) ; print ( x ) \\' 0 tensor ( [ [ 0. 3273, 0. 7174, 0. 3587 ], [ 0. 2250, 0. 3896, 0. 4136 ], [ 0. 3626, 0. 0383, 0. 6274 ], [ 0. 6241, 0. 8079, 0. 2950 ], [ 0. 0804, 0. 9705, 0. 0030 ] ] ) retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = apptainer & oldid = 11997 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 13 august 2024, at 17 : 51. privacy policy about umiacs disclaimers'),\n",
       " Document(id='769d6bd5-d9f4-4e88-bdd1-7ee5205a8ad4', page_content=\"nexus / cml - umiacs nexus / cml from umiacs jump to navigation jump to search the compute nodes from cml ' s previous standalone cluster have folded into nexus as of the scheduled maintenance window for august 2023 ( thursday 08 / 17 / 2023, 5 - 8pm ). the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 usage 2 partitions 3 accounts 4 qos 5 storage 5. 1 home directories 5. 2 project directories 5. 3 scratch directories 5. 3. 1 network scratch directory 5. 3. 2 local scratch directories 5. 4 datasets 5. 5 models usage you can ssh to nexuscml. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexuscml00. umiacs. umd. edu nexuscml01. umiacs. umd. edu all partitions, qoses, and account names from the standalone cml cluster have been moved over to nexus. however, please note that cml - is prepended to all of the values that were present in the standalone cml cluster to distinguish them from existing values in nexus. the lone exception is the base account that was named cml in the standalone cluster ( it is also named just cml in nexus ). here are some before / after examples of job submission with various parameters : standalone cml cluster submission command nexus cluster submission command srun - - partition = dpart - - qos = medium - - account = tomg - - gres = gpu : rtx2080ti : 2 - - pty bash srun - - partition = cml - dpart - - qos = cml - medium - - account = cml - tomg - - gres = gpu : rtx2080ti : 2 - - pty bash srun - - partition = cpu - - qos = cpu - - pty bash srun - - partition = cml - cpu - - qos = cml - cpu - - account = cml - - pty bash srun - - partition = scavenger - - qos = scavenger - - account = scavenger - - gres = gpu : 4 - - pty bash srun - - partition = cml - scavenger - - qos = cml - scavenger - - account = cml - scavenger - - gres = gpu : 4 - - pty bash cml users ( exclusively ) can schedule non - interruptible jobs on cml nodes with any non - scavenger job parameters. please note that the cml - dpart partition has a grptres limit of 100 % of the available cores / ram on all cml # # nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. it also has a max submission limit of 500 jobs per user simultaneously so as to not overload the cluster. this is codified by the partition qos named cml. please note that the cml compute nodes are also in the institute - wide scavenger partition in nexus. cml users still have scavenging priority over these nodes via the cml - scavenger partition ( i. e., all cml - partition jobs ( other than cml - scavenger ) can preempt both cml - scavenger and scavenger partition jobs, and cml - scavenger partition jobs can preempt scavenger partition jobs ). partitions there are three partitions available to general cml slurm users. you must specify a partition when submitting your job. cml - dpart - this is the default partition. job allocations are guaranteed. cml - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs in other cml - partitions are ready to be scheduled. cml - cpu - this partition is for cpu focused jobs. job allocations are guaranteed. there are two additional partitions available solely to specific faculty members and their sponsored accounts. cml - furongh - this partition is for exclusive priority access to dr. furong huang ' s purchased a6000 node. job allocations are guaranteed. cml - zhou - this partition is for exclusive priority access to dr. tianyi zhou\"),\n",
       " Document(id='dca74e13-1ea1-4467-b8b7-aaca27f869b5', page_content='##y within the image. $ hostname & & nvidia - smi - l tron38. umiacs. umd. edu gpu 0 : nvidia rtx a4000 ( uuid : gpu - 4a0a5644 - 9fc8 - 84b4 - 5d22 - 65d45ca36506 ) $ apptainer run - - nv / fs / nexus - containers / pytorch / pytorch _ 1. 13. 0 + cu117. sif 99 984. 5538940429688 199 654. 1710815429688 299 435. 662353515625 399 291. 1429138183594 499 195. 5575714111328 599 132. 3363037109375 699 90. 5206069946289 799 62. 86213684082031 899 44. 56754684448242 999 32. 466392517089844 1099 24. 461835861206055 1199 19. 166893005371094 1299 15. 6642427444458 1399 13. 347112655639648 1499 11. 814264297485352 1599 10. 800163269042969 1699 10. 129261016845703 1799 9. 685370445251465 1899 9. 391674041748047 1999 9. 19735336303711 result : y = 0. 0022362577728927135 + 0. 837898313999176 x + - 0. 0003857926349155605 x ^ 2 + - 0. 09065020829439163 x ^ 3 bind mounts to get data into the container you need to pass some bind mounts. apptainer containers will not automatically mount data from the outside operating system other than your home directory. users need to manually bind mounts for other file paths. - - bind / fs / nexus - scratch / < username > / < projectname > : / mnt in this example, we will exec an interactive session with gpus and binding our nexus scratch directory which allows us to specify the command we want to run inside the container. apptainer exec - - nv - - bind / fs / nexus - scratch / username : / fs / nexus - scratch / username / fs / nexus - containers / pytorch / pytorch _ 1. 13. 0 + cu117. sif bash you can now write / run your own pytorch python code interactively within the container or just make a python script that you can call directly from the apptainer exec command for batch processing. shared containers portable images called singularity image format or. sif files can be copied and shared. nexus maintains some shared containers in / fs / nexus - containers. these are arranged by the application ( s ) that are installed. docker workflow example we have a pytorch _ docker example workflow using our gitlab as a docker registry. you can clone the repository and further customize this to your needs. the workflow is : run docker on a laptop or personal desktop on to create the image, or use podman on a umiacs - supported system. tag the image and and push it to your repository ( this can be any docker registry ) pull the image down onto one of our workstations / clusters and run it with your data. $ apptainer pull pytorch _ docker. sif docker : / / registry. umiacs. umd. edu / derek / pytorch _ docker info : converting oci blobs to sif format info : starting build... getting image source signatures copying blob 85386706b020 done... 2022 / 10 / 14 10 : 58 : 36 info unpack layer : sha256 : b6f46848806c8750a68edc4463bf146ed6c3c4af18f5d3f23281dcdfb1c65055 2022 / 10 / 14 10 : 58 : 43 info unpack layer : sha256 : 44845dc671f759820baac0376198141ca683f554bb16a177a3cfe262c9e368ff info : creating sif file... $ apptainer exec - - nv'),\n",
       " Document(id='b3c93573-6033-4e41-bc17-e5d84be791ca', page_content=\"nexus / vulcan - umiacs nexus / vulcan from umiacs jump to navigation jump to search the compute nodes from vulcan ' s previous standalone cluster have folded into nexus as of the scheduled maintenance window for august 2023 ( thursday 08 / 17 / 2023, 5 - 8pm ). the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 usage 2 nodes 3 partitions 4 accounts 5 qos 6 storage 6. 1 home directories 6. 2 scratch directories 6. 2. 1 network scratch directory 6. 2. 2 local scratch directories 6. 3 datasets 6. 4 project storage 6. 5 object storage 7 migration 7. 1 home directories usage you can ssh to nexusvulcan. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexusvulcan00. umiacs. umd. edu nexusvulcan01. umiacs. umd. edu all partitions, qoses, and account names from the standalone vulcan cluster have been moved over to nexus. however, please note that vulcan - is prepended to all of the values that were present in the standalone vulcan cluster to distinguish them from existing values in nexus. the lone exception is the base account that was named vulcan in the standalone cluster ( it is also named just vulcan in nexus ). here are some before / after examples of job submission with various parameters : standalone vulcan cluster submission command nexus cluster submission command srun - - partition = dpart - - qos = medium - - account = abhinav - - gres = gpu : gtx1080ti : 2 - - pty bash srun - - partition = vulcan - dpart - - qos = vulcan - medium - - account = vulcan - abhinav - - gres = gpu : gtx1080ti : 2 - - pty bash srun - - partition = cpu - - qos = cpu - - pty bash srun - - partition = vulcan - cpu - - qos = vulcan - cpu - - account = vulcan - - pty bash srun - - partition = scavenger - - qos = scavenger - - account = vulcan - - gres = gpu : 4 - - pty bash srun - - partition = vulcan - scavenger - - qos = vulcan - scavenger - - account = vulcan - - gres = gpu : 4 - - pty bash vulcan users ( exclusively ) can schedule non - interruptible jobs on vulcan nodes with any non - scavenger job parameters. please note that the vulcan - dpart partition has a grptres limit of 100 % of the available cores / ram on all vulcan # # in aggregate nodes plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. it also has a max submission limit of 500 jobs per user simultaneously so as to not overload the cluster. this is codified by the partition qos named vulcan. please note that the vulcan compute nodes are also in the institute - wide scavenger partition in nexus. vulcan users still have scavenging priority over these nodes via the vulcan - scavenger partition ( i. e., all vulcan - partition jobs ( other than vulcan - scavenger ) can preempt both vulcan - scavenger and scavenger partition jobs, and vulcan - scavenger partition jobs can preempt scavenger partition jobs ). nodes there are currently 46 gpu nodes available running a mixture of nvidia rtx a6000, nvidia rtx a5000, nvidia rtx a4000, nvidia quadro p6000, nvidia geforce gtx 1080 ti, nvidia geforce rtx 2080 ti, and nvidia tesla p100 cards. there are also 4 cpu - only nodes available. all nodes are scheduled with the slurm resource manager. partitions there are three partitions available to general vulcan slurm users. you must specify a partition when submitting your job. vulcan - dpart - this is the default partition. job allocations are guaranteed. only nodes with gpus from architectures before nvidia ' s ampere architecture are included in this partition. vulcan - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs\"),\n",
       " Document(id='f19ae39a-4386-48c2-a4b1-d2da53f7f4d3', page_content=\"nexus - umiacs nexus from umiacs jump to navigation jump to search the nexus is the combined scheduler of resources in umiacs. the resource manager for nexus is slurm. resources are arranged into partitions where users are able to schedule computational jobs. users are arranged into a number of slurm accounts based on faculty, lab, or center investments. contents 1 getting started 1. 1 access 1. 2 jobs 1. 2. 1 interactive 1. 2. 2 batch 2 partitions 3 quality of service ( qos ) 3. 1 job qos 3. 2 partition qos 4 storage 4. 1 home directories 4. 2 scratch directories 4. 2. 1 network scratch directories 4. 2. 2 local scratch directories 4. 3 faculty allocations 4. 4 project allocations 4. 5 datasets getting started all accounts in umiacs are sponsored. if you don ' t already have a umiacs account, please see accounts for information on getting one. you need a full umiacs account ( not a collaborator account ) in order to access nexus. access your access to submission nodes ( alternatively called login nodes ) for nexus computational resources is determined by your account sponsor ' s department, center, or lab affiliation. you can log into the umiacs directory cr application and select the computational resource ( cr ) in the list that has the prefix nexus. the hosts section lists your available submission nodes - generally a pair of nodes of the format nexus < department, lab, or center abbreviation > [ 00, 01 ], e. g., nexusgroup00 and nexusgroup01. note - umiacs requires multi - factor authentication through our duo instance. this is completely discrete from both umd ' s and csd ' s duo instances. you will need to enroll one or more devices to access resources in umiacs, and will be prompted to enroll when you log into the directory application for the first time. once you have identified your submission nodes, you can ssh directly into them. from there, you are able to submit to the cluster via our slurm workload manager. you need to make sure that your submitted jobs have the correct account, partition, and qos. jobs slurm jobs are submitted by either srun or sbatch depending if you are doing an interactive job or batch job, respectively. you need to provide the where / how / who to run the job and specify the resources you need to run with. for the who / where / how, you may be required to specify - - account, - - partition, and / or - - qos ( respectively ) to be able to adequately submit jobs to the nexus. for resources, you may need to specify - - time for time, - - ntasks for cpus, - - mem for ram, and - - gres = gpu for gpus in your submission arguments to meet your requirements. there are defaults for all four, so if you don ' t specify something, you may be scheduled with a very minimal set of time and resources ( e. g., by default, no gpus are included if you do not specify - - gres = gpu ). for more information about submission flags for gpu resources, see here. you can also can run man srun on your submission node for a complete list of available submission arguments. for a list of available gpu types on nexus and their specs, please see nexus / gpus. interactive once logged into a submission node, you can run simple interactive jobs. if your session is interrupted from the submission node, the job will be killed. as such, we encourage use of a terminal multiplexer such as tmux. $ srun - - pty - - ntasks = 4 - - mem = 2gb - - gres = gpu : 1 nvidia - smi - l gpu 0 : nvidia rtx a4000 ( uuid : gpu - ae5dc1f5 - c266 - 5b9f - 58d5 - 7976e62b3ca1 ) batch batch jobs are scheduled with a script file with an optional ability to embed job scheduling parameters via variables that are defined by # sbatch lines at the top of the file. you can find some examples in our slurm / jobsubmission documentation. partitions the slurm resource manager uses partitions to act as job queues which can restrict size, time and user limits. the nexus has a number of different partitions of resources. different centers, labs, and faculty are able to invest in computational resources that are restricted to approved users through these partitions. partitions usable by all non - class account users : nexus / tron - pool of resources available to all umiacs and csd faculty and graduate students. scavenger - preemption partition that supports nodes from multiple other partitions. more resources are\"),\n",
       " Document(id='0377ac76-defe-4a6f-9335-2d9acf7e70b3', page_content=\"nexus / cbcb - umiacs nexus / cbcb from umiacs jump to navigation jump to search the compute nodes from cbcb ' s previous standalone cluster have folded into nexus as of mid 2023. the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 submission nodes 2 nodes 3 partitions 4 qos 5 jobs 6 storage 7 migration 7. 1 operating system / software submission nodes you can ssh to nexuscbcb. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexuscbcb00. umiacs. umd. edu nexuscbcb01. umiacs. umd. edu nodes all nodes in cbcb - owned partitions ( see below section ) owned by cbcb faculty are named in the format cbcb # #. the sets of nodes are : 22 nodes that were purchased in october 2022 with center - wide funding. they are cbcb [ 00 - 21 ]. 4 nodes from the previous standalone cbcb cluster that moved in as of summer 2023. they are cbcb [ 22 - 25 ]. a few additional nodes purchased by dr. heng huang since then. they are all remaining ' cbcb ' named nodes. nodenames quantity cpu cores per node ( cpus ) memory per node ( type ) local storage per node ( type / location ) gpus per node ( type ) cbcb [ 00 - 21 ] 22 32 ( dual amd epyc 7313 ) ~ 2tb ( ddr4 3200mhz ) ~ 350gb ( sata ssd / scratch0 ), ~ 2tb ( nvme ssd / scratch1 ) 0 cbcb22 1 28 ( dual intel xeon e5 - 2680 v4 ) ~ 768gb ( ddr4 2400mhz ) ~ 650gb ( sata ssd / scratch0 ) 0 cbcb [ 23 - 24 ] 2 24 ( dual intel xeon e5 - 2650 v4 ) ~ 256gb ( ddr4 2400mhz ) ~ 800gb ( sata ssd / scratch0 ) 0 cbcb25 1 24 ( dual intel xeon e5 - 2650 v4 ) ~ 256gb ( ddr4 2400mhz ) ~ 1. 4tb ( sata ssd / scratch0 ) 2 ( 1x nvidia geforce gtx 1080 ti, 1x nvidia geforce rtx 2080 ti ) cbcb26 1 128 ( dual amd epyc 7763 ) ~ 512gb ( ddr4 3200mhz ) ~ 3. 4tb ( nvme ssd / scratch0 ), ~ 14tb ( nvme ssd / scratch1 ) 8 ( nvidia rtx a5000 ) cbcb27 1 64 ( dual amd epyc 7513 ) ~ 256gb ( ddr4 3200mhz ) ~ 3. 4tb ( sata ssd / scratch0 ), ~ 3. 5tb ( nvme ssd / scratch1 ) 8 ( nvidia rtx a6000 ) cbcb [ 28 - 29 ] 2 32 ( dual amd epyc 9124 ) ~ 768gb ( ddr5 4800mhz ) ~ 350gb ( sata ssd / scratch0 ), ~ 7tb ( nvme ssd / scratch1 ) 8 ( nvidia rtx 6000 ada generation ) total 30 1060 ( various ) ~ 49tb ( various ) ~ 94tb ( various ) 34 ( various ) here is the listing of nodes as shown by the slurm alias show _ nodes ( again, all nodes are named in the format cbcb # # ) : [ root @ nexusctl00 ~ ] # show _ nodes | grep cbcb nodelist cpus memory avail _ features gres state cbcb00 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb01 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb02 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb03 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb04 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb05 32 206\")]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(query=\"python notebook nexus\", k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='242f79d4-30d6-403c-9fc5-e93049b7c789', page_content=\". running jupyter notebook on a compute node the steps to run a jupyter notebook from a compute node are listed below. setting up your python virtual environment create a python virtual environment on the compute node you are assigned and activate it. next, install jupyter using pip by following the steps here. you may also use other environment management systems such as conda if desired. running jupyter notebook after you ' ve set up the python virtual environment, submit a job, activate the environment within the job, and run the following command on the compute node you are assigned : jupyter notebook - - no - browser - - port = 8889 - - ip = 0. 0. 0. 0 this will start running the notebook on port 8889. note : you must keep this shell window open to be able to connect. if the submission node for the cluster you are using is not accessible via the public internet, you must also be on a machine connected to the umiacs network or connected to our vpn in order to access the jupyter notebook once you start the ssh tunnel, so ensure this is the case before starting the tunnel. then, on your local machine, run ssh - n - f - l localhost : 8888 : < nodename > : 8889 < username > @ < submissionnode >. umiacs. umd. edu this will tunnel port 8889 from the compute node to port 8888 on your local machine, using < submissionnode > as an intermediate node. make sure to replace < username > with your username, < submissionnode > with the name of the submission node you want to use, and < nodename > with the name of the compute node you are assigned. note that this command will not display any output if the connection is successful due to the included ssh flags. you must also keep this shell window open to be able to connect. for example, assuming your username is username and that you are using the nexus cluster, have been assigned the nexusgroup submission nodes, and are assigned compute node tron00. umiacs. umd. edu : ssh - n - f - l localhost : 8888 : tron00. umiacs. umd. edu : 8889 username @ nexusgroup. umiacs. umd. edu you can then open a web browser and type in localhost : 8888 to access the notebook. notes : later versions of jupyter have token authentication enabled by default - you will need to prepend the /? token = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx part of the url provided by the terminal output after starting the notebook in order to connect if this is the case. e. g. localhost : 8888 /? token = fcc6bd0f996e7aa89376c33cb34f7b80890502aacc97d98e if the port on the compute node mentioned in the example above ( 8889 ) is not working, it may be that someone else has already started a process ( jupyter notebook or otherwise ) using that specific port number on that specific compute node. the port number can be replaced with any other ephemeral port number you ' d like, just make sure to change it in both the command you run on the compute node and the ssh command from your local machine. quick guide to translate pbs / torque to slurm pbs / torque was the previous workload manager and job submission framework used at umiacs prior to slurm ' s adoption. below is a quick guide of how to translate some common pbs / torque commands to slurm ones. user commands pbs / torque slurm job submission qsub [ filename ] sbatch [ filename ] job deletion qdel [ job _ id ] scancel [ job _ id ] job status ( by job ) qstat [ job _ id ] squeue - - job [ job _ id ] full job status ( by job ) qstat - f [ job _ id ] scontrol show job [ job _ id ] job status ( by user ) qstat - u [ username ] squeue - - user = [ username ] environment variables pbs / torque slurm job id $ pbs _ jobid $ slurm _ jobid submit directory $ pbs _ o _ workdir $ slurm _ submit _ dir node list $ pbs _ nodefile $ slurm _ job _ nodelist job specification pbs / torque slurm script directive # pbs # sbatch job name - n [ name ] - - job - name = [ name ] or - j [ name ] node count - l nodes = [ count\"),\n",
       " Document(id='f19ae39a-4386-48c2-a4b1-d2da53f7f4d3', page_content=\"nexus - umiacs nexus from umiacs jump to navigation jump to search the nexus is the combined scheduler of resources in umiacs. the resource manager for nexus is slurm. resources are arranged into partitions where users are able to schedule computational jobs. users are arranged into a number of slurm accounts based on faculty, lab, or center investments. contents 1 getting started 1. 1 access 1. 2 jobs 1. 2. 1 interactive 1. 2. 2 batch 2 partitions 3 quality of service ( qos ) 3. 1 job qos 3. 2 partition qos 4 storage 4. 1 home directories 4. 2 scratch directories 4. 2. 1 network scratch directories 4. 2. 2 local scratch directories 4. 3 faculty allocations 4. 4 project allocations 4. 5 datasets getting started all accounts in umiacs are sponsored. if you don ' t already have a umiacs account, please see accounts for information on getting one. you need a full umiacs account ( not a collaborator account ) in order to access nexus. access your access to submission nodes ( alternatively called login nodes ) for nexus computational resources is determined by your account sponsor ' s department, center, or lab affiliation. you can log into the umiacs directory cr application and select the computational resource ( cr ) in the list that has the prefix nexus. the hosts section lists your available submission nodes - generally a pair of nodes of the format nexus < department, lab, or center abbreviation > [ 00, 01 ], e. g., nexusgroup00 and nexusgroup01. note - umiacs requires multi - factor authentication through our duo instance. this is completely discrete from both umd ' s and csd ' s duo instances. you will need to enroll one or more devices to access resources in umiacs, and will be prompted to enroll when you log into the directory application for the first time. once you have identified your submission nodes, you can ssh directly into them. from there, you are able to submit to the cluster via our slurm workload manager. you need to make sure that your submitted jobs have the correct account, partition, and qos. jobs slurm jobs are submitted by either srun or sbatch depending if you are doing an interactive job or batch job, respectively. you need to provide the where / how / who to run the job and specify the resources you need to run with. for the who / where / how, you may be required to specify - - account, - - partition, and / or - - qos ( respectively ) to be able to adequately submit jobs to the nexus. for resources, you may need to specify - - time for time, - - ntasks for cpus, - - mem for ram, and - - gres = gpu for gpus in your submission arguments to meet your requirements. there are defaults for all four, so if you don ' t specify something, you may be scheduled with a very minimal set of time and resources ( e. g., by default, no gpus are included if you do not specify - - gres = gpu ). for more information about submission flags for gpu resources, see here. you can also can run man srun on your submission node for a complete list of available submission arguments. for a list of available gpu types on nexus and their specs, please see nexus / gpus. interactive once logged into a submission node, you can run simple interactive jobs. if your session is interrupted from the submission node, the job will be killed. as such, we encourage use of a terminal multiplexer such as tmux. $ srun - - pty - - ntasks = 4 - - mem = 2gb - - gres = gpu : 1 nvidia - smi - l gpu 0 : nvidia rtx a4000 ( uuid : gpu - ae5dc1f5 - c266 - 5b9f - 58d5 - 7976e62b3ca1 ) batch batch jobs are scheduled with a script file with an optional ability to embed job scheduling parameters via variables that are defined by # sbatch lines at the top of the file. you can find some examples in our slurm / jobsubmission documentation. partitions the slurm resource manager uses partitions to act as job queues which can restrict size, time and user limits. the nexus has a number of different partitions of resources. different centers, labs, and faculty are able to invest in computational resources that are restricted to approved users through these partitions. partitions usable by all non - class account users : nexus / tron - pool of resources available to all umiacs and csd faculty and graduate students. scavenger - preemption partition that supports nodes from multiple other partitions. more resources are\"),\n",
       " Document(id='769d6bd5-d9f4-4e88-bdd1-7ee5205a8ad4', page_content=\"nexus / cml - umiacs nexus / cml from umiacs jump to navigation jump to search the compute nodes from cml ' s previous standalone cluster have folded into nexus as of the scheduled maintenance window for august 2023 ( thursday 08 / 17 / 2023, 5 - 8pm ). the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 usage 2 partitions 3 accounts 4 qos 5 storage 5. 1 home directories 5. 2 project directories 5. 3 scratch directories 5. 3. 1 network scratch directory 5. 3. 2 local scratch directories 5. 4 datasets 5. 5 models usage you can ssh to nexuscml. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexuscml00. umiacs. umd. edu nexuscml01. umiacs. umd. edu all partitions, qoses, and account names from the standalone cml cluster have been moved over to nexus. however, please note that cml - is prepended to all of the values that were present in the standalone cml cluster to distinguish them from existing values in nexus. the lone exception is the base account that was named cml in the standalone cluster ( it is also named just cml in nexus ). here are some before / after examples of job submission with various parameters : standalone cml cluster submission command nexus cluster submission command srun - - partition = dpart - - qos = medium - - account = tomg - - gres = gpu : rtx2080ti : 2 - - pty bash srun - - partition = cml - dpart - - qos = cml - medium - - account = cml - tomg - - gres = gpu : rtx2080ti : 2 - - pty bash srun - - partition = cpu - - qos = cpu - - pty bash srun - - partition = cml - cpu - - qos = cml - cpu - - account = cml - - pty bash srun - - partition = scavenger - - qos = scavenger - - account = scavenger - - gres = gpu : 4 - - pty bash srun - - partition = cml - scavenger - - qos = cml - scavenger - - account = cml - scavenger - - gres = gpu : 4 - - pty bash cml users ( exclusively ) can schedule non - interruptible jobs on cml nodes with any non - scavenger job parameters. please note that the cml - dpart partition has a grptres limit of 100 % of the available cores / ram on all cml # # nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. it also has a max submission limit of 500 jobs per user simultaneously so as to not overload the cluster. this is codified by the partition qos named cml. please note that the cml compute nodes are also in the institute - wide scavenger partition in nexus. cml users still have scavenging priority over these nodes via the cml - scavenger partition ( i. e., all cml - partition jobs ( other than cml - scavenger ) can preempt both cml - scavenger and scavenger partition jobs, and cml - scavenger partition jobs can preempt scavenger partition jobs ). partitions there are three partitions available to general cml slurm users. you must specify a partition when submitting your job. cml - dpart - this is the default partition. job allocations are guaranteed. cml - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs in other cml - partitions are ready to be scheduled. cml - cpu - this partition is for cpu focused jobs. job allocations are guaranteed. there are two additional partitions available solely to specific faculty members and their sponsored accounts. cml - furongh - this partition is for exclusive priority access to dr. furong huang ' s purchased a6000 node. job allocations are guaranteed. cml - zhou - this partition is for exclusive priority access to dr. tianyi zhou\"),\n",
       " Document(id='b3c93573-6033-4e41-bc17-e5d84be791ca', page_content=\"nexus / vulcan - umiacs nexus / vulcan from umiacs jump to navigation jump to search the compute nodes from vulcan ' s previous standalone cluster have folded into nexus as of the scheduled maintenance window for august 2023 ( thursday 08 / 17 / 2023, 5 - 8pm ). the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 usage 2 nodes 3 partitions 4 accounts 5 qos 6 storage 6. 1 home directories 6. 2 scratch directories 6. 2. 1 network scratch directory 6. 2. 2 local scratch directories 6. 3 datasets 6. 4 project storage 6. 5 object storage 7 migration 7. 1 home directories usage you can ssh to nexusvulcan. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexusvulcan00. umiacs. umd. edu nexusvulcan01. umiacs. umd. edu all partitions, qoses, and account names from the standalone vulcan cluster have been moved over to nexus. however, please note that vulcan - is prepended to all of the values that were present in the standalone vulcan cluster to distinguish them from existing values in nexus. the lone exception is the base account that was named vulcan in the standalone cluster ( it is also named just vulcan in nexus ). here are some before / after examples of job submission with various parameters : standalone vulcan cluster submission command nexus cluster submission command srun - - partition = dpart - - qos = medium - - account = abhinav - - gres = gpu : gtx1080ti : 2 - - pty bash srun - - partition = vulcan - dpart - - qos = vulcan - medium - - account = vulcan - abhinav - - gres = gpu : gtx1080ti : 2 - - pty bash srun - - partition = cpu - - qos = cpu - - pty bash srun - - partition = vulcan - cpu - - qos = vulcan - cpu - - account = vulcan - - pty bash srun - - partition = scavenger - - qos = scavenger - - account = vulcan - - gres = gpu : 4 - - pty bash srun - - partition = vulcan - scavenger - - qos = vulcan - scavenger - - account = vulcan - - gres = gpu : 4 - - pty bash vulcan users ( exclusively ) can schedule non - interruptible jobs on vulcan nodes with any non - scavenger job parameters. please note that the vulcan - dpart partition has a grptres limit of 100 % of the available cores / ram on all vulcan # # in aggregate nodes plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. it also has a max submission limit of 500 jobs per user simultaneously so as to not overload the cluster. this is codified by the partition qos named vulcan. please note that the vulcan compute nodes are also in the institute - wide scavenger partition in nexus. vulcan users still have scavenging priority over these nodes via the vulcan - scavenger partition ( i. e., all vulcan - partition jobs ( other than vulcan - scavenger ) can preempt both vulcan - scavenger and scavenger partition jobs, and vulcan - scavenger partition jobs can preempt scavenger partition jobs ). nodes there are currently 46 gpu nodes available running a mixture of nvidia rtx a6000, nvidia rtx a5000, nvidia rtx a4000, nvidia quadro p6000, nvidia geforce gtx 1080 ti, nvidia geforce rtx 2080 ti, and nvidia tesla p100 cards. there are also 4 cpu - only nodes available. all nodes are scheduled with the slurm resource manager. partitions there are three partitions available to general vulcan slurm users. you must specify a partition when submitting your job. vulcan - dpart - this is the default partition. job allocations are guaranteed. only nodes with gpus from architectures before nvidia ' s ampere architecture are included in this partition. vulcan - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs\"),\n",
       " Document(id='d53deacf-43ef-4226-be31-d097e495f942', page_content='* : * tres = cpu = 16, mem = 2000g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 2000g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage cbcb still has its current storage allocation in place. all data filesystems that were available in the standalone cbcb cluster are also available in nexus. please note about the change in your home directory in the migration section below. cbcb users can also request nexus project allocations. migration operating system / software cbcb \\' s standalone cluster submission and compute nodes were running rhel7. nexus is exclusively running rhel8, so any software you may have compiled may need to be re - compiled to work correctly in this new environment. the cbcb module tree for rhel8 may not yet be fully populated with rhel8 software. if you do not see the modules you need, please reach out to the cbcb software maintainers. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / cbcb & oldid = 12056 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 26 september 2024, at 13 : 26. privacy policy about umiacs disclaimers'),\n",
       " Document(id='4f8e97e4-e5f7-4816-a75c-91bb9b332023', page_content='/ task = 1 reqb : s : c : t = 0 : 0 : * : * tres = cpu = 4, mem = 8g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 8g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage all data filesystems that were available in the standalone mc2 cluster are also available in nexus. mc2 users can also request nexus project allocations. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / mc2 & oldid = 12055 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 26 september 2024, at 13 : 26. privacy policy about umiacs disclaimers'),\n",
       " Document(id='afd0683d-4e99-401a-8249-f71aa02fd8b4', page_content=': * : * tres = cpu = 4, mem = 8g, node = 1, billing = 2266 socks / node = * ntaskspern : b : s : c = 0 : 0 : * : * corespec = * mincpusnode = 1 minmemorynode = 8g mintmpdisknode = 0 features = ( null ) delayboot = 00 : 00 : 00 oversubscribe = ok contiguous = 0 licenses = ( null ) network = ( null ) command = bash workdir = / nfshomes / username power = storage all data filesystems that were available in the standalone clip cluster are also available in nexus. clip users can also request nexus project allocations. retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = nexus / clip & oldid = 12034 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 17 september 2024, at 16 : 35. privacy policy about umiacs disclaimers'),\n",
       " Document(id='0377ac76-defe-4a6f-9335-2d9acf7e70b3', page_content=\"nexus / cbcb - umiacs nexus / cbcb from umiacs jump to navigation jump to search the compute nodes from cbcb ' s previous standalone cluster have folded into nexus as of mid 2023. the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 submission nodes 2 nodes 3 partitions 4 qos 5 jobs 6 storage 7 migration 7. 1 operating system / software submission nodes you can ssh to nexuscbcb. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexuscbcb00. umiacs. umd. edu nexuscbcb01. umiacs. umd. edu nodes all nodes in cbcb - owned partitions ( see below section ) owned by cbcb faculty are named in the format cbcb # #. the sets of nodes are : 22 nodes that were purchased in october 2022 with center - wide funding. they are cbcb [ 00 - 21 ]. 4 nodes from the previous standalone cbcb cluster that moved in as of summer 2023. they are cbcb [ 22 - 25 ]. a few additional nodes purchased by dr. heng huang since then. they are all remaining ' cbcb ' named nodes. nodenames quantity cpu cores per node ( cpus ) memory per node ( type ) local storage per node ( type / location ) gpus per node ( type ) cbcb [ 00 - 21 ] 22 32 ( dual amd epyc 7313 ) ~ 2tb ( ddr4 3200mhz ) ~ 350gb ( sata ssd / scratch0 ), ~ 2tb ( nvme ssd / scratch1 ) 0 cbcb22 1 28 ( dual intel xeon e5 - 2680 v4 ) ~ 768gb ( ddr4 2400mhz ) ~ 650gb ( sata ssd / scratch0 ) 0 cbcb [ 23 - 24 ] 2 24 ( dual intel xeon e5 - 2650 v4 ) ~ 256gb ( ddr4 2400mhz ) ~ 800gb ( sata ssd / scratch0 ) 0 cbcb25 1 24 ( dual intel xeon e5 - 2650 v4 ) ~ 256gb ( ddr4 2400mhz ) ~ 1. 4tb ( sata ssd / scratch0 ) 2 ( 1x nvidia geforce gtx 1080 ti, 1x nvidia geforce rtx 2080 ti ) cbcb26 1 128 ( dual amd epyc 7763 ) ~ 512gb ( ddr4 3200mhz ) ~ 3. 4tb ( nvme ssd / scratch0 ), ~ 14tb ( nvme ssd / scratch1 ) 8 ( nvidia rtx a5000 ) cbcb27 1 64 ( dual amd epyc 7513 ) ~ 256gb ( ddr4 3200mhz ) ~ 3. 4tb ( sata ssd / scratch0 ), ~ 3. 5tb ( nvme ssd / scratch1 ) 8 ( nvidia rtx a6000 ) cbcb [ 28 - 29 ] 2 32 ( dual amd epyc 9124 ) ~ 768gb ( ddr5 4800mhz ) ~ 350gb ( sata ssd / scratch0 ), ~ 7tb ( nvme ssd / scratch1 ) 8 ( nvidia rtx 6000 ada generation ) total 30 1060 ( various ) ~ 49tb ( various ) ~ 94tb ( various ) 34 ( various ) here is the listing of nodes as shown by the slurm alias show _ nodes ( again, all nodes are named in the format cbcb # # ) : [ root @ nexusctl00 ~ ] # show _ nodes | grep cbcb nodelist cpus memory avail _ features gres state cbcb00 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb01 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb02 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb03 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb04 32 2061175 rhel8, zen, epyc - 7313 ( null ) idle cbcb05 32 206\")]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = embedding_model_big.client # this model lets you transform the question embedding to be more like a long answer's embedding\n",
    "# no clue if it's better for us \n",
    "vector_store.similarity_search_by_vector(embedding=model.encode(\"python notebook nexus\", prompt_name=\"s2p_query\"), k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='242f79d4-30d6-403c-9fc5-e93049b7c789', page_content=\". running jupyter notebook on a compute node the steps to run a jupyter notebook from a compute node are listed below. setting up your python virtual environment create a python virtual environment on the compute node you are assigned and activate it. next, install jupyter using pip by following the steps here. you may also use other environment management systems such as conda if desired. running jupyter notebook after you ' ve set up the python virtual environment, submit a job, activate the environment within the job, and run the following command on the compute node you are assigned : jupyter notebook - - no - browser - - port = 8889 - - ip = 0. 0. 0. 0 this will start running the notebook on port 8889. note : you must keep this shell window open to be able to connect. if the submission node for the cluster you are using is not accessible via the public internet, you must also be on a machine connected to the umiacs network or connected to our vpn in order to access the jupyter notebook once you start the ssh tunnel, so ensure this is the case before starting the tunnel. then, on your local machine, run ssh - n - f - l localhost : 8888 : < nodename > : 8889 < username > @ < submissionnode >. umiacs. umd. edu this will tunnel port 8889 from the compute node to port 8888 on your local machine, using < submissionnode > as an intermediate node. make sure to replace < username > with your username, < submissionnode > with the name of the submission node you want to use, and < nodename > with the name of the compute node you are assigned. note that this command will not display any output if the connection is successful due to the included ssh flags. you must also keep this shell window open to be able to connect. for example, assuming your username is username and that you are using the nexus cluster, have been assigned the nexusgroup submission nodes, and are assigned compute node tron00. umiacs. umd. edu : ssh - n - f - l localhost : 8888 : tron00. umiacs. umd. edu : 8889 username @ nexusgroup. umiacs. umd. edu you can then open a web browser and type in localhost : 8888 to access the notebook. notes : later versions of jupyter have token authentication enabled by default - you will need to prepend the /? token = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx part of the url provided by the terminal output after starting the notebook in order to connect if this is the case. e. g. localhost : 8888 /? token = fcc6bd0f996e7aa89376c33cb34f7b80890502aacc97d98e if the port on the compute node mentioned in the example above ( 8889 ) is not working, it may be that someone else has already started a process ( jupyter notebook or otherwise ) using that specific port number on that specific compute node. the port number can be replaced with any other ephemeral port number you ' d like, just make sure to change it in both the command you run on the compute node and the ssh command from your local machine. quick guide to translate pbs / torque to slurm pbs / torque was the previous workload manager and job submission framework used at umiacs prior to slurm ' s adoption. below is a quick guide of how to translate some common pbs / torque commands to slurm ones. user commands pbs / torque slurm job submission qsub [ filename ] sbatch [ filename ] job deletion qdel [ job _ id ] scancel [ job _ id ] job status ( by job ) qstat [ job _ id ] squeue - - job [ job _ id ] full job status ( by job ) qstat - f [ job _ id ] scontrol show job [ job _ id ] job status ( by user ) qstat - u [ username ] squeue - - user = [ username ] environment variables pbs / torque slurm job id $ pbs _ jobid $ slurm _ jobid submit directory $ pbs _ o _ workdir $ slurm _ submit _ dir node list $ pbs _ nodefile $ slurm _ job _ nodelist job specification pbs / torque slurm script directive # pbs # sbatch job name - n [ name ] - - job - name = [ name ] or - j [ name ] node count - l nodes = [ count\"),\n",
       " Document(id='783dcd7e-da73-4fb3-b3a5-415a5a83f91d', page_content='/ derek / pytorch _ docker info : converting oci blobs to sif format info : starting build... getting image source signatures copying blob 85386706b020 done... 2022 / 10 / 14 10 : 58 : 36 info unpack layer : sha256 : b6f46848806c8750a68edc4463bf146ed6c3c4af18f5d3f23281dcdfb1c65055 2022 / 10 / 14 10 : 58 : 43 info unpack layer : sha256 : 44845dc671f759820baac0376198141ca683f554bb16a177a3cfe262c9e368ff info : creating sif file... $ apptainer exec - - nv pytorch _ docker. sif python3 - c \\' from _ _ future _ _ import print _ function ; import torch ; print ( torch. cuda. current _ device ( ) ) ; x = torch. rand ( 5, 3 ) ; print ( x ) \\' 0 tensor ( [ [ 0. 3273, 0. 7174, 0. 3587 ], [ 0. 2250, 0. 3896, 0. 4136 ], [ 0. 3626, 0. 0383, 0. 6274 ], [ 0. 6241, 0. 8079, 0. 2950 ], [ 0. 0804, 0. 9705, 0. 0030 ] ] ) retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = apptainer & oldid = 11997 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 13 august 2024, at 17 : 51. privacy policy about umiacs disclaimers'),\n",
       " Document(id='8b63c936-cc19-4158-8f7c-810214805748', page_content=\"gpu 6 : nvidia rtx a5000 ( uuid : gpu - 8d20d6cd - abf5 - 2630 - ab88 - 6bba438c55fe ) gpu 7 : nvidia rtx a5000 ( uuid : gpu - 93170910 - 5d94 - 6da5 - 8a24 - f561d7da1e2d ) you can also use sbatch to submit your job. here are two examples on how to do that. $ sbatch - - pty - - gres = gpu : 8 - - account = gamma - - partition = gamma - - qos = huge - long - - time = 1 - 23 : 00 : 00 script. sh or $ sbatch script. sh / / script. sh / / #! / bin / bash # sbatch - - gres = gpu : 8 # sbatch - - account = gamma # sbatch - - partition = gamma # sbatch - - qos = huge - long # sbatch - - time = 1 - 23 : 00 : 00 python your _ file. py storage there are 3 types of user storage available to users in gamma : home directories project directories scratch directories there is also read - only storage available for dataset directories. gamma users can also request nexus project allocations. home directories you have 30gb of home directory storage available at / nfshomes / < username >. it has both snapshots and backups enabled. home directories are intended to store personal or configuration files only. we encourage you to not share any data in your home directory. you are encouraged to utilize our gitlab infrastructure to host your code repositories. note : to check your quota on this directory, use the command df - h ~. project directories you can request project based allocations for up to 8tb and up to 180 days with approval from a gamma faculty member. to request an allocation, please contact staff with the faculty member ( s ) that approved the project in the conversation. please include the following details : project name ( short ) description size ( 1tb, 2tb, etc. ) length in days ( 30 days, 90 days, etc. ) other user ( s ) that need to access the allocation, if any these allocations will be available from / fs / gamma - projects under a name that you provide when you request the allocation. near the end of the allocation period, staff will contact you and ask if you would like to renew the allocation ( requires re - approval from a gamma faculty member ). if you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period. staff will then remove the allocation. if you do not respond to staff ' s request by the end of the allocation period, staff will make the allocation temporarily inaccessible. if you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible. if one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation. this data is backed up nightly. scratch directories scratch data has no data protection, there are no snapshots and the data is not backed up. there are two types of scratch directories : network scratch directory local scratch directories network scratch directory you are allocated 100gb of scratch space via nfs from / gammascratch / $ username. it is not backed up or protected in any way. this directory is automounted so you may not see your directory if you run ls / gammascratch but it will be mounted when you cd into your / gammascratch directory. you may request a permanent increase of up to 200gb total space without any faculty approval by contacting staff. if you need space beyond 200gb, you will need faculty approval. this file system is available on all submission, data management, and computational nodes within the cluster. local scratch directories these file systems are not available over nfs and there are no backups or snapshots available for these file systems. each computational node that you can schedule compute jobs on has one or more local scratch directories. these are always named / scratch0, / scratch1, etc. these directories are local to each node, ie. the / scratch0 on two different nodes are completely separate. these directories are almost always more performant than any other storage available to the job. however, you must stage data to these directories within the confines of your jobs and stage the data out before the end of your jobs. these local scratch directories have a tmpwatch job which will delete unaccessed data after 90 days, scheduled via maintenance jobs\"),\n",
       " Document(id='dca74e13-1ea1-4467-b8b7-aaca27f869b5', page_content='##y within the image. $ hostname & & nvidia - smi - l tron38. umiacs. umd. edu gpu 0 : nvidia rtx a4000 ( uuid : gpu - 4a0a5644 - 9fc8 - 84b4 - 5d22 - 65d45ca36506 ) $ apptainer run - - nv / fs / nexus - containers / pytorch / pytorch _ 1. 13. 0 + cu117. sif 99 984. 5538940429688 199 654. 1710815429688 299 435. 662353515625 399 291. 1429138183594 499 195. 5575714111328 599 132. 3363037109375 699 90. 5206069946289 799 62. 86213684082031 899 44. 56754684448242 999 32. 466392517089844 1099 24. 461835861206055 1199 19. 166893005371094 1299 15. 6642427444458 1399 13. 347112655639648 1499 11. 814264297485352 1599 10. 800163269042969 1699 10. 129261016845703 1799 9. 685370445251465 1899 9. 391674041748047 1999 9. 19735336303711 result : y = 0. 0022362577728927135 + 0. 837898313999176 x + - 0. 0003857926349155605 x ^ 2 + - 0. 09065020829439163 x ^ 3 bind mounts to get data into the container you need to pass some bind mounts. apptainer containers will not automatically mount data from the outside operating system other than your home directory. users need to manually bind mounts for other file paths. - - bind / fs / nexus - scratch / < username > / < projectname > : / mnt in this example, we will exec an interactive session with gpus and binding our nexus scratch directory which allows us to specify the command we want to run inside the container. apptainer exec - - nv - - bind / fs / nexus - scratch / username : / fs / nexus - scratch / username / fs / nexus - containers / pytorch / pytorch _ 1. 13. 0 + cu117. sif bash you can now write / run your own pytorch python code interactively within the container or just make a python script that you can call directly from the apptainer exec command for batch processing. shared containers portable images called singularity image format or. sif files can be copied and shared. nexus maintains some shared containers in / fs / nexus - containers. these are arranged by the application ( s ) that are installed. docker workflow example we have a pytorch _ docker example workflow using our gitlab as a docker registry. you can clone the repository and further customize this to your needs. the workflow is : run docker on a laptop or personal desktop on to create the image, or use podman on a umiacs - supported system. tag the image and and push it to your repository ( this can be any docker registry ) pull the image down onto one of our workstations / clusters and run it with your data. $ apptainer pull pytorch _ docker. sif docker : / / registry. umiacs. umd. edu / derek / pytorch _ docker info : converting oci blobs to sif format info : starting build... getting image source signatures copying blob 85386706b020 done... 2022 / 10 / 14 10 : 58 : 36 info unpack layer : sha256 : b6f46848806c8750a68edc4463bf146ed6c3c4af18f5d3f23281dcdfb1c65055 2022 / 10 / 14 10 : 58 : 43 info unpack layer : sha256 : 44845dc671f759820baac0376198141ca683f554bb16a177a3cfe262c9e368ff info : creating sif file... $ apptainer exec - - nv'),\n",
       " Document(id='70bee7ef-0730-430a-aa82-43b80019a923', page_content=\"slurm - umiacs slurm from umiacs jump to navigation jump to search contents 1 simple linux utility for resource management ( slurm ) 1. 1 documentation 1. 2 commands 1. 2. 1 srun 1. 2. 2 salloc 1. 2. 3 sbatch 1. 2. 4 squeue 1. 2. 5 scancel 1. 2. 6 sacct 1. 2. 7 sstat 1. 3 modules 1. 4 running jupyter notebook on a compute node 1. 4. 1 setting up your python virtual environment 1. 4. 2 running jupyter notebook 2 quick guide to translate pbs / torque to slurm simple linux utility for resource management ( slurm ) slurm is an open - source workload manager designed for linux clusters of all sizes. it provides three key functions. first, it allocates exclusive or non - exclusive access to resources ( computer nodes ) to users for some duration of time so they can perform work. second, it provides a framework for starting, executing, and monitoring work ( typically a parallel job ) on a set of allocated nodes. finally, it arbitrates contention for resources by managing a queue of pending work. documentation submitting jobs checking job status checking cluster status understanding job priority job preemption overview official documentation faq related documentation : using vs code commands below are some of the common commands used in slurm. further information on how to use these commands is found in the documentation linked above. to see all flags available for a command, please check the command ' s manual by using man < command > on the command line. srun srun runs a parallel job on a cluster managed by slurm. if necessary, it will first create a resource allocation in which to run the parallel job. salloc salloc allocates a slurm job allocation, which is a set of resources ( nodes ), possibly with some set of constraints ( e. g. number of processors per node ). when salloc successfully obtains the requested allocation, it then runs the command specified by the user. finally, when the user specified command is complete, salloc relinquishes the job allocation. if no command is specified, salloc runs the user ' s default shell. sbatch sbatch submits a batch script to slurm. the batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. the batch script may contain options preceded with # sbatch before any executable commands in the script. squeue squeue views job and job step information for jobs managed by slurm. scancel scancel signals or cancels jobs, job arrays, or job steps. an arbitrary number of jobs or job steps may be signaled using job specification filters or a space separated list of specific job and / or job step ids. sacct sacct displays job accounting data stored in the job accounting log file or slurm database in a variety of forms for your analysis. the sacct command displays information on jobs, job steps, status, and exitcodes by default. you can tailor the output with the use of the - - format = option to specify the fields to be shown. sstat sstat displays job status information for your analysis. the sstat command displays information pertaining to cpu, task, node, resident set size ( rss ) and virtual memory ( vm ). you can tailor the output with the use of the - - fields = option to specify the fields to be shown. modules if you are trying to use gnu modules in a slurm job, please read the section of our modules documentation on non - interactive shell sessions. this also needs to be done if the os version of the compute node you are scheduled on is different from the os version of the submission node you are submitting the job from. running jupyter notebook on a compute node the steps to run a jupyter notebook from a compute node are listed below. setting up your python virtual environment create a python virtual environment on the compute node you are assigned and activate it. next, install jupyter using pip by following the steps here. you may also use other environment management systems such as conda if desired. running jupyter notebook after you ' ve set up the python virtual environment, submit a job, activate the environment within the job, and run the following command on the compute node you are assigned : jupyter notebook - - no - browser - - port = 8889 - - ip = 0. 0. 0. 0 this will start running the notebook on port 8889. note : you must keep this shell window open to be able to connect. if the submission node for the cluster you are using is not accessible via the public internet, you must also be on a machine connected to the\"),\n",
       " Document(id='b8827dbb-a201-469a-9010-e182df739e7d', page_content='as part of your job allocation and each job step will be run on whatever resources you tell them to. in the following example, we have a batch job that will request 2 nodes in the cluster. we then load a specific version of python into our environment and submit two job steps, each one using one node. since srun is blocks until the command finishes, we use the \\' & \\' operator to background the process so that both job steps can run at once ; however, this means that we then need to use the wait command to block processing until all background processes have finished. #! / bin / bash # lines that begin with # sbatch specify commands to be used by slurm for scheduling # sbatch - - job - name = helloworld # sets the job name # sbatch - - output = helloworld. out. % j # indicates a file to redirect stdout to ; % j is the jobid. if set, must be set to a file instead of a directory or else submission will fail. # sbatch - - error = helloworld. out. % j # indicates a file to redirect stderr to ; % j is the jobid. if set, must be set to a file instead of a directory or else submission will fail. # sbatch - - time = 00 : 05 : 00 # how long you would like your job to run ; format = dd - hh : mm : ss # sbatch - - qos = default # set qos, this will determine what resources can be requested # sbatch - - nodes = 2 # number of nodes to allocate for your job # sbatch - - ntasks = 4 # request 4 cpu cores be reserved for your node total # sbatch - - ntasks - per - node = 2 # request 2 cpu cores be reserved per node # sbatch - - mem = 1g # memory required by job ; if unit is not specified mb will be assumed. for multi - node jobs, this argument allocates this much memory * per node * srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # use srun to invoke commands within your job ; using an \\' & \\' srun - - nodes = 1 - - mem = 512m bash - c \" hostname ; python3 - - version \" & # will background the process allowing them to run concurrently wait # wait for any background processes to complete # once the end of the batch script is reached your job allocation will be revoked another useful thing to know is that you can pass additional arguments into your sbatch scripts on the command line and reference them as $ { 1 } for the first argument and so on. more examples slurm / arrayjobs scancel the scancel command can be used to cancel job allocations or job steps that are no longer needed. it can be passed individual job ids or an option to delete all of your jobs or jobs that meet certain criteria. scancel 255 cancel job 255 scancel 255. 3 cancel job step 3 of job 255 scancel - - user username - - partition = tron cancel all jobs for username in the tron partition identifying resources and features the sinfo command can show you additional features of nodes in the cluster but you need to ask it to show some non - default options using a command like sinfo - o \" % 40n % 8c % 8m % 35f % 35g \". $ sinfo - o \" % 40n % 8c % 8m % 35f % 35g \" nodelist cpus memory avail _ features gres legacy00 48 125940 rhel8, zen, epyc - 7402 ( null ) legacy [ 01 - 11, 13 - 19, 22 - 28, 30 ] 12 + 61804 + rhel8, xeon, e5 - 2620 ( null ) cbcb [ 23 - 24 ], twist [ 02 - 05 ] 24 255150 rhel8, xeon, e5 - 2650 ( null ) cbcb26 128 513243 rhel8, zen, epyc - 7763, ampere gpu : rtxa5000 : 8 cbcb27 64 255167 rhel8, zen, epyc - 7513, ampere gpu : rtxa6000 : 8 cbcb [ 00 - 21 ] 32 2061175 rhel8, zen, epyc - 7313 ( null ) cbcb22, cmlcpu [ 00, 06 - 07 ], legacy20 24 + 384270 + rhel8, xeon, e5 - 2680 ( null ) cbcb25 24 255278 rhel8, xeon, e5 - 2650, pascal, turing gpu : rtx2080ti : 1, gp'),\n",
       " Document(id='769d6bd5-d9f4-4e88-bdd1-7ee5205a8ad4', page_content=\"nexus / cml - umiacs nexus / cml from umiacs jump to navigation jump to search the compute nodes from cml ' s previous standalone cluster have folded into nexus as of the scheduled maintenance window for august 2023 ( thursday 08 / 17 / 2023, 5 - 8pm ). the nexus cluster already has a large pool of compute resources made possible through college - level funding for umiacs and csd faculty. details on common nodes already in the cluster ( tron partition ) can be found here. please contact staff with any questions or concerns. contents 1 usage 2 partitions 3 accounts 4 qos 5 storage 5. 1 home directories 5. 2 project directories 5. 3 scratch directories 5. 3. 1 network scratch directory 5. 3. 2 local scratch directories 5. 4 datasets 5. 5 models usage you can ssh to nexuscml. umiacs. umd. edu to log in to a submission host. if you store something in a local directory ( / tmp, / scratch0 ) on one of the two submission hosts, you will need to connect to that same submission host to access it later. the actual submission hosts are : nexuscml00. umiacs. umd. edu nexuscml01. umiacs. umd. edu all partitions, qoses, and account names from the standalone cml cluster have been moved over to nexus. however, please note that cml - is prepended to all of the values that were present in the standalone cml cluster to distinguish them from existing values in nexus. the lone exception is the base account that was named cml in the standalone cluster ( it is also named just cml in nexus ). here are some before / after examples of job submission with various parameters : standalone cml cluster submission command nexus cluster submission command srun - - partition = dpart - - qos = medium - - account = tomg - - gres = gpu : rtx2080ti : 2 - - pty bash srun - - partition = cml - dpart - - qos = cml - medium - - account = cml - tomg - - gres = gpu : rtx2080ti : 2 - - pty bash srun - - partition = cpu - - qos = cpu - - pty bash srun - - partition = cml - cpu - - qos = cml - cpu - - account = cml - - pty bash srun - - partition = scavenger - - qos = scavenger - - account = scavenger - - gres = gpu : 4 - - pty bash srun - - partition = cml - scavenger - - qos = cml - scavenger - - account = cml - scavenger - - gres = gpu : 4 - - pty bash cml users ( exclusively ) can schedule non - interruptible jobs on cml nodes with any non - scavenger job parameters. please note that the cml - dpart partition has a grptres limit of 100 % of the available cores / ram on all cml # # nodes in aggregate plus 50 % of the available cores / ram on legacy # # nodes in aggregate, so your job may need to wait if all available cores / ram ( or gpus ) are in use. it also has a max submission limit of 500 jobs per user simultaneously so as to not overload the cluster. this is codified by the partition qos named cml. please note that the cml compute nodes are also in the institute - wide scavenger partition in nexus. cml users still have scavenging priority over these nodes via the cml - scavenger partition ( i. e., all cml - partition jobs ( other than cml - scavenger ) can preempt both cml - scavenger and scavenger partition jobs, and cml - scavenger partition jobs can preempt scavenger partition jobs ). partitions there are three partitions available to general cml slurm users. you must specify a partition when submitting your job. cml - dpart - this is the default partition. job allocations are guaranteed. cml - scavenger - this is the alternate partition that allows jobs longer run times and more resources but is preemptable when jobs in other cml - partitions are ready to be scheduled. cml - cpu - this partition is for cpu focused jobs. job allocations are guaranteed. there are two additional partitions available solely to specific faculty members and their sponsored accounts. cml - furongh - this partition is for exclusive priority access to dr. furong huang ' s purchased a6000 node. job allocations are guaranteed. cml - zhou - this partition is for exclusive priority access to dr. tianyi zhou\"),\n",
       " Document(id='894b33a9-67ad-4c76-90e8-9f24b652f8cd', page_content='slurm / arrayjobs - umiacs slurm / arrayjobs from umiacs jump to navigation jump to search here is an example to get you started using array jobs in slurm. array computation example job save this code to a file called test. py. import time print ( \\' start at \\' + time. strftime ( \\' % h : % m : % s \\' ) ) print ( \\' sleep for 10 seconds... \\' ) time. sleep ( 10 ) print ( \\' stop at \\' + time. strftime ( \\' % h : % m : % s \\' ) ) submission script save this to a file called array. sh and you should be able to submit the job as sbatch array. sh. #! / bin / bash # # # # # # # # # # # # # # # # # # # # # # job - array example # # # # # # # # # # # # # # # # # # # # # # # sbatch - - job - name = example # sbatch - - array = 1 - 16 # run 16 jobs at the same time # sbatch - - time = 0 - 00 : 05 : 00 # run for 5 minutes ( d - hh : mm : ss ) # sbatch - - mem - per - cpu = 500mb # use 500mb per core # all bash commands must be after all sbatch directives # define and create a unique scratch directory scratch _ directory = / scratch0 / $ { user } / job - array - example / $ { slurm _ jobid } mkdir - p $ { scratch _ directory } cd $ { scratch _ directory } cp $ { slurm _ submit _ dir } / test. py $ { scratch _ directory } # each job will see a different $ { slurm _ array _ task _ id } echo \" now processing task id : : \" $ { slurm _ array _ task _ id } python test. py > output _ $ { slurm _ array _ task _ id }. txt # after the job is done we copy our output back to $ slurm _ submit _ dir cp output _ $ { slurm _ array _ task _ id }. txt $ { slurm _ submit _ dir } # we step out of the scratch directory and remove it cd $ { slurm _ submit _ dir } rm - rf $ { scratch _ directory } # happy end exit 0 retrieved from \" https : / / wiki. umiacs. umd. edu / umiacs / index. php? title = slurm / arrayjobs & oldid = 11731 \" navigation menu personal tools log in namespaces page discussion english views read view source view history more search navigation main page getting started core services lab facilities placing orders support tools what links here related changes special pages printable version permanent link page information this page was last edited on 28 march 2024, at 20 : 30. privacy policy about umiacs disclaimers')]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(query=\"python notebook\", k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.store.keys().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'my', 'name', 'is', 'bob', '.', 'https', ':', '/', '/', 'google', '.', 'com', '/', 'help', '-', 'me', '.', 'lower', '##case', '##r', 'e', '##wr', '##are', 'aw']\n",
      "hello my name is bob. https : / / google. com / help - me. lowercaser ewrare aw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'( https : / / google. com / )'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"HELLO MY NAME IS BOB. https://google.com/HELP-ME. lowercaser ewrare aw\"\n",
    "tokens = tokenizer_big.tokenize(text)  # Tokenize the input text\n",
    "print(tokens)\n",
    "print(tokenizer_big.convert_tokens_to_string(tokens))\n",
    "# chunks = []\n",
    "\n",
    "# # Create sliding window chunks with overlap\n",
    "# for i in range(0, len(tokens), max_tokens - overlap):\n",
    "#     chunk_tokens = tokens[i:i + max_tokens]\n",
    "#     chunk = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "#     chunks.append(chunk)\n",
    "tokenizer_big.decode(tokenizer_big.encode(\"(https://google.com/)\"), clean_up_tokenization_spaces=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_big.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(tokenizer_big, chunk_size=512, chunk_overlap=100, separator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nexus - UMIACS\\nNexus\\nFrom UMIACS\\nJump to navigation\\nJump to search\\nThe Nexus is the combined scheduler of resources in UMIACS.  The resource manager for Nexus is\\nSLURM\\n.  Resources are arranged into partitions where users are able to schedule computational jobs.  Users are arranged into a number of SLURM accounts based on faculty, lab, or center investments.\\nContents\\n1\\nGetting Started\\n1.1\\nAccess\\n1.2\\nJobs\\n1.2.1\\nInteractive\\n1.2.2\\nBatch\\n2\\nPartitions\\n3\\nQuality of Service (QoS)\\n3.1\\nJob QoS\\n3.2\\nPartition QoS\\n4\\nStorage\\n4.1\\nHome Directories\\n4.2\\nScratch Directories\\n4.2.1\\nNetwork Scratch Directories\\n4.2.2\\nLocal Scratch Directories\\n4.3\\nFaculty Allocations\\n4.4\\nProject Allocations\\n4.5\\nDatasets\\nGetting Started\\nAll accounts in UMIACS are sponsored.  If you don\\'t already have a UMIACS account, please see\\nAccounts\\nfor information on getting one.  You need a full UMIACS account (not a\\ncollaborator account\\n) in order to access Nexus.\\nAccess\\nYour access to submission nodes (alternatively called login nodes) for Nexus computational resources is determined by your account sponsor\\'s department, center, or lab affiliation.  You can log into the\\nUMIACS Directory CR application\\nand select the Computational Resource (CR) in the list that has the prefix\\nnexus\\n.  The Hosts section lists your available submission nodes - generally a pair of nodes of the format\\nnexus<department, lab, or center abbreviation>[00,01]\\n, e.g.,\\nnexusgroup00\\nand\\nnexusgroup01\\n.\\nNote\\n- UMIACS requires multi-factor authentication through our\\nDuo\\ninstance.  This is completely discrete from both UMD\\'s and CSD\\'s Duo instances.  You will need to enroll one or more devices to access resources in UMIACS, and will be prompted to enroll when you log into the Directory application for the first time.\\nOnce you have identified your submission nodes, you can\\nSSH\\ndirectly into them.  From there, you are able to submit to the cluster via our\\nSLURM\\nworkload manager.  You need to make sure that your submitted jobs have the correct account, partition, and qos.\\nJobs\\nSLURM\\njobs are\\nsubmitted\\nby either\\nsrun\\nor\\nsbatch\\ndepending if you are doing an interactive job or batch job, respectively.  You need to provide the where/how/who to run the job and specify the resources you need to run with.\\nFor the who/where/how, you may be required to specify\\n--account\\n,\\n--partition\\n, and/or\\n--qos\\n(respectively) to be able to adequately submit jobs to the Nexus.\\nFor resources, you may need to specify\\n--time\\nfor time,\\n--ntasks\\nfor CPUs,\\n--mem\\nfor RAM, and\\n--gres=gpu\\nfor GPUs in your submission arguments to meet your requirements.  There are defaults for all four, so if you don\\'t specify something, you may be scheduled with a very minimal set of time and resources (e.g., by default, NO GPUs are included if you do not specify\\n--gres=gpu\\n).  For more information about submission flags for GPU resources, see\\nhere\\n.  You can also can run\\nman srun\\non your submission node for a complete list of available submission arguments.\\nFor a list of available GPU types on Nexus and their specs, please see\\nNexus/GPUs\\n.\\nInteractive\\nOnce logged into a submission node, you can run simple interactive jobs.  If your session is interrupted from the submission node, the job will be killed.  As such, we encourage use of a terminal multiplexer such as\\nTmux\\n.\\n$ srun --pty --ntasks=4 --mem=2gb --gres=gpu:1 nvidia-smi -L\\nGPU 0: NVIDIA RTX A4000 (UUID: GPU-ae5dc1f5-c266-5b9f-58d5-7976e62b3ca1)\\nBatch\\nBatch jobs are scheduled with a script file with an optional ability to embed job scheduling parameters via variables that are defined by\\n#SBATCH\\nlines at the top of the file.  You can find some examples in our\\nSLURM/JobSubmission\\ndocumentation.\\nPartitions\\nThe SLURM resource manager uses partitions to act as job queues which can restrict size, time and user limits.  The Nexus has a number of different partitions of resources.  Different Centers, Labs, and Faculty are able to invest in computational resources that are restricted to approved users through these partitions.\\nPartitions usable by all non-\\nclass account\\nusers:\\nNexus/Tron\\n- Pool of resources available to all UMIACS and CSD faculty and graduate students.\\nScavenger -\\nPreemption\\npartition that supports nodes from multiple other partitions.  More resources are available to schedule simultaneously than in other partitions, however jobs are subject to preemption rules.  You are responsible for ensuring your jobs handle this preemption correctly.  The SLURM scheduler will simply restart a preempted job with the same submission arguments when it is available to run again. For an overview of things you can check within scripts to determine if your job was preempted/resumed, see\\nSLURM/Preemption\\n.\\nPartitions usable by\\nClassAccounts\\n:\\nClass\\n- Pool available for UMIACS class accounts sponsored by either UMIACS or CSD faculty.\\nPartitions usable by specific lab/center users:\\nNexus/CBCB\\n- CBCB lab pool available for CBCB lab members.\\nNexus/CLIP\\n- CLIP lab pool available for CLIP lab members.\\nNexus/CML\\n- CML lab pool available for CML lab members.\\nNexus/GAMMA\\n- GAMMA lab pool available for GAMMA lab members.\\nNexus/MBRC\\n- MBRC lab pool available for MBRC lab members.\\nNexus/MC2\\n- MC2 lab pool available for MC2 lab members.\\nNexus/Vulcan\\n- Vulcan lab pool available for Vulcan lab members.\\nQuality of Service (QoS)\\nSLURM uses Quality of Service (QoS) both to provide limits on job sizes (termed by us as \"job QoS\") as well as to limit resources used by all jobs running in a partition, either per user or per group (termed by us as \"partition QoS\").\\nJob QoS\\nJob QoS are used to provide limits on the size of job that you can run. You should try to allocate only the resources your job actually needs, as resources that each of your jobs schedules are counted against your\\nfair-share priority\\nin the future.\\ndefault - Default job QoS. Limited to 4 CPU cores, 1 GPU, and 32GB RAM per job.  The maximum wall time per job is 3 days.\\nmedium - Limited to 8 CPU cores, 2 GPUs, and 64GB RAM per job.  The maximum wall time per job is 2 days.\\nhigh - Limited to 16 CPU cores, 4 GPUs, and 128GB RAM per job.  The maximum wall time per job is 1 day.\\nscavenger - No resource limits per job, only a maximum wall time per job of 3 days.  You are responsible for ensuring your job requests multiple nodes if it requests resources beyond what any one node is capable of.  576 total CPU cores, 72 total GPUs, and 2304GB total RAM are permitted simultaneously across all of your jobs running with this job QoS.  This job QoS is both only available in the scavenger partition and the only job QoS available in the scavenger partition. To use this job QoS, include\\n--partition=scavenger\\nand\\n--account=scavenger\\nin your submission arguments.  Do not include any job QoS argument other than\\n--qos=scavenger\\n(optional) or submission will fail.\\nYou can display these job QoS from the command line using the\\nshow_qos\\ncommand.  By default, the command will only show job QoS that you can access.  The above four job QoS are the ones that everyone can access.\\n$ show_qos\\n                Name     MaxWall                        MaxTRES MaxJobsPU                      MaxTRESPU \\n-------------------- ----------- ------------------------------ --------- ------------------------------ \\n             default  3-00:00:00       cpu=4,gres/gpu=1,mem=32G                                          \\n                high  1-00:00:00     cpu=16,gres/gpu=4,mem=128G                                                                        \\n              medium  2-00:00:00       cpu=8,gres/gpu=2,mem=64G                                          \\n           scavenger  3-00:00:00                                           cpu=576,gres/gpu=72,mem=2304G\\nIf you want to see all job QoS, including those that you do not have access to, you can use the\\nshow_qos --all\\ncommand.\\n$ show_qos --all\\n                Name     MaxWall                        MaxTRES MaxJobsPU                      MaxTRESPU \\n-------------------- ----------- ------------------------------ --------- ------------------------------ \\n             cml-cpu  7-00:00:00                                        8                                \\n         cml-default  7-00:00:00       cpu=4,gres/gpu=1,mem=32G         2                                \\n            cml-high  1-12:00:00     cpu=16,gres/gpu=4,mem=128G         2                                \\n       cml-high_long 14-00:00:00              cpu=32,gres/gpu=8         8                     gres/gpu=8 \\n          cml-medium  3-00:00:00       cpu=8,gres/gpu=2,mem=64G         2                                \\n       cml-scavenger  3-00:00:00                                                             gres/gpu=24 \\n       cml-very_high  1-12:00:00     cpu=32,gres/gpu=8,mem=256G         8                    gres/gpu=12 \\n             default  3-00:00:00       cpu=4,gres/gpu=1,mem=32G                                          \\n                high  1-00:00:00     cpu=16,gres/gpu=4,mem=128G                                          \\n             highmem 21-00:00:00                  cpu=32,mem=2T                                          \\n           huge-long 10-00:00:00     cpu=32,gres/gpu=8,mem=256G                                          \\n              medium  2-00:00:00       cpu=8,gres/gpu=2,mem=64G                                          \\n           scavenger  3-00:00:00                                           cpu=576,gres/gpu=72,mem=2304G \\n          vulcan-cpu  2-00:00:00                cpu=1024,mem=4T         4                                \\n      vulcan-default  7-00:00:00       cpu=4,gres/gpu=1,mem=32G         2                                \\n       vulcan-exempt  7-00:00:00     cpu=32,gres/gpu=8,mem=256G         2                                \\n         vulcan-high  1-12:00:00     cpu=16,gres/gpu=4,mem=128G         2                                \\n        vulcan-janus  3-00:00:00    cpu=32,gres/gpu=10,mem=256G                                          \\n       vulcan-medium  3-00:00:00       cpu=8,gres/gpu=2,mem=64G         2                                \\n       vulcan-sailon  3-00:00:00     cpu=32,gres/gpu=8,mem=256G                              gres/gpu=48 \\n    vulcan-scavenger  3-00:00:00     cpu=32,gres/gpu=8,mem=256G\\nTo find out what accounts and partitions you have access to, first use the\\nshow_assoc\\ncommand to show your account/job QoS combinations. Then, use the\\nscontrol show partition\\ncommand and note the\\nAllowAccounts\\nentry for each listed partition. You are able to submit to any partition that allows an account that you have. If you need to use an account other than the default account\\nnexus\\n, you will need to specify it via the\\n--account\\nsubmission argument.\\nPartition QoS\\nPartition QoS are used to limit resources used by all jobs running in a partition, either per user (MaxTRESPU) or per group (GrpTRES).\\nTo view partition QoS, use the\\nshow_partition_qos\\ncommand.\\n$ show_partition_qos\\n                Name MaxSubmitPU                      MaxTRESPU              GrpTRES \\n-------------------- ----------- ------------------------------ -------------------- \\n           scavenger         500  cpu=576,gres/gpu=72,mem=2304G                      \\n                tron         500     cpu=32,gres/gpu=4,mem=256G\\nIf you want to see all partition QoS, including those that you do not have access to, you can use the\\nshow_partition_qos --all\\ncommand.\\n$ show_partition_qos --all\\n                Name MaxSubmitPU                      MaxTRESPU              GrpTRES \\n-------------------- ----------- ------------------------------ -------------------- \\n                Name MaxSubmitPU                      MaxTRESPU              GrpTRES\\n-------------------- ----------- ------------------------------ --------------------\\n                cbcb         500                                 cpu=1260,mem=50016G\\n           cbcb-heng         500\\n               class         500     cpu=32,gres/gpu=4,mem=256G\\n                clip         500                                   cpu=564,mem=5590G\\n                 cml         500                                    cpu=1128,mem=11T\\n         cml-furongh         500\\n       cml-scavenger         500                    gres/gpu=24\\n            cml-zhou         500\\n               gamma         500                                   cpu=648,mem=5454G\\n                mbrc         500                                   cpu=240,mem=2345G\\n                 mc2         500                                   cpu=312,mem=3092G\\n               oasis         500\\n               quics         500                                   cpu=328,mem=3484G\\n           scavenger         500  cpu=576,gres/gpu=72,mem=2304G\\n                tron         500     cpu=32,gres/gpu=4,mem=256G\\n              vulcan         500                                 cpu=1392,mem=12833G\\n       vulcan-ampere         500\\n          vulcan-cpu         500\\n       vulcan-ramani         500\\n    vulcan-scavenger         500\\nNOTE\\n: These QoS cannot be used directly when submitting jobs, with the exception of the scavenger QoS (i.e., they are not in the AllowQos field for their respective partition). Partition QoS limits apply to all jobs running on a given partition, regardless of what job QoS is used.\\nFor example, in the default non-preemption partition (\\ntron\\n), you are restricted to 32 total CPU cores, 4 total GPUs, and 256GB total RAM at once across all jobs you have running in the partition.\\nLab/group-specific partitions may also have their own user limits, and/or may also have group limits on the total number of resources consumed simultaneously by all users that are using their partition, codified by the line in the output above that matches their lab/group name. Note that the values listed above in the two \"TRES\" columns are not fixed and may fluctuate per-partition as more resources are added to or removed from each partition.\\nAll partitions also only allow a maximum of 500 submitted (running (R) or pending (PD)) jobs per user in the partition simultaneously.\\nThis is to prevent excess pending jobs causing\\nbackfill\\nissues with the SLURM scheduler.\\nIf you need to submit more than 500 jobs in batch at once, you can develop and run an \"outer submission script\" that repeatedly attempts to run an \"inner submission script\" (your original submission script) to submit jobs in the batch periodically, until all job submissions are successful. The outer submission script should use looping logic to check if you are at the max job limit and should then retry submission after waiting for some time interval.\\nAn example outer submission script is as follows. In this example,\\nexample_inner.sh\\nis your inner submission script and is not an\\narray job\\n, and you want to run 1000 jobs. If your inner submission script is an array job, adjust the number of jobs accordingly. Array jobs must be of size 500 or less.\\n#!/bin/bash\\nnumjobs=1000\\ni=0\\nwhile [ $i -lt $numjobs ]\\ndo\\n  while [[ \"$(sbatch example_inner.sh 2>&1)\" =~ \"QOSMaxSubmitJobPerUserLimit\" ]]\\n  do\\n    echo \"Currently at maximum job submissions allowed.\"\\n    echo \"Waiting for 5 minutes before trying to submit more jobs.\"\\n    sleep 300\\n  done\\n  i=$(( $i + 1 ))\\n  echo \"Submitted job $i of $numjobs\"\\ndone\\nIt is suggested that you run the outer submission script in a\\nTmux\\nsession to keep the terminal window executing it from being interrupted.\\nStorage\\nAll network storage available in Nexus is currently\\nNFS\\nbased, and comes in a few different flavors. Compute nodes also have local storage that can be used.\\nHome Directories\\nYou have 30GB of home directory storage available at\\n/nfshomes/<username>\\n.  It has both\\nSnapshots\\nand\\nBackups\\nenabled.\\nHome directories are intended to store personal or configuration files only.  We encourage you to not share any data in your home directory.  You are encouraged to utilize our\\nGitLab\\ninfrastructure to host your code repositories.\\nNOTE\\n: To check your quota on this directory, use the command\\ndf -h ~\\n.\\nScratch Directories\\nScratch data has no data protection including no snapshots and the data is not backed up. There are two types of scratch directories in the Nexus compute infrastructure:\\nNetwork scratch directories\\nLocal scratch directories\\nPlease note that\\nclass accounts\\ndo not have network scratch directories.\\nNetwork Scratch Directories\\nYou are allocated 200GB of scratch space via NFS from\\n/fs/nexus-scratch/<USERNAME>\\nwhere <USERNAME> is your UMIACS username.\\nIt is not backed up or protected in any way.\\nThis directory is\\nautomounted\\n; you will need to\\ncd\\ninto the directory or request/specify a fully qualified file path to access it.\\nYou can view your quota usage by running\\ndf -h /fs/nexus-scratch/<USERNAME>\\n.\\nYou may request a permanent increase of up to 400GB total space without any faculty approval by\\ncontacting staff\\n.  If you need space beyond 400GB, you will need faculty approval and/or a\\nproject allocation\\nfor this. If you choose to increase your scratch space beyond 400GB, the increased space is also subject to the 270 TB days limit mentioned in the project allocation section before we check back in for renewal. For example, if you request 1.4TB total space, you may have this for 270 days (1TB beyond the 400GB permanent increase). The amount increased beyond 400GB will also count against your faculty member\\'s 20TB total storage limit mentioned below.\\nThis file system is available on all submission, data management, and computational nodes within the cluster.\\nLocal Scratch Directories\\nEach computational node that you can schedule compute jobs on also has one or more local scratch directories.  These are always named\\n/scratch0\\n,\\n/scratch1\\n, etc. and\\nare not backed up or protected in any way.\\nThese directories are almost always more performant than any other storage available to the job as they are mounted from disks directly attached to the compute node.  However, you must stage your data within the confines of your job and extract the relevant resultant data elsewhere before the end of your job.\\nThese local scratch directories have a tmpwatch job which will\\ndelete unaccessed data after 90 days\\n, scheduled via maintenance jobs to run once a month during our\\nmonthly maintenance windows\\n.  Please make sure you secure any resultant data you wish to keep from these directories at the end of your job.\\nFaculty Allocations\\nEach faculty member can be allocated 1TB of permanent lab space upon request.  We can also support grouping these individual allocations together into larger center, lab, or research group allocations if desired by the faculty.  Please\\ncontact staff\\nto inquire.\\nLab space storage is fully protected.  It has\\nsnapshots\\nenabled and is\\nbacked up nightly\\n.\\nProject Allocations\\nProject allocations are available per user for 270 TB days; you can have a 1TB allocation for up to 270 days, a 3TB allocation for 90 days, etc..\\nA single faculty member can not have more than 20TB of project allocations across all of their sponsored accounts active simultaneously. Network scratch allocation space increases beyond the 400GB permanent maximum also have the increase count against this limit (i.e., a 1TB network scratch allocation would have 600GB counted towards this limit).\\nProject storage is fully protected.  It has\\nsnapshots\\nenabled and is\\nbacked up nightly\\n.\\nThe maximum allocation length you can request is 540 days (500GB space) and the maximum storage space you can request is 9TB (30 day length).\\nTo request an allocation, please\\ncontact staff\\nwith the faculty member(s) that the project is under involved in the conversation.  Please include the following details:\\nProject Name (short)\\nDescription\\nSize (1TB, 2TB, etc.)\\nLength in days (270 days, 135 days, etc.)\\nOther user(s) that need to access the allocation, if any\\nThese allocations are available via\\n/fs/nexus-projects/<project name>\\n.\\nRenewal is not guaranteed to be available due to limits on the amount of total storage.\\nNear the end of the allocation period, staff will contact you and ask if you are still in need of the storage allocation.  If renewal is available, you can renew for up to another 270 TB days with reapproval from the original faculty approver.\\nIf you are no longer in need of the storage allocation, you will need to relocate all desired data within two weeks of the end of the allocation period.  Staff will then remove the allocation.\\nIf you do not respond to staff\\'s request by the end of the allocation period, staff will make the allocation temporarily inaccessible.\\nIf you do respond asking for renewal but the original faculty approver does not respond within two weeks of the end of the allocation period, staff will also make the allocation temporarily inaccessible.\\nIf one month from the end of the allocation period is reached without both you and the faculty approver responding, staff will remove the allocation.\\nDatasets\\nWe have read-only dataset storage available at\\n/fs/nexus-datasets\\n.  If there are datasets that you would like to see curated and available, please see\\nthis page\\n.\\nThe list of Nexus datasets we currently host can be viewed\\nhere\\n.\\nRetrieved from \"\\nhttps://wiki.umiacs.umd.edu/umiacs/index.php?title=Nexus&oldid=12024\\n\"\\nNavigation menu\\nPersonal tools\\nLog in\\nNamespaces\\nPage\\nDiscussion\\nEnglish\\nViews\\nRead\\nView source\\nView history\\nMore\\nSearch\\nNavigation\\nMain Page\\nGetting Started\\nCore Services\\nLab Facilities\\nPlacing Orders\\nSupport\\nTools\\nWhat links here\\nRelated changes\\nSpecial pages\\nPrintable version\\nPermanent link\\nPage information\\nThis page was last edited on 13 September 2024, at 13:21.\\nPrivacy policy\\nAbout UMIACS\\nDisclaimers'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = get_html(\"Nexus.html\", sep=\"\\n\")\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "286\n",
      "347\n",
      "373\n",
      "387\n",
      "352\n",
      "432\n",
      "446\n",
      "433\n",
      "411\n",
      "420\n",
      "409\n",
      "385\n",
      "338\n",
      "377\n",
      "376\n",
      "373\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(html) # ????????????????????????????????????????????????????????????????????\n",
    "print(len(chunks))\n",
    "for c in chunks:\n",
    "    print(len(tokenizer_big.tokenize(c)))\n",
    "# print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[917,\n",
       " 977,\n",
       " 444,\n",
       " 229,\n",
       " 749,\n",
       " 417,\n",
       " 649,\n",
       " 210,\n",
       " 2931,\n",
       " 317,\n",
       " 1069,\n",
       " 207,\n",
       " 147,\n",
       " 755,\n",
       " 427,\n",
       " 896,\n",
       " 426,\n",
       " 768,\n",
       " 322,\n",
       " 615,\n",
       " 626,\n",
       " 720,\n",
       " 2283,\n",
       " 216,\n",
       " 756,\n",
       " 196,\n",
       " 427,\n",
       " 202,\n",
       " 145,\n",
       " 3876,\n",
       " 910,\n",
       " 322,\n",
       " 369,\n",
       " 214,\n",
       " 454,\n",
       " 230,\n",
       " 184,\n",
       " 1049,\n",
       " 166,\n",
       " 1080,\n",
       " 1083,\n",
       " 1083,\n",
       " 1083,\n",
       " 1083,\n",
       " 579,\n",
       " 842,\n",
       " 849,\n",
       " 951,\n",
       " 964,\n",
       " 601,\n",
       " 421,\n",
       " 484,\n",
       " 498,\n",
       " 344,\n",
       " 519,\n",
       " 848,\n",
       " 554,\n",
       " 1318,\n",
       " 567,\n",
       " 2096,\n",
       " 1076,\n",
       " 439,\n",
       " 1082,\n",
       " 798,\n",
       " 190,\n",
       " 564,\n",
       " 158,\n",
       " 404,\n",
       " 405,\n",
       " 412,\n",
       " 524,\n",
       " 538,\n",
       " 424,\n",
       " 698,\n",
       " 968,\n",
       " 263,\n",
       " 1064,\n",
       " 1052,\n",
       " 526,\n",
       " 264,\n",
       " 254,\n",
       " 161,\n",
       " 495,\n",
       " 791,\n",
       " 222,\n",
       " 542,\n",
       " 179,\n",
       " 568,\n",
       " 476,\n",
       " 317,\n",
       " 387,\n",
       " 737,\n",
       " 243,\n",
       " 855,\n",
       " 498,\n",
       " 864,\n",
       " 392,\n",
       " 966,\n",
       " 444,\n",
       " 977,\n",
       " 261,\n",
       " 252,\n",
       " 957,\n",
       " 327,\n",
       " 315,\n",
       " 326,\n",
       " 302,\n",
       " 312,\n",
       " 335,\n",
       " 462,\n",
       " 486,\n",
       " 680,\n",
       " 1111,\n",
       " 301,\n",
       " 149,\n",
       " 252,\n",
       " 1366,\n",
       " 192,\n",
       " 980,\n",
       " 661,\n",
       " 230,\n",
       " 170,\n",
       " 1308,\n",
       " 195,\n",
       " 1351,\n",
       " 288,\n",
       " 1680,\n",
       " 1953,\n",
       " 286,\n",
       " 434,\n",
       " 895,\n",
       " 404,\n",
       " 415,\n",
       " 980,\n",
       " 429,\n",
       " 178,\n",
       " 321,\n",
       " 231,\n",
       " 247,\n",
       " 811,\n",
       " 2258,\n",
       " 478,\n",
       " 952,\n",
       " 572,\n",
       " 292,\n",
       " 155,\n",
       " 249,\n",
       " 1394,\n",
       " 417,\n",
       " 143,\n",
       " 398,\n",
       " 398,\n",
       " 266,\n",
       " 471,\n",
       " 2331,\n",
       " 586,\n",
       " 397,\n",
       " 541,\n",
       " 541,\n",
       " 371,\n",
       " 730,\n",
       " 899,\n",
       " 471,\n",
       " 529,\n",
       " 912,\n",
       " 466,\n",
       " 745,\n",
       " 885,\n",
       " 5242,\n",
       " 486,\n",
       " 2942,\n",
       " 2859,\n",
       " 1090,\n",
       " 3880,\n",
       " 2185,\n",
       " 878,\n",
       " 1331,\n",
       " 1110,\n",
       " 292,\n",
       " 3959,\n",
       " 890,\n",
       " 259,\n",
       " 372,\n",
       " 258,\n",
       " 1562,\n",
       " 1571,\n",
       " 1249,\n",
       " 1258,\n",
       " 626,\n",
       " 5250,\n",
       " 250,\n",
       " 1181,\n",
       " 1171,\n",
       " 187,\n",
       " 913,\n",
       " 1361,\n",
       " 960,\n",
       " 990,\n",
       " 571,\n",
       " 643,\n",
       " 372,\n",
       " 397,\n",
       " 708,\n",
       " 277,\n",
       " 732,\n",
       " 2088,\n",
       " 404,\n",
       " 270,\n",
       " 231,\n",
       " 317,\n",
       " 180,\n",
       " 814,\n",
       " 707,\n",
       " 1215,\n",
       " 230,\n",
       " 949,\n",
       " 667,\n",
       " 2527,\n",
       " 489,\n",
       " 2227,\n",
       " 171,\n",
       " 489,\n",
       " 481,\n",
       " 235,\n",
       " 596,\n",
       " 1240,\n",
       " 179,\n",
       " 455,\n",
       " 1036,\n",
       " 436,\n",
       " 200,\n",
       " 447,\n",
       " 1523,\n",
       " 1104,\n",
       " 1907,\n",
       " 142,\n",
       " 415,\n",
       " 269,\n",
       " 175,\n",
       " 158,\n",
       " 163,\n",
       " 195,\n",
       " 172,\n",
       " 230,\n",
       " 169,\n",
       " 188,\n",
       " 286,\n",
       " 205,\n",
       " 148,\n",
       " 224,\n",
       " 161,\n",
       " 143,\n",
       " 911,\n",
       " 358,\n",
       " 142,\n",
       " 382,\n",
       " 232,\n",
       " 8479,\n",
       " 2939,\n",
       " 2239,\n",
       " 645,\n",
       " 1502,\n",
       " 2326,\n",
       " 8464,\n",
       " 868,\n",
       " 3632,\n",
       " 1069,\n",
       " 904,\n",
       " 1563,\n",
       " 396,\n",
       " 1531,\n",
       " 374,\n",
       " 543,\n",
       " 2560,\n",
       " 1917,\n",
       " 147,\n",
       " 786,\n",
       " 298,\n",
       " 1097,\n",
       " 738,\n",
       " 535,\n",
       " 300,\n",
       " 266,\n",
       " 434,\n",
       " 606,\n",
       " 435,\n",
       " 842,\n",
       " 164,\n",
       " 153,\n",
       " 147,\n",
       " 987,\n",
       " 485,\n",
       " 539,\n",
       " 761,\n",
       " 1326,\n",
       " 1423,\n",
       " 489,\n",
       " 1146,\n",
       " 328,\n",
       " 421,\n",
       " 268,\n",
       " 401,\n",
       " 405,\n",
       " 910,\n",
       " 408,\n",
       " 895,\n",
       " 1175,\n",
       " 1167,\n",
       " 230,\n",
       " 3966,\n",
       " 138,\n",
       " 459,\n",
       " 584,\n",
       " 1549,\n",
       " 258,\n",
       " 279,\n",
       " 898,\n",
       " 643,\n",
       " 368,\n",
       " 1267,\n",
       " 1279,\n",
       " 1081,\n",
       " 574,\n",
       " 629,\n",
       " 583,\n",
       " 634,\n",
       " 677,\n",
       " 852,\n",
       " 971,\n",
       " 462,\n",
       " 482,\n",
       " 631]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "for file in files:\n",
    "    lengths.append(len(tokenizer_big.tokenize(get_html(file))))\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8479\n",
      "Show_available_nodes.html 8479\n",
      "SLURM_JobSubmission.html 8464\n"
     ]
    }
   ],
   "source": [
    "print(max(lengths))\n",
    "for i, l, in enumerate(lengths):\n",
    "    if l >= max(lengths) - 100:\n",
    "        print(files[i], l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accounts.html\n",
      "Accounts_Collaborator.html\n",
      "Adobe.html\n",
      "Archives.html\n",
      "ATL_ConferenceRooms.html\n",
      "Backups.html\n",
      "BarracudaSpamFirewall_QuarantinePassthrough.html\n",
      "Bash.html\n",
      "BashForWindows10.html\n",
      "ClassAccounts_Manage.html\n",
      "CompromisedPasswordFiltering.html\n",
      "ConferenceRooms_ATL3100A.html\n",
      "ConferenceRooms_ATL3100C.html\n",
      "ConferenceRooms_ATL3100D.html\n",
      "ConferenceRooms_Recording.html\n",
      "CoreServices.html\n",
      "Creating_service_admin_user_on_OSX.html\n",
      "EuclidCluster.html\n",
      "FileTransferProtocol.html\n",
      "Fixing_Stuck_Standing_Desk.html\n",
      "HPC.html\n",
      "Iribe.html\n",
      "Iribe_ConferenceRooms_HuddleRoom.html\n",
      "Iribe_Mailroom.html\n",
      "Jekyll.html\n",
      "Jira.html\n",
      "LANDesk.html\n",
      "LDAP.html\n",
      "MacOSDisplayModes.html\n",
      "Main_Page.html\n",
      "MalwareRecovery.html\n",
      "Malware_virus_removal.html\n",
      "Mathematica.html\n",
      "MathematicaActivation.html\n",
      "MFA.html\n",
      "Mpich1.csh.html\n",
      "NAS.html\n",
      "NASProjects.html\n",
      "NautilusThumbnails.html\n",
      "Network.html\n",
      "Network_VPN.html\n",
      "Network_VPN_Android.html\n",
      "NFShomes.html\n",
      "NightlyBackups.html\n",
      "Perl.html\n",
      "QuICS.html\n",
      "RedHat.html\n",
      "RHEL.html\n",
      "Security.html\n",
      "Services_Compute_UserSupported.html\n",
      "Stow.html\n"
     ]
    }
   ],
   "source": [
    "for i, l in enumerate(lengths):\n",
    "    if l in lengths[i + 1:]:\n",
    "        print(files[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
