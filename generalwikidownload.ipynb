{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this only works mediawiki wikis. I think almost every popular wiki uses this, and it's what you see in your head when you hear \"wiki\", ex: wikipedia uses mediawiki, fandom uses mediawiki, random wiki you can think of uses mediawiki, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import os\n",
    "import shutil\n",
    "from slugify import slugify # python-slugify because apparently slugify is also a thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://criminalminds.fandom.com\"  # i have to separate them for the href idk how else to do it\n",
    "wikipath = \"/wiki\"\n",
    "ignorecomments = True\n",
    "discardtags = [\"style\", \"script\"]\n",
    "folderpath = \"./ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1: https://criminalminds.fandom.com/wiki/Special:AllPages\n",
      "page 2: https://criminalminds.fandom.com/wiki/Special:AllPages?from=Behind+the+Scenes%2FTake+4\n",
      "page 3: https://criminalminds.fandom.com/wiki/Special:AllPages?from=Christie+Lynn+Smith\n",
      "page 4: https://criminalminds.fandom.com/wiki/Special:AllPages?from=David+Wells\n",
      "page 5: https://criminalminds.fandom.com/wiki/Special:AllPages?from=Fran%C3%A7ois+Chau\n",
      "page 6: https://criminalminds.fandom.com/wiki/Special:AllPages?from=J.P.+Giuliotti\n",
      "page 7: https://criminalminds.fandom.com/wiki/Special:AllPages?from=Johnny+Lewis\n",
      "page 8: https://criminalminds.fandom.com/wiki/Special:AllPages?from=Lex+Medlin\n",
      "page 9: https://criminalminds.fandom.com/wiki/Special:AllPages?from=Medalion+Rahimi\n",
      "page 10: https://criminalminds.fandom.com/wiki/Special:AllPages?from=Paul+Carafotes\n",
      "page 11: https://criminalminds.fandom.com/wiki/Special:AllPages?from=Robert+Ressler\n",
      "page 12: https://criminalminds.fandom.com/wiki/Special:AllPages?from=Slade+Pearce\n",
      "page 13: https://criminalminds.fandom.com/wiki/Special:AllPages?from=The+Slave+Of+Duty\n",
      "page 14: https://criminalminds.fandom.com/wiki/Special:AllPages?from=Zodiac+Killer\n",
      "4118 4118\n"
     ]
    }
   ],
   "source": [
    "allpages = baseurl + wikipath + \"/Special:AllPages\"\n",
    "bads = [\"AllPages\", \"SpecialPages\", \"Special:\"]\n",
    "\n",
    "pageset = []\n",
    "\n",
    "currpage = requests.get(allpages)\n",
    "currpage = BeautifulSoup(currpage.content, \"html.parser\")\n",
    "\n",
    "pagenum = 1\n",
    "nexturl = None  # housekeeping\n",
    "while currpage is not None:\n",
    "    print(f\"page {pagenum}: {allpages if not nexturl else nexturl}\")\n",
    "    pagenum += 1\n",
    "    for pageli in currpage.select_one(\".mw-allpages-body\").find_all(\"li\"):\n",
    "        liclass = pageli.get(\"class\")\n",
    "        if liclass is not None and liclass[0] == \"allpagesredirect\":\n",
    "            continue\n",
    "        href = pageli.select_one(\"a\")[\"href\"]\n",
    "\n",
    "        if href and not any(bad in href for bad in bads):\n",
    "            pageset.append(baseurl + href)\n",
    "\n",
    "    nav = currpage.select_one(\".mw-allpages-nav\")\n",
    "    currpage = None\n",
    "    if nav is not None:\n",
    "        for navoption in nav.find_all(\"a\"):  # if no \"a\" should still only be a tags\n",
    "            if \"Next page\" in navoption.string:\n",
    "                nexturl = baseurl + navoption[\"href\"]\n",
    "                currpage = requests.get(nexturl)\n",
    "                currpage = BeautifulSoup(currpage.content, \"html.parser\")\n",
    "                break\n",
    "\n",
    "print(len(pageset), len(set(pageset)))\n",
    "if len(pageset) != len(set(pageset)):\n",
    "    print(\"some duplicates made their way through. my code (or the wiki) has a bug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_name(url):\n",
    "    name = url.split(f\"{wikipath}/\")[1].split(\"?\")[\n",
    "        0\n",
    "    ]  # hope ? isn't in the actual page name part\n",
    "    name = slugify(name, lowercase=False)\n",
    "    return name\n",
    "\n",
    "\n",
    "def prune_html(rawhtml):\n",
    "    soup = BeautifulSoup(rawhtml, \"html.parser\")\n",
    "    if len(discardtags) > 0:  # because soup([]) gets everything\n",
    "        for element in soup(discardtags):\n",
    "            element.extract()\n",
    "    if ignorecomments:\n",
    "        comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "        for comment in comments:\n",
    "            comment.extract()\n",
    "    html = str(soup)\n",
    "    return html\n",
    "\n",
    "\n",
    "async def fetch_and_save(session, url):\n",
    "    filename = prepare_name(url)\n",
    "    async with session.get(url) as response, aiofiles.open(\n",
    "        f\"{folderpath}/{filename}.html\", \"w\"\n",
    "    ) as file:\n",
    "        content = await response.text()\n",
    "        content = prune_html(content)\n",
    "        return await file.write(content)\n",
    "\n",
    "\n",
    "async def fetch_and_save_all(links):\n",
    "    async with aiohttp.ClientSession(requote_redirect_url=False) as session:\n",
    "        # parameter is necessary cuz if u have & in link, fandom replaces it with %26 and redirects you, but this library turns it back into a &........\n",
    "        tasks = [fetch_and_save(session, url) for url in links]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "def init_folder(reset=True):\n",
    "    if reset and os.path.exists(folderpath):\n",
    "        shutil.rmtree(folderpath)\n",
    "    if not os.path.exists(folderpath):\n",
    "        os.makedirs(folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew\\AppData\\Local\\Temp\\ipykernel_30480\\1484359785.py:6: RuntimeWarning: coroutine 'fetch_and_save_all' was never awaited\n",
      "  await fetch_and_save_all(pageset)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "init_folder(reset=False)\n",
    "try:\n",
    "    asyncio.run(fetch_and_save_all(pageset))\n",
    "except RuntimeError as e:\n",
    "    if \"running event loop\" in str(e):\n",
    "        await fetch_and_save_all(pageset)\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
