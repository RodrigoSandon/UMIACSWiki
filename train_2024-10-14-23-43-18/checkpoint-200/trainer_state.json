{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8913649025069638,
  "eval_steps": 500,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.022284122562674095,
      "grad_norm": 3.0243477821350098,
      "learning_rate": 4.999317046010329e-05,
      "loss": 3.0344,
      "num_input_tokens_seen": 4400,
      "step": 5
    },
    {
      "epoch": 0.04456824512534819,
      "grad_norm": 3.6294679641723633,
      "learning_rate": 4.997268557182235e-05,
      "loss": 2.5359,
      "num_input_tokens_seen": 8608,
      "step": 10
    },
    {
      "epoch": 0.06685236768802229,
      "grad_norm": 1.864174246788025,
      "learning_rate": 4.9938556527346155e-05,
      "loss": 1.9766,
      "num_input_tokens_seen": 12944,
      "step": 15
    },
    {
      "epoch": 0.08913649025069638,
      "grad_norm": 1.6980981826782227,
      "learning_rate": 4.989080197352834e-05,
      "loss": 1.9466,
      "num_input_tokens_seen": 17472,
      "step": 20
    },
    {
      "epoch": 0.11142061281337047,
      "grad_norm": 1.8951458930969238,
      "learning_rate": 4.9829448001699384e-05,
      "loss": 1.9667,
      "num_input_tokens_seen": 21600,
      "step": 25
    },
    {
      "epoch": 0.13370473537604458,
      "grad_norm": 2.212496042251587,
      "learning_rate": 4.975452813341114e-05,
      "loss": 1.8393,
      "num_input_tokens_seen": 25952,
      "step": 30
    },
    {
      "epoch": 0.15598885793871867,
      "grad_norm": 1.5536103248596191,
      "learning_rate": 4.966608330212198e-05,
      "loss": 1.7832,
      "num_input_tokens_seen": 30176,
      "step": 35
    },
    {
      "epoch": 0.17827298050139276,
      "grad_norm": 1.4920223951339722,
      "learning_rate": 4.956416183083221e-05,
      "loss": 1.6885,
      "num_input_tokens_seen": 34400,
      "step": 40
    },
    {
      "epoch": 0.20055710306406685,
      "grad_norm": 1.9712514877319336,
      "learning_rate": 4.9448819405682193e-05,
      "loss": 1.6146,
      "num_input_tokens_seen": 38512,
      "step": 45
    },
    {
      "epoch": 0.22284122562674094,
      "grad_norm": 1.8144046068191528,
      "learning_rate": 4.9320119045527487e-05,
      "loss": 1.5887,
      "num_input_tokens_seen": 42864,
      "step": 50
    },
    {
      "epoch": 0.24512534818941503,
      "grad_norm": 1.598764181137085,
      "learning_rate": 4.9178131067507625e-05,
      "loss": 1.6543,
      "num_input_tokens_seen": 47200,
      "step": 55
    },
    {
      "epoch": 0.26740947075208915,
      "grad_norm": 1.686844825744629,
      "learning_rate": 4.9022933048627496e-05,
      "loss": 1.5921,
      "num_input_tokens_seen": 51456,
      "step": 60
    },
    {
      "epoch": 0.28969359331476324,
      "grad_norm": 1.6943036317825317,
      "learning_rate": 4.8854609783372014e-05,
      "loss": 1.5432,
      "num_input_tokens_seen": 55760,
      "step": 65
    },
    {
      "epoch": 0.31197771587743733,
      "grad_norm": 1.8688290119171143,
      "learning_rate": 4.867325323737765e-05,
      "loss": 1.6125,
      "num_input_tokens_seen": 60192,
      "step": 70
    },
    {
      "epoch": 0.3342618384401114,
      "grad_norm": 1.682340383529663,
      "learning_rate": 4.84789624971857e-05,
      "loss": 1.5767,
      "num_input_tokens_seen": 64480,
      "step": 75
    },
    {
      "epoch": 0.3565459610027855,
      "grad_norm": 2.0465023517608643,
      "learning_rate": 4.827184371610511e-05,
      "loss": 1.626,
      "num_input_tokens_seen": 68816,
      "step": 80
    },
    {
      "epoch": 0.3788300835654596,
      "grad_norm": 3.37603497505188,
      "learning_rate": 4.805201005621418e-05,
      "loss": 1.6389,
      "num_input_tokens_seen": 73232,
      "step": 85
    },
    {
      "epoch": 0.4011142061281337,
      "grad_norm": 1.8935409784317017,
      "learning_rate": 4.781958162653297e-05,
      "loss": 1.6279,
      "num_input_tokens_seen": 77648,
      "step": 90
    },
    {
      "epoch": 0.4233983286908078,
      "grad_norm": 1.8949732780456543,
      "learning_rate": 4.757468541740019e-05,
      "loss": 1.6247,
      "num_input_tokens_seen": 81760,
      "step": 95
    },
    {
      "epoch": 0.4456824512534819,
      "grad_norm": 1.6411949396133423,
      "learning_rate": 4.731745523109029e-05,
      "loss": 1.5315,
      "num_input_tokens_seen": 86192,
      "step": 100
    },
    {
      "epoch": 0.467966573816156,
      "grad_norm": 2.0290815830230713,
      "learning_rate": 4.7048031608708876e-05,
      "loss": 1.6998,
      "num_input_tokens_seen": 90208,
      "step": 105
    },
    {
      "epoch": 0.49025069637883006,
      "grad_norm": 1.789925456047058,
      "learning_rate": 4.676656175340621e-05,
      "loss": 1.6907,
      "num_input_tokens_seen": 94688,
      "step": 110
    },
    {
      "epoch": 0.5125348189415042,
      "grad_norm": 2.2278521060943604,
      "learning_rate": 4.64731994499508e-05,
      "loss": 1.4913,
      "num_input_tokens_seen": 99120,
      "step": 115
    },
    {
      "epoch": 0.5348189415041783,
      "grad_norm": 1.9144400358200073,
      "learning_rate": 4.6168104980707107e-05,
      "loss": 1.5054,
      "num_input_tokens_seen": 103632,
      "step": 120
    },
    {
      "epoch": 0.5571030640668524,
      "grad_norm": 3.333179235458374,
      "learning_rate": 4.585144503806312e-05,
      "loss": 1.6061,
      "num_input_tokens_seen": 107968,
      "step": 125
    },
    {
      "epoch": 0.5793871866295265,
      "grad_norm": 1.83481764793396,
      "learning_rate": 4.552339263335581e-05,
      "loss": 1.6515,
      "num_input_tokens_seen": 112400,
      "step": 130
    },
    {
      "epoch": 0.6016713091922006,
      "grad_norm": 1.8374454975128174,
      "learning_rate": 4.518412700234406e-05,
      "loss": 1.4213,
      "num_input_tokens_seen": 116704,
      "step": 135
    },
    {
      "epoch": 0.6239554317548747,
      "grad_norm": 2.503974437713623,
      "learning_rate": 4.4833833507280884e-05,
      "loss": 1.411,
      "num_input_tokens_seen": 120848,
      "step": 140
    },
    {
      "epoch": 0.6462395543175488,
      "grad_norm": 2.86281418800354,
      "learning_rate": 4.447270353563828e-05,
      "loss": 1.5449,
      "num_input_tokens_seen": 125216,
      "step": 145
    },
    {
      "epoch": 0.6685236768802229,
      "grad_norm": 2.9234931468963623,
      "learning_rate": 4.410093439554019e-05,
      "loss": 1.536,
      "num_input_tokens_seen": 129616,
      "step": 150
    },
    {
      "epoch": 0.6908077994428969,
      "grad_norm": 1.8262721300125122,
      "learning_rate": 4.3718729207960586e-05,
      "loss": 1.5422,
      "num_input_tokens_seen": 134080,
      "step": 155
    },
    {
      "epoch": 0.713091922005571,
      "grad_norm": 2.036581039428711,
      "learning_rate": 4.332629679574566e-05,
      "loss": 1.4544,
      "num_input_tokens_seen": 138528,
      "step": 160
    },
    {
      "epoch": 0.7353760445682451,
      "grad_norm": 2.210577964782715,
      "learning_rate": 4.2923851569520685e-05,
      "loss": 1.4864,
      "num_input_tokens_seen": 142672,
      "step": 165
    },
    {
      "epoch": 0.7576601671309192,
      "grad_norm": 3.3150064945220947,
      "learning_rate": 4.251161341054396e-05,
      "loss": 1.4795,
      "num_input_tokens_seen": 146912,
      "step": 170
    },
    {
      "epoch": 0.7799442896935933,
      "grad_norm": 2.3394458293914795,
      "learning_rate": 4.208980755057178e-05,
      "loss": 1.4489,
      "num_input_tokens_seen": 151264,
      "step": 175
    },
    {
      "epoch": 0.8022284122562674,
      "grad_norm": 1.9560227394104004,
      "learning_rate": 4.16586644488001e-05,
      "loss": 1.4483,
      "num_input_tokens_seen": 155392,
      "step": 180
    },
    {
      "epoch": 0.8245125348189415,
      "grad_norm": 2.055889844894409,
      "learning_rate": 4.1218419665950094e-05,
      "loss": 1.4264,
      "num_input_tokens_seen": 159696,
      "step": 185
    },
    {
      "epoch": 0.8467966573816156,
      "grad_norm": 2.2068474292755127,
      "learning_rate": 4.076931373556646e-05,
      "loss": 1.5578,
      "num_input_tokens_seen": 164176,
      "step": 190
    },
    {
      "epoch": 0.8690807799442897,
      "grad_norm": 2.7869811058044434,
      "learning_rate": 4.0311592032598754e-05,
      "loss": 1.4489,
      "num_input_tokens_seen": 168480,
      "step": 195
    },
    {
      "epoch": 0.8913649025069638,
      "grad_norm": 2.371483087539673,
      "learning_rate": 3.9845504639337535e-05,
      "loss": 1.7075,
      "num_input_tokens_seen": 172800,
      "step": 200
    }
  ],
  "logging_steps": 5,
  "max_steps": 672,
  "num_input_tokens_seen": 172800,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7802849171865600.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
