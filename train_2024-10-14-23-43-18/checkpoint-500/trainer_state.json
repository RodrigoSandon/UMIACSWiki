{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.2284122562674096,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.022284122562674095,
      "grad_norm": 3.0243477821350098,
      "learning_rate": 4.999317046010329e-05,
      "loss": 3.0344,
      "num_input_tokens_seen": 4400,
      "step": 5
    },
    {
      "epoch": 0.04456824512534819,
      "grad_norm": 3.6294679641723633,
      "learning_rate": 4.997268557182235e-05,
      "loss": 2.5359,
      "num_input_tokens_seen": 8608,
      "step": 10
    },
    {
      "epoch": 0.06685236768802229,
      "grad_norm": 1.864174246788025,
      "learning_rate": 4.9938556527346155e-05,
      "loss": 1.9766,
      "num_input_tokens_seen": 12944,
      "step": 15
    },
    {
      "epoch": 0.08913649025069638,
      "grad_norm": 1.6980981826782227,
      "learning_rate": 4.989080197352834e-05,
      "loss": 1.9466,
      "num_input_tokens_seen": 17472,
      "step": 20
    },
    {
      "epoch": 0.11142061281337047,
      "grad_norm": 1.8951458930969238,
      "learning_rate": 4.9829448001699384e-05,
      "loss": 1.9667,
      "num_input_tokens_seen": 21600,
      "step": 25
    },
    {
      "epoch": 0.13370473537604458,
      "grad_norm": 2.212496042251587,
      "learning_rate": 4.975452813341114e-05,
      "loss": 1.8393,
      "num_input_tokens_seen": 25952,
      "step": 30
    },
    {
      "epoch": 0.15598885793871867,
      "grad_norm": 1.5536103248596191,
      "learning_rate": 4.966608330212198e-05,
      "loss": 1.7832,
      "num_input_tokens_seen": 30176,
      "step": 35
    },
    {
      "epoch": 0.17827298050139276,
      "grad_norm": 1.4920223951339722,
      "learning_rate": 4.956416183083221e-05,
      "loss": 1.6885,
      "num_input_tokens_seen": 34400,
      "step": 40
    },
    {
      "epoch": 0.20055710306406685,
      "grad_norm": 1.9712514877319336,
      "learning_rate": 4.9448819405682193e-05,
      "loss": 1.6146,
      "num_input_tokens_seen": 38512,
      "step": 45
    },
    {
      "epoch": 0.22284122562674094,
      "grad_norm": 1.8144046068191528,
      "learning_rate": 4.9320119045527487e-05,
      "loss": 1.5887,
      "num_input_tokens_seen": 42864,
      "step": 50
    },
    {
      "epoch": 0.24512534818941503,
      "grad_norm": 1.598764181137085,
      "learning_rate": 4.9178131067507625e-05,
      "loss": 1.6543,
      "num_input_tokens_seen": 47200,
      "step": 55
    },
    {
      "epoch": 0.26740947075208915,
      "grad_norm": 1.686844825744629,
      "learning_rate": 4.9022933048627496e-05,
      "loss": 1.5921,
      "num_input_tokens_seen": 51456,
      "step": 60
    },
    {
      "epoch": 0.28969359331476324,
      "grad_norm": 1.6943036317825317,
      "learning_rate": 4.8854609783372014e-05,
      "loss": 1.5432,
      "num_input_tokens_seen": 55760,
      "step": 65
    },
    {
      "epoch": 0.31197771587743733,
      "grad_norm": 1.8688290119171143,
      "learning_rate": 4.867325323737765e-05,
      "loss": 1.6125,
      "num_input_tokens_seen": 60192,
      "step": 70
    },
    {
      "epoch": 0.3342618384401114,
      "grad_norm": 1.682340383529663,
      "learning_rate": 4.84789624971857e-05,
      "loss": 1.5767,
      "num_input_tokens_seen": 64480,
      "step": 75
    },
    {
      "epoch": 0.3565459610027855,
      "grad_norm": 2.0465023517608643,
      "learning_rate": 4.827184371610511e-05,
      "loss": 1.626,
      "num_input_tokens_seen": 68816,
      "step": 80
    },
    {
      "epoch": 0.3788300835654596,
      "grad_norm": 3.37603497505188,
      "learning_rate": 4.805201005621418e-05,
      "loss": 1.6389,
      "num_input_tokens_seen": 73232,
      "step": 85
    },
    {
      "epoch": 0.4011142061281337,
      "grad_norm": 1.8935409784317017,
      "learning_rate": 4.781958162653297e-05,
      "loss": 1.6279,
      "num_input_tokens_seen": 77648,
      "step": 90
    },
    {
      "epoch": 0.4233983286908078,
      "grad_norm": 1.8949732780456543,
      "learning_rate": 4.757468541740019e-05,
      "loss": 1.6247,
      "num_input_tokens_seen": 81760,
      "step": 95
    },
    {
      "epoch": 0.4456824512534819,
      "grad_norm": 1.6411949396133423,
      "learning_rate": 4.731745523109029e-05,
      "loss": 1.5315,
      "num_input_tokens_seen": 86192,
      "step": 100
    },
    {
      "epoch": 0.467966573816156,
      "grad_norm": 2.0290815830230713,
      "learning_rate": 4.7048031608708876e-05,
      "loss": 1.6998,
      "num_input_tokens_seen": 90208,
      "step": 105
    },
    {
      "epoch": 0.49025069637883006,
      "grad_norm": 1.789925456047058,
      "learning_rate": 4.676656175340621e-05,
      "loss": 1.6907,
      "num_input_tokens_seen": 94688,
      "step": 110
    },
    {
      "epoch": 0.5125348189415042,
      "grad_norm": 2.2278521060943604,
      "learning_rate": 4.64731994499508e-05,
      "loss": 1.4913,
      "num_input_tokens_seen": 99120,
      "step": 115
    },
    {
      "epoch": 0.5348189415041783,
      "grad_norm": 1.9144400358200073,
      "learning_rate": 4.6168104980707107e-05,
      "loss": 1.5054,
      "num_input_tokens_seen": 103632,
      "step": 120
    },
    {
      "epoch": 0.5571030640668524,
      "grad_norm": 3.333179235458374,
      "learning_rate": 4.585144503806312e-05,
      "loss": 1.6061,
      "num_input_tokens_seen": 107968,
      "step": 125
    },
    {
      "epoch": 0.5793871866295265,
      "grad_norm": 1.83481764793396,
      "learning_rate": 4.552339263335581e-05,
      "loss": 1.6515,
      "num_input_tokens_seen": 112400,
      "step": 130
    },
    {
      "epoch": 0.6016713091922006,
      "grad_norm": 1.8374454975128174,
      "learning_rate": 4.518412700234406e-05,
      "loss": 1.4213,
      "num_input_tokens_seen": 116704,
      "step": 135
    },
    {
      "epoch": 0.6239554317548747,
      "grad_norm": 2.503974437713623,
      "learning_rate": 4.4833833507280884e-05,
      "loss": 1.411,
      "num_input_tokens_seen": 120848,
      "step": 140
    },
    {
      "epoch": 0.6462395543175488,
      "grad_norm": 2.86281418800354,
      "learning_rate": 4.447270353563828e-05,
      "loss": 1.5449,
      "num_input_tokens_seen": 125216,
      "step": 145
    },
    {
      "epoch": 0.6685236768802229,
      "grad_norm": 2.9234931468963623,
      "learning_rate": 4.410093439554019e-05,
      "loss": 1.536,
      "num_input_tokens_seen": 129616,
      "step": 150
    },
    {
      "epoch": 0.6908077994428969,
      "grad_norm": 1.8262721300125122,
      "learning_rate": 4.3718729207960586e-05,
      "loss": 1.5422,
      "num_input_tokens_seen": 134080,
      "step": 155
    },
    {
      "epoch": 0.713091922005571,
      "grad_norm": 2.036581039428711,
      "learning_rate": 4.332629679574566e-05,
      "loss": 1.4544,
      "num_input_tokens_seen": 138528,
      "step": 160
    },
    {
      "epoch": 0.7353760445682451,
      "grad_norm": 2.210577964782715,
      "learning_rate": 4.2923851569520685e-05,
      "loss": 1.4864,
      "num_input_tokens_seen": 142672,
      "step": 165
    },
    {
      "epoch": 0.7576601671309192,
      "grad_norm": 3.3150064945220947,
      "learning_rate": 4.251161341054396e-05,
      "loss": 1.4795,
      "num_input_tokens_seen": 146912,
      "step": 170
    },
    {
      "epoch": 0.7799442896935933,
      "grad_norm": 2.3394458293914795,
      "learning_rate": 4.208980755057178e-05,
      "loss": 1.4489,
      "num_input_tokens_seen": 151264,
      "step": 175
    },
    {
      "epoch": 0.8022284122562674,
      "grad_norm": 1.9560227394104004,
      "learning_rate": 4.16586644488001e-05,
      "loss": 1.4483,
      "num_input_tokens_seen": 155392,
      "step": 180
    },
    {
      "epoch": 0.8245125348189415,
      "grad_norm": 2.055889844894409,
      "learning_rate": 4.1218419665950094e-05,
      "loss": 1.4264,
      "num_input_tokens_seen": 159696,
      "step": 185
    },
    {
      "epoch": 0.8467966573816156,
      "grad_norm": 2.2068474292755127,
      "learning_rate": 4.076931373556646e-05,
      "loss": 1.5578,
      "num_input_tokens_seen": 164176,
      "step": 190
    },
    {
      "epoch": 0.8690807799442897,
      "grad_norm": 2.7869811058044434,
      "learning_rate": 4.0311592032598754e-05,
      "loss": 1.4489,
      "num_input_tokens_seen": 168480,
      "step": 195
    },
    {
      "epoch": 0.8913649025069638,
      "grad_norm": 2.371483087539673,
      "learning_rate": 3.9845504639337535e-05,
      "loss": 1.7075,
      "num_input_tokens_seen": 172800,
      "step": 200
    },
    {
      "epoch": 0.9136490250696379,
      "grad_norm": 2.1665761470794678,
      "learning_rate": 3.937130620877863e-05,
      "loss": 1.4593,
      "num_input_tokens_seen": 177184,
      "step": 205
    },
    {
      "epoch": 0.935933147632312,
      "grad_norm": 2.37739896774292,
      "learning_rate": 3.888925582549006e-05,
      "loss": 1.5757,
      "num_input_tokens_seen": 181600,
      "step": 210
    },
    {
      "epoch": 0.958217270194986,
      "grad_norm": 2.5308849811553955,
      "learning_rate": 3.8399616864057816e-05,
      "loss": 1.4399,
      "num_input_tokens_seen": 185792,
      "step": 215
    },
    {
      "epoch": 0.9805013927576601,
      "grad_norm": 2.579838514328003,
      "learning_rate": 3.790265684518767e-05,
      "loss": 1.5097,
      "num_input_tokens_seen": 190112,
      "step": 220
    },
    {
      "epoch": 1.0027855153203342,
      "grad_norm": 2.775521755218506,
      "learning_rate": 3.73986472895417e-05,
      "loss": 1.452,
      "num_input_tokens_seen": 194192,
      "step": 225
    },
    {
      "epoch": 1.0250696378830084,
      "grad_norm": 2.33152174949646,
      "learning_rate": 3.6887863569389386e-05,
      "loss": 1.3127,
      "num_input_tokens_seen": 198560,
      "step": 230
    },
    {
      "epoch": 1.0473537604456824,
      "grad_norm": 2.369178056716919,
      "learning_rate": 3.6370584758154366e-05,
      "loss": 1.5808,
      "num_input_tokens_seen": 202880,
      "step": 235
    },
    {
      "epoch": 1.0696378830083566,
      "grad_norm": 2.2536191940307617,
      "learning_rate": 3.5847093477938956e-05,
      "loss": 1.2569,
      "num_input_tokens_seen": 207216,
      "step": 240
    },
    {
      "epoch": 1.0919220055710306,
      "grad_norm": 2.413609266281128,
      "learning_rate": 3.5317675745109866e-05,
      "loss": 1.2092,
      "num_input_tokens_seen": 211360,
      "step": 245
    },
    {
      "epoch": 1.1142061281337048,
      "grad_norm": 2.7684261798858643,
      "learning_rate": 3.4782620814029375e-05,
      "loss": 1.2704,
      "num_input_tokens_seen": 215872,
      "step": 250
    },
    {
      "epoch": 1.1364902506963788,
      "grad_norm": 2.8164494037628174,
      "learning_rate": 3.424222101901738e-05,
      "loss": 1.182,
      "num_input_tokens_seen": 220000,
      "step": 255
    },
    {
      "epoch": 1.158774373259053,
      "grad_norm": 3.1143815517425537,
      "learning_rate": 3.369677161463068e-05,
      "loss": 1.3262,
      "num_input_tokens_seen": 224304,
      "step": 260
    },
    {
      "epoch": 1.181058495821727,
      "grad_norm": 2.7332279682159424,
      "learning_rate": 3.314657061434681e-05,
      "loss": 1.3304,
      "num_input_tokens_seen": 228736,
      "step": 265
    },
    {
      "epoch": 1.2033426183844012,
      "grad_norm": 3.510199546813965,
      "learning_rate": 3.259191862774037e-05,
      "loss": 1.3506,
      "num_input_tokens_seen": 233088,
      "step": 270
    },
    {
      "epoch": 1.2256267409470751,
      "grad_norm": 4.1530303955078125,
      "learning_rate": 3.203311869624107e-05,
      "loss": 1.1504,
      "num_input_tokens_seen": 237408,
      "step": 275
    },
    {
      "epoch": 1.2479108635097493,
      "grad_norm": 2.742326498031616,
      "learning_rate": 3.147047612756302e-05,
      "loss": 1.3063,
      "num_input_tokens_seen": 241744,
      "step": 280
    },
    {
      "epoch": 1.2701949860724233,
      "grad_norm": 3.0238027572631836,
      "learning_rate": 3.090429832889586e-05,
      "loss": 1.1873,
      "num_input_tokens_seen": 246096,
      "step": 285
    },
    {
      "epoch": 1.2924791086350975,
      "grad_norm": 3.4958889484405518,
      "learning_rate": 3.0334894638948753e-05,
      "loss": 1.3221,
      "num_input_tokens_seen": 250432,
      "step": 290
    },
    {
      "epoch": 1.3147632311977717,
      "grad_norm": 2.8366010189056396,
      "learning_rate": 2.9762576158939125e-05,
      "loss": 1.1904,
      "num_input_tokens_seen": 254784,
      "step": 295
    },
    {
      "epoch": 1.3370473537604457,
      "grad_norm": 3.5559847354888916,
      "learning_rate": 2.918765558261841e-05,
      "loss": 1.2938,
      "num_input_tokens_seen": 259344,
      "step": 300
    },
    {
      "epoch": 1.3593314763231197,
      "grad_norm": 3.134235382080078,
      "learning_rate": 2.8610447025427683e-05,
      "loss": 1.344,
      "num_input_tokens_seen": 263600,
      "step": 305
    },
    {
      "epoch": 1.3816155988857939,
      "grad_norm": 3.509870767593384,
      "learning_rate": 2.8031265852876537e-05,
      "loss": 1.1681,
      "num_input_tokens_seen": 267888,
      "step": 310
    },
    {
      "epoch": 1.403899721448468,
      "grad_norm": 3.1615304946899414,
      "learning_rate": 2.7450428508239024e-05,
      "loss": 1.2522,
      "num_input_tokens_seen": 272208,
      "step": 315
    },
    {
      "epoch": 1.426183844011142,
      "grad_norm": 3.0251753330230713,
      "learning_rate": 2.686825233966061e-05,
      "loss": 1.1555,
      "num_input_tokens_seen": 276512,
      "step": 320
    },
    {
      "epoch": 1.448467966573816,
      "grad_norm": 2.9840028285980225,
      "learning_rate": 2.6285055426770934e-05,
      "loss": 1.2186,
      "num_input_tokens_seen": 280928,
      "step": 325
    },
    {
      "epoch": 1.4707520891364902,
      "grad_norm": 4.011053562164307,
      "learning_rate": 2.5701156406896725e-05,
      "loss": 1.0896,
      "num_input_tokens_seen": 285008,
      "step": 330
    },
    {
      "epoch": 1.4930362116991645,
      "grad_norm": 3.5404889583587646,
      "learning_rate": 2.5116874300970138e-05,
      "loss": 1.4326,
      "num_input_tokens_seen": 289264,
      "step": 335
    },
    {
      "epoch": 1.5153203342618384,
      "grad_norm": 2.750112533569336,
      "learning_rate": 2.4532528339227452e-05,
      "loss": 1.3034,
      "num_input_tokens_seen": 293648,
      "step": 340
    },
    {
      "epoch": 1.5376044568245124,
      "grad_norm": 3.680396556854248,
      "learning_rate": 2.3948437786793377e-05,
      "loss": 1.1998,
      "num_input_tokens_seen": 297760,
      "step": 345
    },
    {
      "epoch": 1.5598885793871866,
      "grad_norm": 3.8092727661132812,
      "learning_rate": 2.3364921769246423e-05,
      "loss": 1.3357,
      "num_input_tokens_seen": 302128,
      "step": 350
    },
    {
      "epoch": 1.5821727019498608,
      "grad_norm": 3.3261232376098633,
      "learning_rate": 2.278229909826037e-05,
      "loss": 1.2066,
      "num_input_tokens_seen": 306304,
      "step": 355
    },
    {
      "epoch": 1.6044568245125348,
      "grad_norm": 2.991302490234375,
      "learning_rate": 2.2200888097417307e-05,
      "loss": 1.1233,
      "num_input_tokens_seen": 310512,
      "step": 360
    },
    {
      "epoch": 1.6267409470752088,
      "grad_norm": 2.8760874271392822,
      "learning_rate": 2.162100642828737e-05,
      "loss": 1.115,
      "num_input_tokens_seen": 314944,
      "step": 365
    },
    {
      "epoch": 1.649025069637883,
      "grad_norm": 3.1403517723083496,
      "learning_rate": 2.104297091687013e-05,
      "loss": 1.2218,
      "num_input_tokens_seen": 319312,
      "step": 370
    },
    {
      "epoch": 1.6713091922005572,
      "grad_norm": 4.2717742919921875,
      "learning_rate": 2.0467097380492544e-05,
      "loss": 1.1957,
      "num_input_tokens_seen": 323664,
      "step": 375
    },
    {
      "epoch": 1.6935933147632312,
      "grad_norm": 5.2042975425720215,
      "learning_rate": 1.9893700455257996e-05,
      "loss": 1.2455,
      "num_input_tokens_seen": 327904,
      "step": 380
    },
    {
      "epoch": 1.7158774373259051,
      "grad_norm": 3.6646888256073,
      "learning_rate": 1.932309342414067e-05,
      "loss": 1.1768,
      "num_input_tokens_seen": 332304,
      "step": 385
    },
    {
      "epoch": 1.7381615598885793,
      "grad_norm": 3.513810873031616,
      "learning_rate": 1.8755588045819327e-05,
      "loss": 1.2777,
      "num_input_tokens_seen": 336784,
      "step": 390
    },
    {
      "epoch": 1.7604456824512535,
      "grad_norm": 2.925241231918335,
      "learning_rate": 1.81914943843438e-05,
      "loss": 1.2077,
      "num_input_tokens_seen": 341088,
      "step": 395
    },
    {
      "epoch": 1.7827298050139275,
      "grad_norm": 3.3884339332580566,
      "learning_rate": 1.7631120639727393e-05,
      "loss": 1.0483,
      "num_input_tokens_seen": 345424,
      "step": 400
    },
    {
      "epoch": 1.8050139275766015,
      "grad_norm": 3.4629807472229004,
      "learning_rate": 1.7074772979557802e-05,
      "loss": 1.1946,
      "num_input_tokens_seen": 349728,
      "step": 405
    },
    {
      "epoch": 1.8272980501392757,
      "grad_norm": 3.8593883514404297,
      "learning_rate": 1.6522755371718333e-05,
      "loss": 1.1723,
      "num_input_tokens_seen": 353712,
      "step": 410
    },
    {
      "epoch": 1.84958217270195,
      "grad_norm": 3.020015001296997,
      "learning_rate": 1.5975369418311112e-05,
      "loss": 1.2505,
      "num_input_tokens_seen": 357904,
      "step": 415
    },
    {
      "epoch": 1.8718662952646241,
      "grad_norm": 3.1672580242156982,
      "learning_rate": 1.5432914190872757e-05,
      "loss": 1.1803,
      "num_input_tokens_seen": 362112,
      "step": 420
    },
    {
      "epoch": 1.894150417827298,
      "grad_norm": 4.515994071960449,
      "learning_rate": 1.4895686066972703e-05,
      "loss": 1.2732,
      "num_input_tokens_seen": 366240,
      "step": 425
    },
    {
      "epoch": 1.916434540389972,
      "grad_norm": 4.615565776824951,
      "learning_rate": 1.4363978568283412e-05,
      "loss": 1.3164,
      "num_input_tokens_seen": 370416,
      "step": 430
    },
    {
      "epoch": 1.9387186629526463,
      "grad_norm": 4.71895170211792,
      "learning_rate": 1.3838082200210931e-05,
      "loss": 1.2085,
      "num_input_tokens_seen": 374784,
      "step": 435
    },
    {
      "epoch": 1.9610027855153205,
      "grad_norm": 4.07344913482666,
      "learning_rate": 1.331828429317345e-05,
      "loss": 1.1608,
      "num_input_tokens_seen": 379024,
      "step": 440
    },
    {
      "epoch": 1.9832869080779945,
      "grad_norm": 3.671696424484253,
      "learning_rate": 1.2804868845614525e-05,
      "loss": 1.0964,
      "num_input_tokens_seen": 383408,
      "step": 445
    },
    {
      "epoch": 2.0055710306406684,
      "grad_norm": 3.5722827911376953,
      "learning_rate": 1.229811636883677e-05,
      "loss": 1.2197,
      "num_input_tokens_seen": 387680,
      "step": 450
    },
    {
      "epoch": 2.0278551532033426,
      "grad_norm": 4.130892753601074,
      "learning_rate": 1.1798303733740802e-05,
      "loss": 1.0667,
      "num_input_tokens_seen": 392032,
      "step": 455
    },
    {
      "epoch": 2.050139275766017,
      "grad_norm": 3.263188600540161,
      "learning_rate": 1.130570401955322e-05,
      "loss": 0.9788,
      "num_input_tokens_seen": 396176,
      "step": 460
    },
    {
      "epoch": 2.0724233983286906,
      "grad_norm": 4.138238906860352,
      "learning_rate": 1.0820586364626104e-05,
      "loss": 1.0454,
      "num_input_tokens_seen": 400432,
      "step": 465
    },
    {
      "epoch": 2.094707520891365,
      "grad_norm": 4.847702980041504,
      "learning_rate": 1.0343215819389782e-05,
      "loss": 0.9915,
      "num_input_tokens_seen": 404720,
      "step": 470
    },
    {
      "epoch": 2.116991643454039,
      "grad_norm": 3.8637712001800537,
      "learning_rate": 9.873853201538971e-06,
      "loss": 0.9912,
      "num_input_tokens_seen": 409280,
      "step": 475
    },
    {
      "epoch": 2.139275766016713,
      "grad_norm": 4.850520133972168,
      "learning_rate": 9.412754953531663e-06,
      "loss": 1.0813,
      "num_input_tokens_seen": 413712,
      "step": 480
    },
    {
      "epoch": 2.1615598885793874,
      "grad_norm": 4.458592414855957,
      "learning_rate": 8.960173002478336e-06,
      "loss": 0.9845,
      "num_input_tokens_seen": 418000,
      "step": 485
    },
    {
      "epoch": 2.183844011142061,
      "grad_norm": 4.50455904006958,
      "learning_rate": 8.51635462249828e-06,
      "loss": 1.057,
      "num_input_tokens_seen": 422320,
      "step": 490
    },
    {
      "epoch": 2.2061281337047354,
      "grad_norm": 3.113966464996338,
      "learning_rate": 8.081542299618139e-06,
      "loss": 0.8823,
      "num_input_tokens_seen": 426624,
      "step": 495
    },
    {
      "epoch": 2.2284122562674096,
      "grad_norm": 4.872287750244141,
      "learning_rate": 7.65597359928646e-06,
      "loss": 1.0723,
      "num_input_tokens_seen": 430880,
      "step": 500
    }
  ],
  "logging_steps": 5,
  "max_steps": 672,
  "num_input_tokens_seen": 430880,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.945654890725376e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
