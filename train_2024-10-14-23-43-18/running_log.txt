[INFO|configuration_utils.py:675] 2024-10-15 00:07:01,572 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:07:01,574 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|tokenization_utils_base.py:2206] 2024-10-15 00:07:01,613 >> loading file tokenizer.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json

[INFO|tokenization_utils_base.py:2206] 2024-10-15 00:07:01,613 >> loading file tokenizer.model from cache at None

[INFO|tokenization_utils_base.py:2206] 2024-10-15 00:07:01,613 >> loading file added_tokens.json from cache at None

[INFO|tokenization_utils_base.py:2206] 2024-10-15 00:07:01,613 >> loading file special_tokens_map.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json

[INFO|tokenization_utils_base.py:2206] 2024-10-15 00:07:01,614 >> loading file tokenizer_config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json

[INFO|tokenization_utils_base.py:2470] 2024-10-15 00:07:02,062 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|configuration_utils.py:675] 2024-10-15 00:07:02,206 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:07:02,207 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|tokenization_utils_base.py:2206] 2024-10-15 00:07:02,246 >> loading file tokenizer.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json

[INFO|tokenization_utils_base.py:2206] 2024-10-15 00:07:02,246 >> loading file tokenizer.model from cache at None

[INFO|tokenization_utils_base.py:2206] 2024-10-15 00:07:02,246 >> loading file added_tokens.json from cache at None

[INFO|tokenization_utils_base.py:2206] 2024-10-15 00:07:02,246 >> loading file special_tokens_map.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json

[INFO|tokenization_utils_base.py:2206] 2024-10-15 00:07:02,246 >> loading file tokenizer_config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json

[INFO|tokenization_utils_base.py:2470] 2024-10-15 00:07:02,635 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|configuration_utils.py:675] 2024-10-15 00:08:49,655 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:08:49,680 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|modeling_utils.py:3732] 2024-10-15 00:08:50,720 >> loading weights file model.safetensors from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/model.safetensors.index.json

[INFO|modeling_utils.py:1622] 2024-10-15 00:09:57,685 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.

[INFO|configuration_utils.py:1099] 2024-10-15 00:09:57,779 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128009
}


[INFO|modeling_utils.py:4574] 2024-10-15 00:10:40,180 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


[INFO|modeling_utils.py:4582] 2024-10-15 00:10:40,182 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

[INFO|configuration_utils.py:1054] 2024-10-15 00:10:40,301 >> loading configuration file generation_config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/generation_config.json

[INFO|configuration_utils.py:1099] 2024-10-15 00:10:40,301 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128009
  ],
  "max_length": 4096,
  "temperature": 0.6,
  "top_p": 0.9
}


[WARNING|other.py:349] 2024-10-15 00:10:41,027 >> Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.

[INFO|trainer.py:667] 2024-10-15 00:10:41,046 >> Using auto half precision backend

[INFO|trainer.py:2243] 2024-10-15 00:10:47,618 >> ***** Running training *****

[INFO|trainer.py:2244] 2024-10-15 00:10:47,618 >>   Num examples = 3,590

[INFO|trainer.py:2245] 2024-10-15 00:10:47,618 >>   Num Epochs = 3

[INFO|trainer.py:2246] 2024-10-15 00:10:47,619 >>   Instantaneous batch size per device = 2

[INFO|trainer.py:2249] 2024-10-15 00:10:47,619 >>   Total train batch size (w. parallel, distributed & accumulation) = 16

[INFO|trainer.py:2250] 2024-10-15 00:10:47,619 >>   Gradient Accumulation steps = 8

[INFO|trainer.py:2251] 2024-10-15 00:10:47,619 >>   Total optimization steps = 672

[INFO|trainer.py:2252] 2024-10-15 00:10:47,626 >>   Number of trainable parameters = 20,971,520

[INFO|trainer.py:3705] 2024-10-15 00:15:34,542 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-100

[INFO|configuration_utils.py:675] 2024-10-15 00:15:34,894 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:15:34,898 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|tokenization_utils_base.py:2641] 2024-10-15 00:15:35,389 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-100/tokenizer_config.json

[INFO|tokenization_utils_base.py:2650] 2024-10-15 00:15:35,395 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-100/special_tokens_map.json

[INFO|trainer.py:3705] 2024-10-15 00:20:03,588 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-200

[INFO|configuration_utils.py:675] 2024-10-15 00:20:03,731 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:20:03,732 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|tokenization_utils_base.py:2641] 2024-10-15 00:20:04,130 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-200/tokenizer_config.json

[INFO|tokenization_utils_base.py:2650] 2024-10-15 00:20:04,136 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-200/special_tokens_map.json

[INFO|trainer.py:3705] 2024-10-15 00:24:38,809 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-300

[INFO|configuration_utils.py:675] 2024-10-15 00:24:39,080 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:24:39,082 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|tokenization_utils_base.py:2641] 2024-10-15 00:24:39,423 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-300/tokenizer_config.json

[INFO|tokenization_utils_base.py:2650] 2024-10-15 00:24:39,428 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-300/special_tokens_map.json

[INFO|trainer.py:3705] 2024-10-15 00:29:10,055 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-400

[INFO|configuration_utils.py:675] 2024-10-15 00:29:10,233 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:29:10,234 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|tokenization_utils_base.py:2641] 2024-10-15 00:29:10,574 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-400/tokenizer_config.json

[INFO|tokenization_utils_base.py:2650] 2024-10-15 00:29:10,579 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-400/special_tokens_map.json

[INFO|trainer.py:3705] 2024-10-15 00:33:37,199 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-500

[INFO|configuration_utils.py:675] 2024-10-15 00:33:37,340 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:33:37,341 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|tokenization_utils_base.py:2641] 2024-10-15 00:33:37,683 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-500/tokenizer_config.json

[INFO|tokenization_utils_base.py:2650] 2024-10-15 00:33:37,688 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-500/special_tokens_map.json

[INFO|trainer.py:3705] 2024-10-15 00:38:11,793 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-600

[INFO|configuration_utils.py:675] 2024-10-15 00:38:11,978 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:38:11,979 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|tokenization_utils_base.py:2641] 2024-10-15 00:38:12,334 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-600/tokenizer_config.json

[INFO|tokenization_utils_base.py:2650] 2024-10-15 00:38:12,338 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-600/special_tokens_map.json

[INFO|trainer.py:3705] 2024-10-15 00:41:24,621 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-672

[INFO|configuration_utils.py:675] 2024-10-15 00:41:24,731 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:41:24,732 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|tokenization_utils_base.py:2641] 2024-10-15 00:41:25,062 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-672/tokenizer_config.json

[INFO|tokenization_utils_base.py:2650] 2024-10-15 00:41:25,066 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/checkpoint-672/special_tokens_map.json

[INFO|trainer.py:2505] 2024-10-15 00:41:25,898 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)



[INFO|trainer.py:3705] 2024-10-15 00:41:25,919 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18

[INFO|configuration_utils.py:675] 2024-10-15 00:41:26,017 >> loading configuration file config.json from cache at /fs/classhomes/fall2024/cmsc473/c4730027/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json

[INFO|configuration_utils.py:742] 2024-10-15 00:41:26,018 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|tokenization_utils_base.py:2641] 2024-10-15 00:41:26,373 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/tokenizer_config.json

[INFO|tokenization_utils_base.py:2650] 2024-10-15 00:41:26,378 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2024-10-14-23-43-18/special_tokens_map.json

[INFO|modelcard.py:449] 2024-10-15 00:41:27,163 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

